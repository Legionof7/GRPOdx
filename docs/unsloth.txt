# https://docs.unsloth.ai/basics/reasoning-grpo-and-rl llms-full.txt

## GRPO and Reinforcement Learning
This article covers everything you need to know about GRPO, Reinforcement Learning (RL) and reward functions, along with tips, and the basics of using GRPO with Unsloth. If you're looking for a quickstart tutorial for using GRPO, see our guide [here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo):

[âš¡Tutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo)

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#grpo-notebooks)    GRPO notebooks:

- [Llama 3.1 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)

- [Phi-4 (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb)

- [Qwen2.5 (3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb)


DeepSeek developed [GRPO](https://unsloth.ai/blog/grpo) (Group Relative Policy Optimization) to train their R1 reasoning models. This RL technique optimizes responses efficiently without a value function model, reducing memory and computational costs compared to PPO (Proximal Policy Optimization).

- Usecases for GRPO isnâ€™t just for code or mathâ€”its reasoning process can enhance tasks like email automation, database retrieval, law, and medicine, greatly improving accuracy based on your dataset and reward function!

- With 15GB VRAM, Unsloth allows you to transform any model up to 17B parameters like Llama 3.1 (8B), Phi-4 (14B), Mistral (7B) or Qwen2.5 (7B) into a reasoning model

- **Minimum requirement:** Just â€¯5GB VRAM is enough to train your own reasoning model locally (for any model with 1.5B parameters or less)

- If you're not getting any reasoning, make sure you have enough training steps and ensure your [reward function/verifier](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier) is working. We provide examples for reward functions [here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-function-examples).

- Previous demonstrations show that you could achieve your own "aha" moment with Qwen2.5 (3B) - but it required 2xA100 GPUs (160GB VRAM). Now, with Unsloth, you can achieve the same "aha" moment using just a single 5GB VRAM GPU.

- Previously, GRPO was only supported for full fine-tuning, but we've made it work with QLoRA and LoRA

- On [**20K context lengths**](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#grpo-requirement-guidelines) for example with 8 generations per prompt, Unsloth uses only 54.3GB of VRAM for Llama 3.1 (8B), whilst standard implementations (+ Flash Attention 2) take **510.8GB (90% less for Unsloth)**.

- Please note, this isnâ€™t fine-tuning DeepSeekâ€™s R1 distilled models or using distilled data from R1 for tuning which Unsloth already supported. This is converting a standard model into a full-fledged reasoning model using GRPO.


In a test example, even though we only trained Phi-4 with 100 steps using GRPO, the results are already clear. The model without GRPO does not have the thinking token, whilst the one trained with GRPO does and also has the correct answer.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FyBeJAvfolzfEYyftji76%252Fprompt%2520only%2520example.png%3Falt%3Dmedia%26token%3D3903995a-d9d5-4cdc-9020-c4efe7fff651&width=768&dpr=4&quality=100&sign=80d59783&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#training-with-grpo)    Training with GRPO

For a tutorial on how to transform any open LLM into a reasoning model using Unsloth & GRPO, [see here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo).

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#how-grpo-trains-a-model)    **How GRPO Trains a Model**

1. For each question-answer pair, the model generates multiple possible responses (e.g., 8 variations).

2. Each response is evaluated using reward functions.

3. Training Steps:



- If you have 300 rows of data, that's 300 training steps (or 900 steps if trained for 3 epochs).

- You can increase the number of generated responses per question (e.g., from 8 to 16).


4. The model learns by updating its weights every step.


### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#basics-tips)    Basics/Tips

- Wait for at least **300 steps** for the reward to actually increase. In order to get decent results, you may need to trade for a minimum of 12 hours (this is how GRPO works), but keep in mind this isn't compulsory as you can stop at anytime.

- For optimal results have at least **500 rows of data**. You can try with even 10 rows of data but it's better to have more.

- Each training run will always be different depending on your model, data, reward function/verifier etc. so though 300 steps is what we wrote as the minimum, sometimes it might be 1000 steps or more. So, it depends on various factors.

- If you're using GRPO with Unsloth locally, please "pip install diffusers" as well if you get an error. Please also use the latest version of vLLM.

- Itâ€™s advised to apply GRPO to a model at least **1.5B in parameters** to correctly generate thinking tokens as smaller models may not.

- For GRPO's [**GPU VRAM requirements**](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#grpo-requirement-guidelines) **for QLoRA 4-bit**, the general rule is the model parameters = the amount of VRAM you will need (you can use less VRAM but this just to be safe). The more context length you set, the more VRAM. LoRA 16-bit will use at minimum 4x more VRAM.

- **Continuous fine-tuning is** possible and you can just leave GRPO running in the background.

- In the example notebooks, we use the [**GSM8K dataset**](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#gsm8k-reward-functions), the current most popular choice for R1-style training.

- If youâ€™re using a base model, ensure you have a chat template.

- The more you train with GRPO the better. The best part of GRPO is you don't even need that much data. All you need is a great reward function/verifier and the more time spent training, the better your model will get. Expect your reward vs step to increase as time progresses like this:





![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FUROleqJQ5aEp8MjTCWFf%252Funnamed.png%3Falt%3Dmedia%26token%3D12ca4975-7a0c-4d10-9178-20db28ad0451&width=768&dpr=4&quality=100&sign=a2046ca5&sv=2)

- Training loss tracking for GRPO is now built directly into Unsloth, eliminating the need for external tools like wandb etc. It contains full logging details for all reward functions now including the total aggregated reward function itself.


![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fjo7fVFoFG2xbZPgL45el%252FScreenshot%25202025-02-20%2520at%252004-52-52%2520Copy%2520of%2520Yet%2520another%2520copy%2520of%2520Llama3.1_%288B%29-GRPO.ipynb%2520-%2520Colab.png%3Falt%3Dmedia%26token%3D041c17b1-ab98-4ab6-b6fb-8c7e5a8c07df&width=768&dpr=4&quality=100&sign=b8126c85&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#reward-functions-verifier)    Reward Functions / Verifier

In Reinforcement Learning a **Reward Function** and a **Verifier** serve distinct roles in evaluating a modelâ€™s output. In general, you could interpret them as the same thing however, technically they're not but it does not matter as much as they are usually used in conjunction with each other.

**Verifier**:

- Determines whether the generated response is correct or incorrect.

- It does not assign a numerical scoreâ€”it simply verifies correctness.

- Example: If a model generates "5" for "2+2", the verifier checks and labels it as "wrong" (since the correct answer is 4).

- Verifiers can also execute code (e.g., in Python) to validate logic, syntax, and correctness without needing manual evaluation.


**Reward Function**:

- Converts verification results (or other criteria) into a numerical score.

- Example: If an answer is wrong, it might assign a penalty (-1, -2, etc.), while a correct answer could get a positive score (+1, +2).

- It can also penalize based on criteria beyond correctness, such as excessive length or poor readability.


**Key Differences**:

- A **Verifier** checks correctness but doesnâ€™t score.

- A **Reward Function** assigns a score but doesnâ€™t necessarily verify correctness itself.

- A Reward Function _can_ use a Verifier, but they are technically not the same.


### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#understanding-reward-functions)    **Understanding Reward Functions**

GRPO's primary goal is to maximize reward and learn how an answer was derived, rather than simply memorizing and reproducing responses from its training data.

- With every training step, GRPO **adjusts model weights** to maximize the reward. This process fine-tunes the model incrementally.

- **Regular fine-tuning** (without GRPO) only **maximizes next-word prediction probability** but does not optimize for a reward. GRPO **optimizes for a reward function** rather than just predicting the next word.

- You can **reuse data** across multiple epochs.

- **Default reward functions** can be predefined to be used on a wide array of use cases or you can ask ChatGPT/local model to generate them for you.

- Thereâ€™s no single correct way to design reward functions or verifiers - the possibilities are endless. However, they must be well-designed and meaningful, as poorly crafted rewards can unintentionally degrade model performance.


### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#reward-function-examples)    Reward Function Examples

You can refer to the examples below. You can input your generations into an LLM like ChatGPT 4o or Llama 3.1 (8B) and design a reward function and verifier to evaluate it. For example, feed your generations into a LLM of your choice and set a rule: "If the answer sounds too robotic, deduct 3 points." This helps refine outputs based on quality criteria

#### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#example-1-simple-arithmetic-task)    **Example \#1: Simple Arithmetic Task**

- **Question:** `"2 + 2"`

- **Answer:** `"4"`

- **Reward Function 1:**



- If a number is detected â†’ **+1**

- If no number is detected â†’ **-1**


- **Reward Function 2:**



- If the number matches the correct answer â†’ **+3**

- If incorrect â†’ **-3**


- **Total Reward:** _Sum of all reward functions_


#### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#example-2-email-automation-task)    **Example \#2: Email Automation Task**

- **Question:** Inbound email

- **Answer:** Outbound email

- **Reward Functions:**



- If the answer contains a required keyword â†’ **+1**

- If the answer exactly matches the ideal response â†’ **+1**

- If the response is too long â†’ **-1**

- If the recipient's name is included â†’ **+1**

- If a signature block (phone, email, address) is present â†’ **+1**


### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#gsm8k-reward-functions)    GSM8K Reward Functions

In our examples, we've built on existing GSM8K reward functions by [@willccbb](https://x.com/willccbb) which is popular and shown to be quite effective:

- **correctness\_reward\_func** â€“ Rewards exact label matches.

- **int\_reward\_func** â€“ Encourages integer-only answers.

- **soft\_format\_reward\_func** â€“ Checks structure but allows minor newline mismatches.

- **strict\_format\_reward\_func** â€“ Ensures response structure matches the prompt, including newlines.

- **xmlcount\_reward\_func** â€“ Ensures exactly one of each XML tag in the response.


## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#using-vllm)    Using vLLM

You can now use [vLLM](https://github.com/vllm-project/vllm/) directly in your finetuning stack, which allows for much more throughput and allows you to finetune and do inference on the model at the same time! On 1x A100 40GB, expect 4000 tokens / s or so with Unslothâ€™s dynamic 4bit quant of Llama 3.2 3B Instruct. On a 16GB Tesla T4 (free Colab GPU), you can get 300 tokens / s.

We also magically removed double memory usage when loading vLLM and Unsloth together, allowing for savings of 5GB or so for Llama 3.1 8B and 3GB for Llama 3.2 3B. Unsloth could originally finetune Llama 3.3 70B Instruct in 1x 48GB GPU with Llama 3.3 70B weights taking 40GB of VRAM. If we do not remove double memory usage, then weâ€™ll need >= 80GB of VRAM when loading Unsloth and vLLM together.

But with Unsloth, you can still finetune and get the benefits of fast inference in one package in under 48GB of VRAM! To use fast inference, first install vllm, and instantiate Unsloth with fast\_inference:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install unsloth vllm
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct",
    fast_inference = True,
)
model.fast_generate(["Hello!"])
```

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#grpo-requirement-guidelines)    GRPO Requirement Guidelines

When youâ€™re using Unsloth to do GRPO, we smartly reduce VRAM usage by over 90% when compared to standard implementations with Flash Attention 2 by using multiple tricks! On 20K context lengths for example with 8 generations per prompt, Unsloth uses only **54.3GB of VRAM for Llama 3.1 8B**, whilst standard implementations take **510.8GB (90% less for Unsloth)**.

1. For GRPO's **GPU VRAM requirements for QLoRA 4-bit**, the general rule is the model parameters = the amount of VRAM you will need (you can use less VRAM but this just to be safe). The more context length you set, the more VRAM. LoRA 16-bit will use at minimum 4x more VRAM.

2. Our new memory efficient linear kernels for GRPO slashes memory usage by 8x or more. This shaves 68.5GB of memory, whilst being actually faster through the help of torch.compile!

3. We leverage our smart [Unsloth gradient checkpointing](https://unsloth.ai/blog/long-context) algorithm which we released a while ago. It smartly offloads intermediate activations to system RAM asynchronously whilst being only 1% slower. This shaves 52GB of memory.

4. Unsloth also uses the same GPU / CUDA memory space as the underlying inference engine (vLLM), unlike implementations in other packages. This shaves 16GB of memory.


Metrics

Unsloth

Standard + FA2

Training Memory Cost (GB)

42GB

414GB

GRPO Memory Cost (GB)

9.8GB

78.3GB

Inference Cost (GB)

0GB

16GB

Inference KV Cache for 20K context length (GB)

2.5GB

2.5GB

Total Memory Usage

54.33GB (90% less)

510.8GB

In typical standard GRPO implementations, you need to create 2 logits of size (8. 20K) to calculate the GRPO loss. This takes 2 \* 2 bytes \* 8 (num generations) \* 20K (context length) \* 128256 (vocabulary size) = 78.3GB in VRAM.

Unsloth shaves 8x memory usage for long context GRPO, so we need only an extra 9.8GB in extra VRAM for 20K context lengths!

We also need to from the KV Cache in 16bit. Llama 3.1 8B has 32 layers, and both K and V are 1024 in size. So memory usage for 20K context length = 2 \* 2 bytes \* 32 layers \* 20K context length \* 1024 = 2.5GB per batch. We would set the batch size for vLLM to 8, but we shall leave it at 1 for our calculations to save VRAM. Otherwise you will need 20GB for the KV cache.

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#how-grpo-works)    How GRPO Works:

DeepSeekâ€™s researchers observed an "aha moment" when training R1-Zero with pure reinforcement learning (RL). The model learned to extend its thinking time by reevaluating its initial approach, without any human guidance or predefined instructions.

1. The model generates groups of responses.

2. Each response is scored based on correctness or another metric created by some set reward function rather than an LLM reward model.

3. The average score of the group is computed.

4. Each response's score is compared to the group average.

5. The model is reinforced to favor higher-scoring responses.


As an example, assume we want a model to solve:

What is 1+1? >> Chain of thought/working out >> The answer is 2.

What is 2+2? >> Chain of thought/working out >> The answer is 4.

Originally, one had to collect large swathes of data to fill the working out / chain of thought process. But GRPO (the algorithm DeepSeek uses) or other RL algorithms can steer the model to automatically exhibit reasoning capabilities and create the reasoning trace. Instead, we need to create good reward functions or verifiers. For example, if it gets the correct answer, give it a score of 1. If some words are mis-spelt, minus 0.1. And so on! We can provide many many functions to reward the process.

[PreviousFine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-guide) [NextTutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo)

Last updated 2 days ago

Was this helpful?

* * *

## Train Reasoning Model
DeepSeek developed [GRPO](https://unsloth.ai/blog/grpo) (Group Relative Policy Optimization) to train their R1 reasoning models.

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo\#quickstart)    Quickstart

These instructions are for our pre-made Google Colab [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks). If you are installing Unsloth locally, you can also copy our notebooks inside your favorite code editor.

#### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo\#the-grpo-notebooks-we-are-using-llama-3.1-8b-phi-4-14b-and-qwen2.5-3b)    The GRPO notebooks we are using: [Llama 3.1 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb), [Phi-4 (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb) and [Qwen2.5 (3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb)

1

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo\#install-unsloth)    Install Unsloth

If you're using our Colab notebook, click **Runtime > Run all**. We'd highly recommend you checking out our [Fine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-guide) before getting started.

If installing locally, ensure you have the correct [requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements) and use `pip install unsloth ` on Linux or follow our [Windows install](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation) instructions.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FCovHTH7dI2GcwNZm5TxF%252Fimage.png%3Falt%3Dmedia%26token%3Da157e33b-ad01-4174-a01c-67f742e4e732&width=768&dpr=4&quality=100&sign=e2f6a15e&sv=2)

2

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo\#learn-about-grpo-and-reward-functions)    Learn about GRPO & Reward Functions

Before we get started, it is recommended to learn more about GRPO, reward functions and how they work. Read more about them including [tips & tricks](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#basics-tips) [here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#basics-tips).

You will also need enough VRAM. In general, model parameters = amount of VRAM you will need. In Colab, we are using their free 16GB VRAM GPUs which can train any model up to 16B in parameters.

3

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo\#configure-desired-settings)    Configure desired settings

We have pre-selected optimal settings for the best results for you already and you can change the model to whichever you want listed in our [supported models](https://docs.unsloth.ai/get-started/all-our-models). Would not recommend changing other settings if you're a beginner.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fyd3RkyPKInZBbvX1Memf%252Fimage.png%3Falt%3Dmedia%26token%3Da9ca4ce4-2e9f-4b5a-a65c-646d267411c8&width=768&dpr=4&quality=100&sign=89b12b4e&sv=2)

4

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo\#select-your-dataset)    Select your dataset

We have pre-selected OpenAI's [GSM8K](https://huggingface.co/datasets/openai/gsm8k) dataset already but you could change it to your own or any public one on Hugging Face. You can read more about [datasets here](https://docs.unsloth.ai/basics/datasets-101).

Your dataset should still have at least 2 columns for question and answer pairs. However the answer must not reveal the reasoning behind how it derived the answer from the question. See below for an example:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FqdTVcMEeJ3kzPToSY1X8%252Fimage.png%3Falt%3Dmedia%26token%3D3dd8d9d7-1847-42b6-a73a-f9c995b798b1&width=768&dpr=4&quality=100&sign=7dd5cdc9&sv=2)

5

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo\#reward-functions-verifier)    Reward Functions/Verifier

[Reward Functions/Verifiers](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier) lets us know if the model is doing well or not according to the dataset you have provided. Each generation run will be assessed on how it performs to the score of the average of the rest of generations. You can create your own reward functions however we have already pre-selected them for you with [Will's GSM8K](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#gsm8k-reward-functions) reward functions. With this, we have 5 different ways which we can reward each generation.

You can input your generations into an LLM like ChatGPT 4o or Llama 3.1 (8B) and design a reward function and verifier to evaluate it. For example, feed your generations into a LLM of your choice and set a rule: "If the answer sounds too robotic, deduct 3 points." This helps refine outputs based on quality criteria. **See examples** of what they can look like [here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-function-examples).

**Example Reward Function for an Email Automation Task:**

- **Question:** Inbound email

- **Answer:** Outbound email

- **Reward Functions:**



- If the answer contains a required keyword â†’ **+1**

- If the answer exactly matches the ideal response â†’ **+1**

- If the response is too long â†’ **-1**

- If the recipient's name is included â†’ **+1**

- If a signature block (phone, email, address) is present â†’ **+1**


![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F6GRcqgUKmKn2dWCk4nWK%252Fimage.png%3Falt%3Dmedia%26token%3Dac153141-03f8-4795-9074-ad592289bd70&width=768&dpr=4&quality=100&sign=3f226098&sv=2)

6

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo\#train-your-model)    Train your model

We have pre-selected hyperparameters for the most optimal results however you could change them. Read all about [parameters here](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia).

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F1MpLSyaOH3j8MhQvquqX%252Fimage.png%3Falt%3Dmedia%26token%3D818034b1-f2db-464d-a108-3b2c6897edb7&width=768&dpr=4&quality=100&sign=4da81ed4&sv=2)

You should see the reward increase overtime. We would recommend you train for at least 300 steps which may take 30 mins however, for optimal results, you should train for longer.

You will also see sample answers which allows you to see how the model is learning. Some may have steps, XML tags, attempts etc. and the idea is as trains it's going to get better and better because it's going to get scored higher and higher until we get the outputs we desire with long reasoning chains of answers.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FyRmUGe8laUKIl0RKwlE6%252Fimage.png%3Falt%3Dmedia%26token%3D3ff931cc-0d2b-4a9c-bbe1-b6289b22d157&width=768&dpr=4&quality=100&sign=40488764&sv=2)

7

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo\#run-and-save-your-model)    Run & Save your model

Run your model by clicking the play button. In the first example, there is usually no reasoning in the answer and in order to see the reasoning, we need to first save the LoRA we just trained with GRPO first.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FkLHdlRVKN58tM7SGKp3O%252Fimage.png%3Falt%3Dmedia%26token%3Db43a8164-7eae-4ec4-bf59-976078f9be31&width=768&dpr=4&quality=100&sign=e2241c49&sv=2)

The first inference example run has no reasoning. You must load the LoRA and test it to reveal the reasoning.

Then we load the LoRA and test it. Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!

You can then save your model to GGUF, Ollama etc. by following our [guide here](https://docs.unsloth.ai/get-started/fine-tuning-guide#id-7.-running--saving-the-model).

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FYdz5ch20Ig8JlumBesle%252Fimage.png%3Falt%3Dmedia%26token%3D8aea2867-b8a8-470a-aa4b-a7b9cdd64c3c&width=768&dpr=4&quality=100&sign=a9ddceab&sv=2)

If you are still not getting any reasoning, you may have either trained for too less steps or your reward function/verifier was not optimal.

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo\#video-tutorials)    Video Tutorials

Here are some video tutorials created by amazing YouTubers who we think are fantastic!

Train A DeepSeek Style Reasoning Model With UnslothAI (Local Tutorial) - YouTube

Bijan Bowen

12.2K subscribers

[Train A DeepSeek Style Reasoning Model With UnslothAI (Local Tutorial)](https://www.youtube.com/watch?v=SoPE1cUz3Hs)

Bijan Bowen

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Share

Include playlist

An error occurred while retrieving sharing information. Please try again later.

[Watch on](https://www.youtube.com/watch?v=SoPE1cUz3Hs&embeds_referring_euri=https%3A%2F%2Fcdn.iframe.ly%2F)

0:00

0:00 / 34:47â€¢Live

â€¢

[Watch on YouTube](https://www.youtube.com/watch?v=SoPE1cUz3Hs "Watch on YouTube")

Local GRPO on your own device

Deepseek-R1 & Training Your Own Reasoning Model - YouTube

AI Makerspace

12K subscribers

[Deepseek-R1 & Training Your Own Reasoning Model](https://www.youtube.com/watch?v=bbFEYPx9Hpo)

AI Makerspace

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Share

Include playlist

An error occurred while retrieving sharing information. Please try again later.

[Watch on](https://www.youtube.com/watch?t=3289&v=bbFEYPx9Hpo&embeds_referring_euri=https%3A%2F%2Fcdn.iframe.ly%2F)

54:49

54:49 / 1:09:58â€¢Live

â€¢

[Watch on YouTube](https://www.youtube.com/watch?v=bbFEYPx9Hpo "Watch on YouTube")

Great to learn about how to prep your dataset and explanations behind Reinforcement Learning + GRPO basics

This ONE TRICK Turns your LLM like DeepSeek R1ðŸ’¥ Train your own DeepLlama for Free! ðŸ’¥ - YouTube

1littlecoder

95.1K subscribers

[This ONE TRICK Turns your LLM like DeepSeek R1ðŸ’¥ Train your own DeepLlama for Free! ðŸ’¥](https://www.youtube.com/watch?v=juOh1afy-IE)

1littlecoder

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Share

Include playlist

An error occurred while retrieving sharing information. Please try again later.

[Watch on](https://www.youtube.com/watch?v=juOh1afy-IE&embeds_referring_euri=https%3A%2F%2Fcdn.iframe.ly%2F)

0:00

0:00 / 22:32â€¢Live

â€¢

[Watch on YouTube](https://www.youtube.com/watch?v=juOh1afy-IE "Watch on YouTube")

ðŸš€Create the Aha Moment Locally \| Turn any model into a Reasoning Model using Unsloth - YouTube

Prompt Engineer

15.8K subscribers

[ðŸš€Create the Aha Moment Locally \| Turn any model into a Reasoning Model using Unsloth](https://www.youtube.com/watch?v=oF0_eMhzRaQ)

Prompt Engineer

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Share

Include playlist

An error occurred while retrieving sharing information. Please try again later.

[Watch on](https://www.youtube.com/watch?v=oF0_eMhzRaQ&embeds_referring_euri=https%3A%2F%2Fcdn.iframe.ly%2F)

0:00

0:00 / 15:43â€¢Live

â€¢

[Watch on YouTube](https://www.youtube.com/watch?v=oF0_eMhzRaQ "Watch on YouTube")

[PreviousReasoning - GRPO & RL](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl) [NextReinforcement Learning - DPO, ORPO & KTO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto)

Last updated 8 days ago

Was this helpful?

* * *

## Reinforcement Learning Techniques
DPO (Direct Preference Optimization), ORPO (Odds Ratio Preference Optimization), PPO, KTO Reward Modelling all work with Unsloth.

We have Google Colab notebooks for reproducing ORPO, DPO Zephyr, KTO and SimPO:

- [ORPO notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb)

- [DPO Zephyr notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb)

- [KTO notebook](https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing)

- [SimPO notebook](https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing)


We're also in ðŸ¤—Hugging Face's official docs! We're on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto\#dpo-code)    DPO Code

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
python
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0" # Optional set GPU device ID

from unsloth import FastLanguageModel, PatchDPOTrainer
from unsloth import is_bfloat16_supported
PatchDPOTrainer()
import torch
from transformers import TrainingArguments
from trl import DPOTrainer

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/zephyr-sft-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 64,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",\
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 64,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
)

dpo_trainer = DPOTrainer(
    model = model,
    ref_model = None,
    args = TrainingArguments(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 8,
        warmup_ratio = 0.1,
        num_train_epochs = 3,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        seed = 42,
        output_dir = "outputs",
    ),
    beta = 0.1,
    train_dataset = YOUR_DATASET_HERE,
    # eval_dataset = YOUR_DATASET_HERE,
    tokenizer = tokenizer,
    max_length = 1024,
    max_prompt_length = 512,
)
dpo_trainer.train()
```

[PreviousTutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo) [NextTutorial: How to Run DeepSeek-R1 Locally](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally)

Last updated 14 days ago

Was this helpful?

* * *

## Finetune Llama-3 Chatbot
By the end of this tutorial, you will create a custom chatbot by **finetuning Llama-3** with [**Unsloth**](https://github.com/unslothai/unsloth) for free. It can run locally via [**Ollama**](https://github.com/ollama/ollama) on your PC, or in a free GPU instance through [**Google Colab**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb). You will be able to interact with the chatbot interactively like below:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FXlEQrBR24CKI9lQIzOS7%252FAssistant%2520example.png%3Falt%3Dmedia%26token%3Dfac7f5b0-69f4-4998-baee-3feee44f8c16&width=768&dpr=4&quality=100&sign=39273e6a&sv=2)

**Unsloth** makes finetuning much easier, and can automatically export the finetuned model to **Ollama** with integrated automatic `Modelfile` creation! If you need help, you can join our Discord server: [https://discord.com/invite/unsloth](https://discord.com/invite/unsloth)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-1.-what-is-unsloth)    1\. What is Unsloth?

[Unsloth](https://github.com/unslothai/unsloth) makes finetuning LLMs like Llama-3, Mistral, Phi-3 and Gemma 2x faster, use 70% less memory, and with no degradation in accuracy! We will be using Google Colab which provides a free GPU during this tutorial. You can access our free notebooks below:

- [Ollama Llama-3 Alpaca](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb) (notebook which we will be using)

- [CSV/Excel Ollama Guide](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing)


#### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#you-will-also-need-to-login-into-your-google-account)    _**You will also need to login into your Google account!**_

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FqnogsAv2zZ5WPFkXwQ5t%252FColab%2520Screen.png%3Falt%3Dmedia%26token%3D8722cf50-898f-4f15-be7a-7223b8b7440b&width=768&dpr=4&quality=100&sign=c93e1323&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-2.-what-is-ollama)    2\. What is Ollama?

[Ollama](https://github.com/ollama/ollama) allows you to run language models from your own computer in a quick and simple way! It quietly launches a program which can run a language model like Llama-3 in the background. If you suddenly want to ask the language model a question, you can simply submit a request to Ollama, and it'll quickly return the results to you! We'll be using Ollama as our inference engine!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FqKwhUFNW52GnKMi5ClLW%252FOllama.png%3Falt%3Dmedia%26token%3D27ccad2f-12a2-4188-96d9-ee3023d7f274&width=768&dpr=4&quality=100&sign=e04cd2e2&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-3.-install-unsloth)    3\. Install Unsloth

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FQzuUQL60uFWHpaAvDPYD%252FColab%2520Options.png%3Falt%3Dmedia%26token%3Dfb808ec5-20c5-4f42-949e-14ed26a44987&width=768&dpr=4&quality=100&sign=be097a14&sv=2)

If you have never used a Colab notebook, a quick primer on the notebook itself:

1. **Play Button at each "cell".** Click on this to run that cell's code. You must not skip any cells and you must run every cell in chronological order. If you encounter any errors, simply rerun the cell you did not run before. Another option is to click CTRL + ENTER if you don't want to click the play button.

2. **Runtime Button in the top toolbar.** You can also use this button and hit "Run all" to run the entire notebook in 1 go. This will skip all the customization steps, and can be a good first try.

3. **Connect / Reconnect T4 button.** You can click here for more advanced system statistics.


The first installation cell looks like below: Remember to click the PLAY button in the brackets \[ \]. We grab our open source Github package, and install some other packages.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F9DTAK0evMnZcnLXzKLx4%252Fimage.png%3Falt%3Dmedia%26token%3Db4781438-3858-4d6c-a560-5afcbbc12fa8&width=768&dpr=4&quality=100&sign=e78940c8&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-4.-selecting-a-model-to-finetune)    4\. Selecting a model to finetune

Let's now select a model for finetuning! We defaulted to Llama-3 from Meta / Facebook which was trained on a whopping 15 trillion "tokens". Assume a token is like 1 English word. That's approximately 350,000 thick Encyclopedias worth! Other popular models include Mistral, Phi-3 (trained using GPT-4 output) and Gemma from Google (13 trillion tokens!).

Unsloth supports these models and more! In fact, simply type a model from the Hugging Face model hub to see if it works! We'll error out if it doesn't work.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fmdci7SWqnAZiW8KzzDp0%252Fimage.png%3Falt%3Dmedia%26token%3D8ede6c31-3cc9-4005-ae44-0b056750e8d4&width=768&dpr=4&quality=100&sign=f453cf0e&sv=2)

There are 3 other settings which you can toggle:

1. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
max_seq_length = 2048
```





This determines the context length of the model. Gemini for example has over 1 million context length, whilst Llama-3 has 8192 context length. We allow you to select ANY number - but we recommend setting it 2048 for testing purposes. Unsloth also supports very long context finetuning, and we show we can provide 4x longer context lengths than the best.

2. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
dtype = None
```





Keep this as None, but you can select torch.float16 or torch.bfloat16 for newer GPUs.

3. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
load_in_4bit = True
```





We do finetuning in 4 bit quantization. This reduces memory usage by 4x, allowing us to actually do finetuning in a free 16GB memory GPU. 4 bit quantization essentially converts weights into a limited set of numbers to reduce memory usage. A drawback of this is there is a 1-2% accuracy degradation. Set this to False on larger GPUs like H100s if you want that tiny extra accuracy.


![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FegXn4FqK96xXZWMz4NH5%252Fimage.png%3Falt%3Dmedia%26token%3D7531f78d-390b-470b-a91e-4463eea6537f&width=768&dpr=4&quality=100&sign=c6859bb2&sv=2)

If you run the cell, you will get some print outs of the Unsloth version, which model you are using, how much memory your GPU has, and some other statistics. Ignore this for now.

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-5.-parameters-for-finetuning)    5\. Parameters for finetuning

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FqRTuI7x0FYlHTXqbi0hu%252Fimage.png%3Falt%3Dmedia%26token%3D4b0e0032-dbf1-4148-ba92-c18356862765&width=768&dpr=4&quality=100&sign=f94a3d99&sv=2)

Now to customize your finetune, you can edit the numbers above, but you can ignore it, since we already select quite reasonable numbers.

The goal is to change these numbers to increase accuracy, but also **counteract over-fitting**. Over-fitting is when you make the language model memorize a dataset, and not be able to answer novel new questions. We want to a final model to answer unseen questions, and not do memorization.

1. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
```





The rank of the finetuning process. A larger number uses more memory and will be slower, but can increase accuracy on harder tasks. We normally suggest numbers like 8 (for fast finetunes), and up to 128. Too large numbers can causing over-fitting, damaging your model's quality.

2. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",\
                     "gate_proj", "up_proj", "down_proj",],
```





We select all modules to finetune. You can remove some to reduce memory usage and make training faster, but we highly do not suggest this. Just train on all modules!

3. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
lora_alpha = 16,
```





The scaling factor for finetuning. A larger number will make the finetune learn more about your dataset, but can promote over-fitting. We suggest this to equal to the rank `r`, or double it.

4. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
lora_dropout = 0, # Supports any, but = 0 is optimized
```





Leave this as 0 for faster training! Can reduce over-fitting, but not that much.

5. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
bias = "none",    # Supports any, but = "none" is optimized
```





Leave this as 0 for faster and less over-fit training!

6. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
```





Options include `True`, `False ` and `"unsloth"`. We suggest `"unsloth"` since we reduce memory usage by an extra 30% and support extremely long context finetunes.You can read up here: [https://unsloth.ai/blog/long-context](https://unsloth.ai/blog/long-context) for more details.

7. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
random_state = 3407,
```





The number to determine deterministic runs. Training and finetuning needs random numbers, so setting this number makes experiments reproducible.

8. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
use_rslora = False,  # We support rank stabilized LoRA
```





Advanced feature to set the `lora_alpha = 16` automatically. You can use this if you want!

9. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
loftq_config = None, # And LoftQ
```





Advanced feature to initialize the LoRA matrices to the top r singular vectors of the weights. Can improve accuracy somewhat, but can make memory usage explode at the start.


## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-6.-alpaca-dataset)    6\. Alpaca Dataset

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FKSmRDpkySelZfWSrWxDm%252Fimage.png%3Falt%3Dmedia%26token%3D5401e4da-796a-42ad-8b85-2263f3e59e86&width=768&dpr=4&quality=100&sign=28ad8509&sv=2)

We will now use the Alpaca Dataset created by calling GPT-4 itself. It is a list of 52,000 instructions and outputs which was very popular when Llama-1 was released, since it made finetuning a base LLM be competitive with ChatGPT itself.

You can access the GPT4 version of the Alpaca dataset here: [https://huggingface.co/datasets/vicgalle/alpaca-gpt4](https://huggingface.co/datasets/vicgalle/alpaca-gpt4). An older first version of the dataset is here: [https://github.com/tatsu-lab/stanford\_alpaca](https://github.com/tatsu-lab/stanford_alpaca). Below shows some examples of the dataset:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FzKhujR9Nxz95VFSdf4J5%252Fimage.png%3Falt%3Dmedia%26token%3Da3c52718-eaf1-4a3d-b325-414d8e67722e&width=768&dpr=4&quality=100&sign=2afb3a12&sv=2)

You can see there are 3 columns in each row - an instruction, and input and an output. We essentially combine each row into 1 large prompt like below. We then use this to finetune the language model, and this made it very similar to ChatGPT. We call this process **supervised instruction finetuning**.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FieYX44Vjd0OygJvO0jaR%252Fimage.png%3Falt%3Dmedia%26token%3Deb67fa41-a280-4656-8be6-5b6bf6f587c2&width=768&dpr=4&quality=100&sign=68f5594e&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-7.-multiple-columns-for-finetuning)    7\. Multiple columns for finetuning

But a big issue is for ChatGPT style assistants, we only allow 1 instruction / 1 prompt, and not multiple columns / inputs. For example in ChatGPT, you can see we must submit 1 prompt, and not multiple prompts.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FpFUWhntUQLu05l4ns7Pq%252Fimage.png%3Falt%3Dmedia%26token%3De989e4a6-6033-4741-b97f-d0c3ce8f5888&width=768&dpr=4&quality=100&sign=a9eb969a&sv=2)

This essentially means we have to "merge" multiple columns into 1 large prompt for finetuning to actually function!

For example the very famous Titanic dataset has many many columns. Your job was to predict whether a passenger has survived or died based on their age, passenger class, fare price etc. We can't simply pass this into ChatGPT, but rather, we have to "merge" this information into 1 large prompt.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FrydHBjHoJT7w8FwzKAXK%252FMerge-1.png%3Falt%3Dmedia%26token%3Dec812057-0475-4717-87fe-311f14735c37&width=768&dpr=4&quality=100&sign=8211e070&sv=2)

For example, if we ask ChatGPT with our "merged" single prompt which includes all the information for that passenger, we can then ask it to guess or predict whether the passenger has died or survived.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FJVkv73fRWvwwFxMym7uW%252Fimage.png%3Falt%3Dmedia%26token%3D59b97b76-f2f2-46c9-8940-60a37e4e7d62&width=768&dpr=4&quality=100&sign=37c0f3a1&sv=2)

Other finetuning libraries require you to manually prepare your dataset for finetuning, by merging all your columns into 1 prompt. In Unsloth, we simply provide the function called `to_sharegpt` which does this in 1 go!

To access the Titanic finetuning notebook or if you want to upload a CSV or Excel file, go here: [https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing)

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F9fo2IBA7P0tNwhNR9Prm%252Fimage.png%3Falt%3Dmedia%26token%3D7bd7244a-0fea-4e57-9038-a8a360138056&width=768&dpr=4&quality=100&sign=a94d397b&sv=2)

Now this is a bit more complicated, since we allow a lot of customization, but there are a few points:

- You must enclose all columns in curly braces `{}`. These are the column names in the actual CSV / Excel file.

- Optional text components must be enclosed in `[[]]`. For example if the column "input" is empty, the merging function will not show the text and skip this. This is useful for datasets with missing values.

- Select the output or target / prediction column in `output_column_name`. For the Alpaca dataset, this will be `output`.


For example in the Titanic dataset, we can create a large merged prompt format like below, where each column / piece of text becomes optional.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FRMvBpfXC9ToCRL0oCJfN%252Fimage.png%3Falt%3Dmedia%26token%3Dc257c7fc-8a9c-4d4f-ab3d-6894ae49f2a9&width=768&dpr=4&quality=100&sign=4ec813ed&sv=2)

For example, pretend the dataset looks like this with a lot of missing data:

Embarked

Age

Fare

S

23

18

7.25

Then, we do not want the result to be:

1. The passenger embarked from S. Their age is 23. Their fare is **EMPTY**.

2. The passenger embarked from **EMPTY**. Their age is 18. Their fare is $7.25.


Instead by optionally enclosing columns using `[[]]`, we can exclude this information entirely.

1. \[\[The passenger embarked from S.\]\] \[\[Their age is 23.\]\] \[\[Their fare is **EMPTY**.\]\]

2. \[\[The passenger embarked from **EMPTY**.\]\] \[\[Their age is 18.\]\] \[\[Their fare is $7.25.\]\]


becomes:

1. The passenger embarked from S. Their age is 23.

2. Their age is 18. Their fare is $7.25.


## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-8.-multi-turn-conversations)    8\. Multi turn conversations

A bit issue if you didn't notice is the Alpaca dataset is single turn, whilst remember using ChatGPT was interactive and you can talk to it in multiple turns. For example, the left is what we want, but the right which is the Alpaca dataset only provides singular conversations. We want the finetuned language model to somehow learn how to do multi turn conversations just like ChatGPT.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FWCAN7bYUt6QWwCWUxisL%252Fdiff.png%3Falt%3Dmedia%26token%3D29821fd9-2181-4d1d-8b93-749b69bcf400&width=768&dpr=4&quality=100&sign=d4f1b675&sv=2)

So we introduced the `conversation_extension` parameter, which essentially selects some random rows in your single turn dataset, and merges them into 1 conversation! For example, if you set it to 3, we randomly select 3 rows and merge them into 1! Setting them too long can make training slower, but could make your chatbot and final finetune much better!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FWi1rRNBFC2iDmCvSJsZt%252Fcombine.png%3Falt%3Dmedia%26token%3Dbef37a55-b272-4be3-89b5-9767c219a380&width=768&dpr=4&quality=100&sign=ae98ba1b&sv=2)

Then set `output_column_name` to the prediction / output column. For the Alpaca dataset dataset, it would be the output column.

We then use the `standardize_sharegpt` function to just make the dataset in a correct format for finetuning! Always call this!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FE75C4Y848VNF6luLuPRR%252Fimage.png%3Falt%3Dmedia%26token%3Daac1d79b-ecca-4e56-939d-d97dcbbf30eb&width=768&dpr=4&quality=100&sign=d48e3c76&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-9.-customizable-chat-templates)    9\. Customizable Chat Templates

We can now specify the chat template for finetuning itself. The very famous Alpaca format is below:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F8SWcsgH47Uhkm0IclDs5%252Fimage.png%3Falt%3Dmedia%26token%3Dfa03d7aa-d568-468d-9884-18e925a0551f&width=768&dpr=4&quality=100&sign=dff54efb&sv=2)

But remember we said this was a bad idea because ChatGPT style finetunes require only 1 prompt? Since we successfully merged all dataset columns into 1 using Unsloth, we essentially can create the below style chat template with 1 input column (instruction) and 1 output:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FyuMpSLIpPLEbcdh970UJ%252Fimage.png%3Falt%3Dmedia%26token%3D87c4d5e1-accf-4847-9971-63e3a47b4a5f&width=768&dpr=4&quality=100&sign=728095c1&sv=2)

We just require you must put a `{INPUT}` field for the instruction and an `{OUTPUT}` field for the model's output field. We in fact allow an optional `{SYSTEM}` field as well which is useful to customize a system prompt just like in ChatGPT. For example, below are some cool examples which you can customize the chat template to be:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fi6B8IP1OZmmxBYr6k4W3%252Fimage.png%3Falt%3Dmedia%26token%3D061d1b4c-4b22-4d1b-a423-8d4c15e40efa&width=768&dpr=4&quality=100&sign=dd8c7435&sv=2)

For the ChatML format used in OpenAI models:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F3OEJaXooJCICJR6DJIJP%252Fimage.png%3Falt%3Dmedia%26token%3D4fa85cf1-463d-4090-a838-591c4f94efea&width=768&dpr=4&quality=100&sign=a1f23ff9&sv=2)

Or you can use the Llama-3 template itself (which only functions by using the instruct version of Llama-3): We in fact allow an optional `{SYSTEM}` field as well which is useful to customize a system prompt just like in ChatGPT.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F4qQXd0hIvh9fJNO2cJ04%252Fimage.png%3Falt%3Dmedia%26token%3D614b9200-7375-47f5-ac15-ce9aa891ede4&width=768&dpr=4&quality=100&sign=c9811100&sv=2)

Or in the Titanic prediction task where you had to predict if a passenger died or survived in this Colab notebook which includes CSV and Excel uploading: [https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing)

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F1iQitC3PwcuV0LpHEhdP%252Fimage.png%3Falt%3Dmedia%26token%3Dd117f681-afb0-4d5f-b534-f51013fe772a&width=768&dpr=4&quality=100&sign=20577629&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-10.-train-the-model)    10\. Train the model

Let's train the model now! We normally suggest people to not edit the below, unless if you want to finetune for longer steps or want to train on large batch sizes.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FoPTTR7ppdxhZR2iPpE0R%252Fimage.png%3Falt%3Dmedia%26token%3D1dca98a5-c927-4e93-8e96-977015f4eeb9&width=768&dpr=4&quality=100&sign=18baea65&sv=2)

We do not normally suggest changing the parameters above, but to elaborate on some of them:

1. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
per_device_train_batch_size = 2,
```





Increase the batch size if you want to utilize the memory of your GPU more. Also increase this to make training more smooth and make the process not over-fit. We normally do not suggest this, since this might make training actually slower due to padding issues. We normally instead ask you to increase `gradient_accumulation_steps` which just does more passes over the dataset.

2. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
gradient_accumulation_steps = 4,
```





Equivalent to increasing the batch size above itself, but does not impact memory consumption! We normally suggest people increasing this if you want smoother training loss curves.

3. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
max_steps = 60, # num_train_epochs = 1,
```





We set steps to 60 for faster training. For full training runs which can take hours, instead comment out `max_steps`, and replace it with `num_train_epochs = 1`. Setting it to 1 means 1 full pass over your dataset. We normally suggest 1 to 3 passes, and no more, otherwise you will over-fit your finetune.

4. Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
learning_rate = 2e-4,
```





Reduce the learning rate if you want to make the finetuning process slower, but also converge to a higher accuracy result most likely. We normally suggest 2e-4, 1e-4, 5e-5, 2e-5 as numbers to try.


![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FxwOA09mtcimcQOCjP4PG%252Fimage.png%3Falt%3Dmedia%26token%3D39a0f525-6d4e-4c3b-af0d-82d8960d87be&width=768&dpr=4&quality=100&sign=853c0062&sv=2)

You will see a log of some numbers! This is the training loss, and your job is to set parameters to make this go to as close to 0.5 as possible! If your finetune is not reaching 1, 0.8 or 0.5, you might have to adjust some numbers. If your loss goes to 0, that's probably not a good sign as well!

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-11.-inference-running-the-model)    11\. Inference / running the model

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FRX9Byv1hlSpvmonT1PLw%252Fimage.png%3Falt%3Dmedia%26token%3D6043cd8c-c6a3-4cc5-a019-48baeed3b5a2&width=768&dpr=4&quality=100&sign=7c7ce43f&sv=2)

Now let's run the model after we completed the training process! You can edit the yellow underlined part! In fact, because we created a multi turn chatbot, we can now also call the model as if it saw some conversations in the past like below:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F6DXSlsHkN8cZiiAxAV0Z%252Fimage.png%3Falt%3Dmedia%26token%3D846307de-7386-4bbe-894e-7d9e572244fe&width=768&dpr=4&quality=100&sign=6482b95b&sv=2)

Reminder Unsloth itself provides **2x faster inference** natively as well, so always do not forget to call `FastLanguageModel.for_inference(model)`. If you want the model to output longer responses, set `max_new_tokens = 128` to some larger number like 256 or 1024. Notice you will have to wait longer for the result as well!

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-12.-saving-the-model)    12\. Saving the model

We can now save the finetuned model as a small 100MB file called a LoRA adapter like below. You can instead push to the Hugging Face hub as well if you want to upload your model! Remember to get a Hugging Face token via [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) and add your token!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FBz0YDi6Sc2oEP5QWXgSz%252Fimage.png%3Falt%3Dmedia%26token%3D33d9e4fd-e7dc-4714-92c5-bfa3b00f86c4&width=768&dpr=4&quality=100&sign=d6933a01&sv=2)

After saving the model, we can again use Unsloth to run the model itself! Use `FastLanguageModel` again to call it for inference!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FzymBQrqwt4GUmCIN0Iec%252Fimage.png%3Falt%3Dmedia%26token%3D41a110e4-8263-426f-8fa7-cdc295cc8210&width=768&dpr=4&quality=100&sign=b2a207c3&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-13.-exporting-to-ollama)    13\. Exporting to Ollama

Finally we can export our finetuned model to Ollama itself! First we have to install Ollama in the Colab notebook:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FqNvGTAGwZKXxkMQqzloS%252Fimage.png%3Falt%3Dmedia%26token%3Ddb503499-0c74-4281-b3bf-400fa20c9ce2&width=768&dpr=4&quality=100&sign=6d57e83a&sv=2)

Then we export the finetuned model we have to llama.cpp's GGUF formats like below:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FZduLjedyfUbTmYqF85pa%252Fimage.png%3Falt%3Dmedia%26token%3Df5bac541-b99f-4d9b-82f7-033f8de780f2&width=768&dpr=4&quality=100&sign=1fdb7647&sv=2)

Reminder to convert `False` to `True` for 1 row, and not change every row to `True`, or else you'll be waiting for a very time! We normally suggest the first row getting set to `True`, so we can export the finetuned model quickly to `Q8_0` format (8 bit quantization). We also allow you to export to a whole list of quantization methods as well, with a popular one being `q4_k_m`.

Head over to [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) to learn more about GGUF. We also have some manual instructions of how to export to GGUF if you want here: [https://github.com/unslothai/unsloth/wiki#manually-saving-to-gguf](https://github.com/unslothai/unsloth/wiki#manually-saving-to-gguf)

You will see a long list of text like below - please wait 5 to 10 minutes!!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FcuUAx0RNtrQACvU7uWCL%252Fimage.png%3Falt%3Dmedia%26token%3Ddc67801a-a363-48e2-8572-4c6d0d8d0d93&width=768&dpr=4&quality=100&sign=cc7f7372&sv=2)

And finally at the very end, it'll look like below:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FxRh07PEQjAmmz3s2HJUP%252Fimage.png%3Falt%3Dmedia%26token%3D3552a3c9-4d4f-49ee-a31e-0a64327419f0&width=768&dpr=4&quality=100&sign=1e9c9f0d&sv=2)

Then, we have to run Ollama itself in the background. We use `subprocess` because Colab doesn't like asynchronous calls, but normally one just runs `ollama serve` in the terminal / command prompt.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FszDuikrg4HY8lGefwpRQ%252Fimage.png%3Falt%3Dmedia%26token%3Dec1c8762-661d-4b13-ab4f-ed1a7b9fda00&width=768&dpr=4&quality=100&sign=fc72e538&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-14.-automatic-modelfile-creation)    14\. Automatic `Modelfile` creation

The trick Unsloth provides is we automatically create a `Modelfile` which Ollama requires! This is a just a list of settings and includes the chat template which we used for the finetune process! You can also print the `Modelfile` generated like below:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fh6inH6k5ggxUP80Gltgj%252Fimage.png%3Falt%3Dmedia%26token%3D805bafb1-2795-4743-9bd2-323ab4f0881e&width=768&dpr=4&quality=100&sign=456e8653&sv=2)

We then ask Ollama to create a model which is Ollama compatible, by using the `Modelfile`

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F1123bSSwmjWXliaRUL5U%252Fimage.png%3Falt%3Dmedia%26token%3D2e72f1a0-1ff8-4189-8d9c-d31e39385555&width=768&dpr=4&quality=100&sign=52a4fd99&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-15.-ollama-inference)    15\. Ollama Inference

And we can now call the model for inference if you want to do call the Ollama server itself which is running on your own local machine / in the free Colab notebook in the background. Remember you can edit the yellow underlined part.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fk5mdsJ57hQ1Ar3KY6VXY%252FInference.png%3Falt%3Dmedia%26token%3D8cf0cbf9-0534-4bae-a887-89f45a3de771&width=768&dpr=4&quality=100&sign=8489fe55&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#id-16.-interactive-chatgpt-style)    16\. Interactive ChatGPT style

But to actually run the finetuned model like a ChatGPT, we have to do a bit more! First click the terminal icon![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FUb17xtyDliAKhJEL9KuH%252Fimage.png%3Falt%3Dmedia%26token%3Df612e9b7-7d05-4039-a476-646026c6c8e6&width=300&dpr=4&quality=100&sign=b1c272f5&sv=2) and a Terminal will pop up. It's on the left sidebar.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FRWPEy4fW8ytOljQYLn55%252FWhere_Terminal.png%3Falt%3Dmedia%26token%3D4ddf3017-2380-4615-958f-a465a76f7bac&width=768&dpr=4&quality=100&sign=32fba259&sv=2)

Then, you might have to press ENTER twice to remove some weird output in the Terminal window. Wait a few seconds and type `ollama run unsloth_model` then hit ENTER.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FL4aLJtoWh3HCkQ6f4J0Q%252FTerminal_Type.png%3Falt%3Dmedia%26token%3D9063f511-1e45-4a44-a9c1-14f0de4e4571&width=768&dpr=4&quality=100&sign=835f2f2&sv=2)

And finally, you can interact with the finetuned model just like an actual ChatGPT! Hit CTRL + D to exit the system, and hit ENTER to converse with the chatbot!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fo3vIehaOLOOBlBGBS7lX%252FAssistant.png%3Falt%3Dmedia%26token%3D25319dd2-384c-4744-a2dd-398f48a3b20f&width=768&dpr=4&quality=100&sign=d95d479&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\#youve-done-it)    You've done it!

You've successfully finetuned a language model and exported it to Ollama with Unsloth 2x faster and with 70% less VRAM! And all this for free in a Google Colab notebook!

If you want to learn how to do reward modelling, do continued pretraining, export to vLLM or GGUF, do text completion, or learn more about finetuning tips and tricks, head over to our [Github](https://github.com/unslothai/unsloth#-finetune-for-free).

If you need any help on finetuning, you can also join our Discord server [here](https://discord.gg/unsloth). If you want help with Ollama, you can also join their server [here](https://discord.gg/ollama).

And finally, we want to thank you for reading and following this far! We hope this made you understand some of the nuts and bolts behind finetuning language models, and we hope this was useful!

To access our Alpaca dataset example click [here](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing), and our CSV / Excel finetuning guide is [here](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing).

[PreviousTutorial: How to Run Gemma 3 effectively](https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively) [NextDatasets 101](https://docs.unsloth.ai/basics/datasets-101)

Last updated 24 days ago

Was this helpful?

* * *

## Unsloth Requirements
## [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements\#system-requirements)    System Requirements

- **Operating System**: Works on Linux and Windows.

- Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.

- If you have different versions of torch, transformers etc., `pip install unsloth ` will automatically install all the latest versions of those libraries so you don't need to worry about version compatibility.

- Your device must have `xformers`, `torch`, `BitsandBytes` and `triton` support.

- Unsloth only works if you have a NVIDIA GPU. Make sure you also have disk space to train & save your model


## [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements\#fine-tuning-vram-requirements)    Fine-tuning VRAM requirements:

How much GPU memory do I need for LLM fine-tuning using Unsloth?

A common issue when you OOM or run out of memory is because you set your batch size too high. Set it to 1, 2, or 3 to use less VRAM.

**For context length benchmarks, see** [**here**](https://docs.unsloth.ai/basics/unsloth-benchmarks#context-length-benchmarks) **.**

Check this table for VRAM requirements sorted by model parameters and fine-tuning method. QLoRA uses 4-bit, LoRA uses 16-bit. Keep in mind that sometimes it may require more VRAM so these numbers are the absolute minimum:

Model parameters

QLoRA (4-bit) VRAM

LoRA (16-bit) VRAM

3B

3.5 GB

8 GB

7B

5 GB

19 GB

8B

6 GB

22 GB

9B

6.5 GB

24 GB

11B

7.5 GB

29 GB

14B

8.5 GB

33 GB

27B

22GB

64GB

32B

26 GB

76 GB

40B

30GB

96GB

70B

41 GB

164 GB

81B

48GB

192GB

90B

53GB

212GB

405B

237 GB

950 GB

[PreviousBeginner? Start here!](https://docs.unsloth.ai/get-started/beginner-start-here) [NextFAQ + Is Fine-tuning Right For Me?](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me)

Last updated 1 day ago

Was this helpful?

* * *

## AI Model Collection
See the table below for all GGUF, 16-bit and 4-bit bnb uploaded models on [Hugging Face](https://huggingface.co/unsloth).

- GGUFs can be used to run in your favorite places like Ollama, Open WebUI and llama.cpp.

- 4-bit and 16-bit models can be used for inference serving or fine-tuning.


â€¢ GGUF + 4-bit â€¢ 16-bit original

Here's a table of all our GGUF + 4-bit model uploads:

Model

GGUF

Instruct (4-bit)

Base (4-bit)

[Gemma 3](https://docs.unsloth.ai/) (new)

- [1B](https://huggingface.co/unsloth/gemma-3-1b-it-GGUF)

- [4B](https://huggingface.co/unsloth/gemma-3-12b-it-GGUF)

- [12B](https://huggingface.co/unsloth/gemma-2-12b-it-GGUF)

- [27B](https://huggingface.co/unsloth/gemma-3-27b-it-GGUF)


- [1B](https://huggingface.co/unsloth/gemma-3-1b-it-bnb-4bit)

- [4B](https://huggingface.co/unsloth/gemma-3-4b-it-bnb-4bit)

- [12B](https://huggingface.co/unsloth/gemma-3-12b-it-bnb-4bit)

- [27B](https://huggingface.co/unsloth/gemma-3-27b-it-bnb-4bit)


- [1B](https://huggingface.co/unsloth/gemma-3-1b-pt-bnb-4bit)

- [4B](https://huggingface.co/unsloth/gemma-3-4b-pt-bnb-4bit)

- [12B](https://huggingface.co/unsloth/gemma-3-12b-pt-bnb-4bit)

- [27B](https://huggingface.co/unsloth/gemma-3-27b-pt-bnb-4bit)


[DeepSeek-R1](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5) (new)

- [R1](https://huggingface.co/unsloth/DeepSeek-R1-GGUF)

- [R1 Zero](https://huggingface.co/unsloth/DeepSeek-R1-Zero-GGUF)

- [Llama 3 (8B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF)

- [Llama 3.3 (70B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF)

- [Qwen 2.5 (14B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF)

- [Qwen 2.5 (32B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF)

- [Qwen 2.5 (1.5B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF)

- [Qwen 2.5 (7B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF)


- [Llama 3 (8B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit)

- [Llama 3.3 (70B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-bnb-4bit)

- [Qwen 2.5 (14B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit)

- [Qwen 2.5 (32B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit)

- [Qwen 2.5 (1.5B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit)

- [Qwen 2.5 (7B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit)


[Phi-4](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa) (new)

- [Phi-4](https://huggingface.co/unsloth/phi-4-GGUF)

- [Phi-4-mini](https://huggingface.co/unsloth/Phi-4-mini-instruct-GGUF)


- [Phi-4](https://huggingface.co/unsloth/phi-4-unsloth-bnb-4bit)

- [Phi-4-mini](https://huggingface.co/unsloth/Phi-4-mini-instruct-unsloth-bnb-4bit)


[QwQ-32B](https://huggingface.co/collections/unsloth/qwen-qwq-32b-collection-676b3b29c20c09a8c71a6235) (new)

- [32B](https://huggingface.co/unsloth/QwQ-32B-GGUF)


- [32B](https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit)


[Llama 3.2](https://huggingface.co/collections/unsloth/llama-32-66f46afde4ca573864321a22)

- [1B](https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF)

- [3B](https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-GGUF)


- [1B](https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-bnb-4bit)

- [3B](https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-bnb-4bit)

- [11B Vision](https://huggingface.co/unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit)

- [90B Vision](https://huggingface.co/unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit)


- [1B](https://huggingface.co/unsloth/Llama-3.2-1B-bnb-4bit)

- [3B](https://huggingface.co/unsloth/Llama-3.2-3B-bnb-4bit)

- [11B Vision](https://huggingface.co/unsloth/Llama-3.2-11B-Vision-unsloth-bnb-4bit)

- [90B Vision](https://huggingface.co/unsloth/Llama-3.2-90B-Vision-bnb-4bit)


[Llama 3.3](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f)

- [70B](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-GGUF)


- [70B](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-bnb-4bit)


[Llama 3.1](https://huggingface.co/collections/unsloth/llama-31-collection-6753dca76f47d9ce1696495f)

- [8B](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit)

- [70B](https://huggingface.co/unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit)

- [405B](https://huggingface.co/unsloth/Meta-Llama-3.1-405B-Instruct-bnb-4bit/)


- [8B](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit)

- [70B](https://huggingface.co/unsloth/Meta-Llama-3.1-70B-bnb-4bit)

- [405B](https://huggingface.co/unsloth/Meta-Llama-3.1-405B-bnb-4bit)


Mistral

- [Small 2501 (24B)](https://huggingface.co/unsloth/Mistral-Small-24B-Instruct-2501-GGUF) \- new

- [NeMo 2407 (12B)](https://huggingface.co/unsloth/Mistral-Nemo-Instruct-2407-GGUF)


- [Small 2501 (24B)](https://huggingface.co/unsloth/Mistral-Small-24B-Instruct-2501-unsloth-bnb-4bit) \- new

- [NeMo 2407 (12B)](https://huggingface.co/unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit)

- [Small 2409 (22B)](https://huggingface.co/unsloth/Mistral-Small-Instruct-2409-bnb-4bit)

- [Large 2407](https://huggingface.co/unsloth/Mistral-Large-Instruct-2407-bnb-4bit)

- [7B (v0.3)](https://huggingface.co/unsloth/mistral-7b-instruct-v0.3-bnb-4bit)

- [7B (v0.2)](https://huggingface.co/unsloth/mistral-7b-instruct-v0.2-bnb-4bit)

- [Pixtral (12B) 2409](https://huggingface.co/unsloth/Pixtral-12B-2409-bnb-4bit)


- [Small 2501 (24B)](https://huggingface.co/unsloth/Mistral-Small-24B-Base-2501-unsloth-bnb-4bit) \- new

- [NeMo 2407 (12B)](https://huggingface.co/unsloth/Mistral-Nemo-Base-2407-bnb-4bit)

- [7B (v0.3)](https://huggingface.co/unsloth/mistral-7b-v0.3-bnb-4bit)

- [7B (v0.2)](https://huggingface.co/unsloth/mistral-7b-v0.2-bnb-4bit)

- [Pixtral (12B) 2409](https://huggingface.co/unsloth/Pixtral-12B-2409-unsloth-bnb-4bit)


[DeepSeek V3](https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c)

- [2 to 8-bit](https://huggingface.co/unsloth/DeepSeek-V3-GGUF)


[Qwen2.5-VL](https://huggingface.co/collections/unsloth/qwen25-vl-all-versions-679ca6c784fad5bd976a05a1) (new)

- [3B](https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit)

- [7B](https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit)

- [72B](https://huggingface.co/unsloth/Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit)


- [3B](https://huggingface.co/unsloth/Qwen2.5-VL-3B-Instruct)

- [7B](https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct)

- [72B](https://huggingface.co/unsloth/Qwen2.5-VL-72B-Instruct)


[Qwen 2.5](https://huggingface.co/collections/unsloth/qwen-25-66fe4c08fb9ada518e8a0d3f)

- [0.5B](https://huggingface.co/unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit)

- [1.5B](https://huggingface.co/unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit)

- [3B](https://huggingface.co/unsloth/Qwen2.5-3B-Instruct-bnb-4bit)

- [7B](https://huggingface.co/unsloth/Qwen2.5-7B-Instruct-bnb-4bit)

- [14B](https://huggingface.co/unsloth/Qwen2.5-14B-Instruct-bnb-4bit)

- [32B](https://huggingface.co/unsloth/Qwen2.5-32B-Instruct-bnb-4bit)

- [72B](https://huggingface.co/unsloth/Qwen2-72B-Instruct-bnb-4bit)

- [QwQ](https://huggingface.co/unsloth/QwQ-32B-Preview-unsloth-bnb-4bit)

- [QVQ](https://huggingface.co/unsloth/QVQ-72B-Preview-bnb-4bit)


- [0.5B](https://huggingface.co/unsloth/Qwen2.5-0.5B-bnb-4bit)

- [1.5B](https://huggingface.co/unsloth/Qwen2.5-1.5B-bnb-4bit)

- [3B](https://huggingface.co/unsloth/Qwen2.5-3B-bnb-4bit)

- [7B](https://huggingface.co/unsloth/Qwen2.5-7B-bnb-4bit)

- [14B](https://huggingface.co/unsloth/Qwen2.5-14B-bnb-4bit)

- [32B](https://huggingface.co/unsloth/Qwen2.5-32B-bnb-4bit)

- [72B](https://huggingface.co/unsloth/Qwen2.5-72B-bnb-4bit)


Gemma 2

- [All variants](https://huggingface.co/unsloth/gemma-2-it-GGUF)


- [2B](https://huggingface.co/unsloth/gemma-2-2b-it-bnb-4bit)

- [9B](https://huggingface.co/unsloth/gemma-2-9b-it-bnb-4bit)

- [27B](https://huggingface.co/unsloth/gemma-2-27b-it-bnb-4bit)


- [2B](https://huggingface.co/unsloth/gemma-2-2b-bnb-4bit)

- [9B](https://huggingface.co/unsloth/gemma-2-9b-bnb-4bit)

- [27B](https://huggingface.co/unsloth/gemma-2-27b-bnb-4bit)


Phi-3.5

- [mini](https://huggingface.co/unsloth/Phi-3.5-mini-instruct-bnb-4bit)


Phi-3

- [mini](https://huggingface.co/unsloth/Phi-3-mini-4k-instruct-bnb-4bit)

- [medium](https://huggingface.co/unsloth/Phi-3-medium-4k-instruct-bnb-4bit)


Llama 3

- [8B](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)

- [70B](https://huggingface.co/unsloth/llama-3-70b-bnb-4bit)


- [8B](https://huggingface.co/unsloth/llama-3-8b-bnb-4bit)

- [70B](https://huggingface.co/unsloth/llama-3-70b-bnb-4bit)


Llava

- [1.5 (7B)](https://huggingface.co/unsloth/llava-1.5-7b-hf-bnb-4bit)

- [1.6 Mistral (7B)](https://huggingface.co/unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit)


[Qwen 2.5 Coder](https://huggingface.co/collections/unsloth/qwen-25-coder-6732bc833ed65dd1964994d4)

- [0.5B](https://huggingface.co/unsloth/Qwen2.5-Coder-0.5B-Instruct-128K-GGUF)

- [1.5B](https://huggingface.co/unsloth/Qwen2.5-Coder-1.5B-Instruct-128K-GGUF)

- [3B](https://huggingface.co/unsloth/Qwen2.5-Coder-3B-Instruct-128K-GGUF)

- [7B](https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF)

- [14B](https://huggingface.co/unsloth/Qwen2.5-Coder-14B-Instruct-128K-GGUF)

- [32B](https://huggingface.co/unsloth/Qwen2.5-Coder-32B-Instruct-128K-GGUF)


- [0.5B](https://huggingface.co/unsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit)

- [1.5B](https://huggingface.co/unsloth/Qwen2.5-Coder-1.5B-Instruct-bnb-4bit)

- [3B](https://huggingface.co/unsloth/Qwen2.5-Coder-3B-Instruct-bnb-4bit)

- [7B](https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit)

- [14B](https://huggingface.co/unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit)

- [32B](https://huggingface.co/unsloth/Qwen2.5-Coder-32B-Instruct-bnb-4bit)


- [0.5B](https://huggingface.co/unsloth/Qwen2.5-Coder-0.5B-bnb-4bit)

- [1.5B](https://huggingface.co/unsloth/Qwen2.5-Coder-1.5B-bnb-4bit)

- [3B](https://huggingface.co/unsloth/Qwen2.5-Coder-3B-bnb-4bit)

- [7B](https://huggingface.co/unsloth/Qwen2.5-Coder-7B-bnb-4bit)

- [14B](https://huggingface.co/unsloth/Qwen2.5-Coder-32B-bnb-4bit)

- [32B](https://huggingface.co/unsloth/Qwen2.5-Coder-32B-bnb-4bit)


Qwen2 VL

- [2B](https://huggingface.co/unsloth/Qwen2-VL-2B-Instruct-unsloth-bnb-4bit)

- [7B](https://huggingface.co/unsloth/Qwen2-VL-7B-Instruct-unsloth-bnb-4bit)

- [72B](https://huggingface.co/unsloth/Qwen2-VL-72B-Instruct-bnb-4bit)


Llama 2

- [7B](https://huggingface.co/unsloth/llama-2-7b-chat-bnb-4bit)


- [7B](https://huggingface.co/unsloth/llama-2-7b-bnb-4bit)

- [13B](https://huggingface.co/unsloth/llama-2-13b-bnb-4bit)


SmolLM2

- [135M](https://huggingface.co/unsloth/SmolLM2-135M-Instruct-GGUF)

- [360M](https://huggingface.co/unsloth/SmolLM2-360M-Instruct-GGUF)

- [1.7B](https://huggingface.co/unsloth/SmolLM2-1.7B-Instruct-GGUF)


- [135M](https://huggingface.co/unsloth/SmolLM2-135M-Instruct-bnb-4bit)

- [360M](https://huggingface.co/unsloth/SmolLM2-360M-Instruct-bnb-4bit)

- [1.7B](https://huggingface.co/unsloth/SmolLM2-1.7B-Instruct-bnb-4bit)


- [135M](https://huggingface.co/unsloth/SmolLM2-135M-bnb-4bit)

- [360M](https://huggingface.co/unsloth/SmolLM2-360M-bnb-4bit)

- [1.7B](https://huggingface.co/unsloth/SmolLM2-1.7B-bnb-4bit)


TinyLlama

- [Instruct](https://huggingface.co/unsloth/tinyllama-chat-bnb-4bit)


- [Base](https://huggingface.co/unsloth/tinyllama-bnb-4bit)


Qwen2

- [1.5B](https://huggingface.co/unsloth/Qwen2-1.5B-Instruct-bnb-4bit)

- [7B](https://huggingface.co/unsloth/Qwen2-7B-Instruct-bnb-4bit)

- [72B](https://huggingface.co/unsloth/Qwen2-72B-Instruct-bnb-4bit)


- [1.5B](https://huggingface.co/unsloth/Qwen2-1.5B-bnb-4bit)

- [7B](https://huggingface.co/unsloth/Qwen2-7B-bnb-4bit)

- [72B](https://huggingface.co/unsloth/Qwen2-7B-bnb-4bit)


Zephyr SFT

- [Instruct](https://huggingface.co/unsloth/zephyr-sft-bnb-4bit)


CodeLlama

- [7B](https://huggingface.co/unsloth/codellama-7b-bnb-4bit)

- [13B](https://huggingface.co/unsloth/codellama-13b-bnb-4bit)

- [34B](https://huggingface.co/unsloth/codellama-34b-bnb-4bit)


Yi

- [34B](https://huggingface.co/unsloth/yi-34b-chat-bnb-4bit)


- [6B (v 1.5)](https://huggingface.co/unsloth/Yi-1.5-6B-bnb-4bit)

- [6B](https://huggingface.co/unsloth/yi-6b-bnb-4bit)

- [34B](https://huggingface.co/unsloth/yi-34b-bnb-4bit)


Here's a table of all our 16-bit or 8-bit original model uploads:

Model

Instruct

Base

[Gemma 3](https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b) (new)

- [1B](https://huggingface.co/unsloth/gemma-3-1b-it)

- [4B](https://huggingface.co/unsloth/gemma-3-4b-it)

- [12B](https://huggingface.co/unsloth/gemma-3-12b-it)

- [27B](https://huggingface.co/unsloth/gemma-3-27b-it)


- [1B](https://huggingface.co/unsloth/gemma-3-1b-pt)

- [4B](https://huggingface.co/unsloth/gemma-3-4b-pt)

- [12B](https://huggingface.co/unsloth/gemma-3-12b-pt)

- [27B](https://huggingface.co/unsloth/gemma-3-27b-pt)


[DeepSeek-R1 (new)](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5)

- [R1](https://huggingface.co/unsloth/DeepSeek-R1)

- [R1 Zero](https://huggingface.co/unsloth/DeepSeek-R1-Zero)

- [Llama 3 (8B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B)

- [Llama 3.3 (70B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B)

- [Qwen 2.5 (14B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B)

- [Qwen 2.5 (32B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B)

- [Qwen 2.5 (1.5B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B)

- [Qwen 2.5 (7B)](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B)

- [R1 (bf16)](https://huggingface.co/unsloth/DeepSeek-R1-BF16)


[Llama 3.2](https://huggingface.co/collections/unsloth/llama-32-66f46afde4ca573864321a22)

- [1B](https://huggingface.co/unsloth/Llama-3.2-1B-Instruct)

- [3B](https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-bnb-4bit)

- [11B Vision](https://huggingface.co/unsloth/Llama-3.2-11B-Vision-Instruct)

- [90B Vision](https://huggingface.co/unsloth/Llama-3.2-90B-Vision-Instruct)


- [1B](https://huggingface.co/unsloth/Llama-3.2-1B)

- [3B](https://huggingface.co/unsloth/Llama-3.2-3B)

- [11B Vision](https://huggingface.co/unsloth/Llama-3.2-11B-Vision)

- [90B Vision](https://huggingface.co/unsloth/Llama-3.2-90B-Vision)


[Llama 3.3](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f)

- [70B](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct)


[Phi-4](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa) (new)

- [Phi-4](https://huggingface.co/unsloth/phi-4)

- [Phi-4-mini](https://huggingface.co/unsloth/Phi-4-mini-instruct)


[Llama 3.1](https://huggingface.co/collections/unsloth/llama-31-collection-6753dca76f47d9ce1696495f)

- [8B](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct)

- [70B](https://huggingface.co/unsloth/Meta-Llama-3.1-70B-Instruct)


- [8B](https://huggingface.co/unsloth/Meta-Llama-3.1-8B)

- [70B](https://huggingface.co/unsloth/Meta-Llama-3.1-70B)


Mistral

- [Mistral Small 2501 (24B) - new](https://huggingface.co/unsloth/Mistral-Small-24B-Instruct-2501)

- [NeMo 2407 (12B)](https://huggingface.co/unsloth/Mistral-Nemo-Instruct-2407)

- [Small 2409 (22B)](https://huggingface.co/unsloth/Mistral-Small-Instruct-2409)

- [7B (v0.3)](https://huggingface.co/unsloth/mistral-7b-instruct-v0.3)

- [7B (v0.2)](https://huggingface.co/unsloth/mistral-7b-instruct-v0.2)

- [Pixtral (12B) 2409](https://huggingface.co/unsloth/Pixtral-12B-2409)


- [Mistral Small 2501 (24B) - new](https://huggingface.co/unsloth/Mistral-Small-24B-Base-2501)

- [NeMo 2407 (12B)](https://huggingface.co/unsloth/Mistral-Nemo-Base-2407)

- [7B (v0.3)](https://huggingface.co/unsloth/mistral-7b-v0.3)

- [7B (v0.2)](https://huggingface.co/unsloth/mistral-7b-v0.2)

- [Pixtral (12B) 2409](https://huggingface.co/unsloth/Pixtral-12B-Base-2409)


Gemma 2

- [2B](https://huggingface.co/unsloth/gemma-2b-it)

- [9B](https://huggingface.co/unsloth/gemma-9b-it)

- [27B](https://huggingface.co/unsloth/gemma-27b-it)


- [2B](https://huggingface.co/unsloth/gemma-2-2b)

- [9B](https://huggingface.co/unsloth/gemma-2-9b)

- [27B](https://huggingface.co/unsloth/gemma-2-27b)


DeepSeek V3

- [bf16](https://huggingface.co/unsloth/DeepSeek-V3-bf16)

- [original 8-bit](https://huggingface.co/unsloth/DeepSeek-V3)


Phi-3.5

- [mini](https://huggingface.co/unsloth/Phi-3.5-mini-instruct)


Phi-3

- [mini](https://huggingface.co/unsloth/Phi-3-mini-4k-instruct)

- [medium](https://huggingface.co/unsloth/Phi-3-medium-4k-instruct)


Llama 3

- [8B](https://huggingface.co/unsloth/llama-3-8b-Instruct)


- [8B](https://huggingface.co/unsloth/llama-3-8b)


[Qwen 2.5](https://huggingface.co/collections/unsloth/qwen-25-66fe4c08fb9ada518e8a0d3f)

- [0.5B](https://huggingface.co/unsloth/Qwen2.5-0.5B-Instruct)

- [1.5B](https://huggingface.co/unsloth/Qwen2.5-1.5B-Instruct)

- [3B](https://huggingface.co/unsloth/Qwen2.5-3B-Instruct)

- [7B](https://huggingface.co/unsloth/Qwen2.5-7B-Instruct)

- [14B](https://huggingface.co/unsloth/Qwen2.5-14B-Instruct)

- [32B](https://huggingface.co/unsloth/Qwen2.5-32B-Instruct)

- [72B](https://huggingface.co/unsloth/Qwen2.5-72B-Instruct)


- [0.5B](https://huggingface.co/unsloth/Qwen2.5-0.5B)

- [1.5B](https://huggingface.co/unsloth/Qwen2.5-1.5B)

- [3B](https://huggingface.co/unsloth/Qwen2.5-3B)

- [7B](https://huggingface.co/unsloth/Qwen2.5-7B)

- [14B](https://huggingface.co/unsloth/Qwen2.5-14B)

- [32B](https://huggingface.co/unsloth/Qwen2.5-32B)

- [72B](https://huggingface.co/unsloth/Qwen2.5-72B)


Llava

- [1.5 (7B)](https://huggingface.co/unsloth/llava-1.5-7b-hf)

- [1.6 Mistral (7B)](https://huggingface.co/unsloth/llava-v1.6-mistral-7b-hf)


Qwen2 VL

- [2B](https://huggingface.co/unsloth/Qwen2-VL-2B-Instruct)

- [7B](https://huggingface.co/unsloth/Qwen2-VL-7B-Instruct)

- [72B](https://huggingface.co/unsloth/Qwen2-VL-72B-Instruct)


Llama 2

- [7B](https://huggingface.co/unsloth/llama-2-7b-chat)


- [7B](https://huggingface.co/unsloth/llama-2-7b)

- [13B](https://huggingface.co/unsloth/llama-2-13b)


SmolLM2

- [135M](https://huggingface.co/unsloth/SmolLM2-135M-Instruct)

- [360M](https://huggingface.co/unsloth/SmolLM2-360M-Instruct)

- [1.7B](https://huggingface.co/unsloth/SmolLM2-1.7B-Instruct)


- [135M](https://huggingface.co/unsloth/SmolLM2-135M)

- [360M](https://huggingface.co/unsloth/SmolLM2-360M)

- [1.7B](https://huggingface.co/unsloth/SmolLM2-1.7B)


TinyLlama

- [Instruct](https://huggingface.co/unsloth/tinyllama-chat)


- [Base](https://huggingface.co/unsloth/tinyllama)


Qwen2

- [1.5B](https://huggingface.co/unsloth/Qwen2-1.5B-Instruct)

- [7B](https://huggingface.co/unsloth/Qwen2-7B-Instruct)


- [1.5B](https://huggingface.co/unsloth/Qwen2-1.5B)

- [7B](https://huggingface.co/unsloth/Qwen2-7B)


Zephyr SFT

- [Instruct](https://huggingface.co/unsloth/zephyr-sft)


[PreviousUnsloth Notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) [NextInstalling + Updating](https://docs.unsloth.ai/get-started/installing-+-updating)

Last updated 2 hours ago

Was this helpful?

* * *

## Unsloth Pip Installation Guide
## [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/pip-install\#recommended-installation)    **Recommended installation:**

**Install with pip (recommended)** for Linux devices:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install unsloth
```

* * *

## [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/pip-install\#advanced-pip-installation)    Advanced Pip Installation

Do **NOT** use this if you have [Conda](https://docs.unsloth.ai/get-started/installing-+-updating/conda-install).

Pip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5` and CUDA versions.

For other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.

For example, if you have `torch 2.4` and `CUDA 12.1`, use:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install --upgrade pip
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
```

Another example, if you have `torch 2.5` and `CUDA 12.4`, use:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install --upgrade pip
pip install "unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git"
```

And other examples:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install "unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
```

Or, run the below in a terminal to get the **optimal** pip installation command:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
```

Or, run the below manually in a Python REPL:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
try: import torch
except: raise ImportError('Install torch via `pip install torch`')
from packaging.version import Version as V
v = V(torch.__version__)
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] >= 8
if cuda != "12.1" and cuda != "11.8" and cuda != "12.4": raise RuntimeError(f"CUDA = {cuda} not supported!")
if   v <= V('2.1.0'): raise RuntimeError(f"Torch = {v} too old!")
elif v <= V('2.1.1'): x = 'cu{}{}-torch211'
elif v <= V('2.1.2'): x = 'cu{}{}-torch212'
elif v  < V('2.3.0'): x = 'cu{}{}-torch220'
elif v  < V('2.4.0'): x = 'cu{}{}-torch230'
elif v  < V('2.5.0'): x = 'cu{}{}-torch240'
elif v  < V('2.6.0'): x = 'cu{}{}-torch250'
else: raise RuntimeError(f"Torch = {v} too new!")
x = x.format(cuda.replace(".", ""), "-ampere" if is_ampere else "")
print(f'pip install --upgrade pip && pip install "unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git"')
```

[PreviousUpdating](https://docs.unsloth.ai/get-started/installing-+-updating/updating) [NextWindows Installation](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation)

Last updated 9 days ago

Was this helpful?

* * *

## Unsloth Installation Guide
Only use Conda if you have it. If not, use [Pip](https://docs.unsloth.ai/get-started/installing-+-updating/pip-install).

Select either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install unsloth
```

If you're looking to install Conda in a Linux environment, [read here](https://docs.anaconda.com/miniconda/), or run the below:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh
```

[PreviousWindows Installation](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation) [NextGoogle Colab](https://docs.unsloth.ai/get-started/installing-+-updating/google-colab)

Last updated 9 days ago

Was this helpful?

* * *

## Saving Models to GGUF
LocallyManual Saving

To save to GGUF, use the below to save locally:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
model.save_pretrained_gguf("dir", tokenizer, quantization_method = "q4_k_m")
model.save_pretrained_gguf("dir", tokenizer, quantization_method = "q8_0")
model.save_pretrained_gguf("dir", tokenizer, quantization_method = "f16")
```

For to push to hub:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
model.push_to_hub_gguf("hf_username/dir", tokenizer, quantization_method = "q4_k_m")
model.push_to_hub_gguf("hf_username/dir", tokenizer, quantization_method = "q8_0")
```

All supported quantization options for `quantization_method` are listed below:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
# https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/quantize.cpp#L19
# From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html
ALLOWED_QUANTS = \
{
    "not_quantized"  : "Recommended. Fast conversion. Slow inference, big files.",
    "fast_quantized" : "Recommended. Fast conversion. OK inference, OK file size.",
    "quantized"      : "Recommended. Slow conversion. Fast inference, small files.",
    "f32"     : "Not recommended. Retains 100% accuracy, but super slow and memory hungry.",
    "f16"     : "Fastest conversion + retains 100% accuracy. Slow and memory hungry.",
    "q8_0"    : "Fast conversion. High resource use, but generally acceptable.",
    "q4_k_m"  : "Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K",
    "q5_k_m"  : "Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K",
    "q2_k"    : "Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.",
    "q3_k_l"  : "Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K",
    "q3_k_m"  : "Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K",
    "q3_k_s"  : "Uses Q3_K for all tensors",
    "q4_0"    : "Original quant method, 4-bit.",
    "q4_1"    : "Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.",
    "q4_k_s"  : "Uses Q4_K for all tensors",
    "q4_k"    : "alias for q4_k_m",
    "q5_k"    : "alias for q5_k_m",
    "q5_0"    : "Higher accuracy, higher resource usage and slower inference.",
    "q5_1"    : "Even higher accuracy, resource usage and slower inference.",
    "q5_k_s"  : "Uses Q5_K for all tensors",
    "q6_k"    : "Uses Q8_K for all tensors",
    "iq2_xxs" : "2.06 bpw quantization",
    "iq2_xs"  : "2.31 bpw quantization",
    "iq3_xxs" : "3.06 bpw quantization",
    "q3_k_xs" : "3-bit extra small quantization",
}
```

First save your model to 16bit:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
model.save_pretrained_merged("merged_model", tokenizer, save_method = "merged_16bit",)
```

Then use the terminal and do:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
git clone --recursive https://github.com/ggerganov/llama.cpp
make clean -C llama.cpp
make all -j -C llama.cpp
pip install gguf protobuf

python llama.cpp/convert-hf-to-gguf.py FOLDER --outfile OUTPUT --outtype f16
```

Or follow the steps at https://rentry.org/llama-cpp-conversions#merging-loras-into-a-model using the model name "merged\_model" to merge to GGUF.

[PreviousRunning & Saving Models](https://docs.unsloth.ai/basics/running-and-saving-models) [NextSaving to Ollama](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama)

Last updated 8 months ago

Was this helpful?

* * *

## Fine-tuning from Checkpoints
You must edit the `Trainer` first to add `save_strategy` and `save_steps`. Below saves a checkpoint every 50 steps to the folder `outputs`.

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
trainer = SFTTrainer(
    ....
    args = TrainingArguments(
        ....
        output_dir = "outputs",
        save_strategy = "steps",
        save_steps = 50,
    ),
)
```

Then in the trainer do:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
trainer_stats = trainer.train(resume_from_checkpoint = True)
```

Which will start from the latest checkpoint and continue training.

## [Direct link to heading](https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint\#wandb-integration)    Wandb Integration

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
# Install library
!pip install wandb --upgrade

# Setting up Wandb
!wandb login <token>

import os

os.environ["WANDB_PROJECT"] = "<name>"
os.environ["WANDB_LOG_MODEL"] = "checkpoint"
```

Then in `TrainingArguments()` set

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
report_to = "wandb",
logging_steps = 1, # Change if needed
save_steps = 100 # Change if needed
run_name = "<name>" # (Optional)
```

To train the model, do `trainer.train()`; to resume training, do

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
import wandb
run = wandb.init()
artifact = run.use_artifact('<username>/<Wandb-project-name>/<run-id>', type='model')
artifact_dir = artifact.download()
trainer.train(resume_from_checkpoint=artifact_dir)
```

[PreviousVision Fine-tuning](https://docs.unsloth.ai/basics/vision-fine-tuning) [NextErrors/Troubleshooting](https://docs.unsloth.ai/basics/errors-troubleshooting)

Last updated 8 months ago

Was this helpful?

* * *

## Run DeepSeek-R1 Locally
A guide on how you can run our 1.58-bit Dynamic Quants for DeepSeek-R1 using llama.cpp.

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally\#using-llama.cpp-recommended)    Using llama.cpp (recommended)

1. Do not forget about `<ï½œUserï½œ>` and `<ï½œAssistantï½œ>` tokens! - Or use a chat template formatter

2. Obtain the latest `llama.cpp` at: [github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp). You can follow the build instructions below as well:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
apt-get update
apt-get install build-essential cmake curl libcurl4-openssl-dev -y
git clone https://github.com/ggerganov/llama.cpp
cmake llama.cpp -B llama.cpp/build \
    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON
cmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split
cp llama.cpp/build/bin/llama-* llama.cpp
```

1. It's best to use `--min-p 0.05` to counteract very rare token predictions - I found this to work well especially for the 1.58bit model.

2. Download the model via:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
# pip install huggingface_hub hf_transfer
# import os # Optional for faster downloading
# os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

from huggingface_hub import snapshot_download
snapshot_download(
  repo_id = "unsloth/DeepSeek-R1-GGUF",
  local_dir = "DeepSeek-R1-GGUF",
  allow_patterns = ["*UD-IQ1_S*"], # Select quant type UD-IQ1_S for 1.58bit
)
```

1. Example with Q4\_0 K quantized cache **Notice -no-cnv disables auto conversation mode**


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
   ./llama.cpp/llama-cli \
	  --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
	  --cache-type-k q4_0 \
	  --threads 12 -no-cnv --prio 2 \
	  --temp 0.6 \
	  --ctx-size 8192 \
	  --seed 3407 \
	  --prompt "<ï½œUserï½œ>Create a Flappy Bird game in Python.<ï½œAssistantï½œ>"
```

Example output:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
 <think>
 Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.
 Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.
 Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.
 I also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.
 Is there a scenario where 1 plus 1 wouldn't be 2? I can't think of any...
```

1. If you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
  ./llama.cpp/llama-cli \
    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --cache-type-k q4_0 \
    --threads 12 -no-cnv --prio 2 \
    --n-gpu-layers 7 \
    --temp 0.6 \
    --ctx-size 8192 \
    --seed 3407 \
    --prompt "<ï½œUserï½œ>Create a Flappy Bird game in Python.<ï½œAssistantï½œ>"
```

1. If you want to merge the weights together, use this script:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
./llama.cpp/llama-gguf-split --merge \
    DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    merged_file.gguf
```

1. DeepSeek R1 has 61 layers. For example with a 24GB GPU or 80GB GPU, you can expect to offload after rounding down (reduce by 1 if it goes out of memory):


Quant

File Size

24GB GPU

80GB GPU

2x80GB GPU

1.58bit

131GB

7

33

All layers 61

1.73bit

158GB

5

26

57

2.22bit

183GB

4

22

49

2.51bit

212GB

2

19

32

### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally\#running-on-mac-apple-devices)    Running on Mac / Apple devices

For Apple Metal devices, be careful of --n-gpu-layers. If you find the machine going out of memory, reduce it. For a 128GB unified memory machine, you should be able to offload 59 layers or so.

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
./llama.cpp/llama-cli \
    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --cache-type-k q4_0 \
    --threads 16 \
    --prio 2 \
    --temp 0.6 \
    --ctx-size 8192 \
    --seed 3407 \
    --n-gpu-layers 59 \
    -no-cnv \
    --prompt "<ï½œUserï½œ>Create a Flappy Bird game in Python.<ï½œAssistantï½œ>"
```

### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally\#run-in-ollama-open-webui)    Run in Ollama/Open WebUI

Open WebUI has made an step-by-step tutorial on how to run R1 here: [docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/](https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/)

If you want to use Ollama for inference on GGUFs, you need to first merge the 3 GGUF split files into 1 like the code below. Then you will need to run the model locally.

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
./llama.cpp/llama-gguf-split --merge \
  DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
	merged_file.gguf
```

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally\#deepseek-chat-template)    DeepSeek Chat Template

All distilled versions and the main 671B R1 model use the same chat template:

`<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What is 1+1?<ï½œAssistantï½œ>It's 2.<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>Explain more!<ï½œAssistantï½œ>`

A BOS is forcibly added, and an EOS separates each interaction. To counteract double BOS tokens during inference, you should only call _tokenizer.encode(..., add\_special\_tokens = False)_ since the chat template auto adds a BOS token as well.
For llama.cpp / GGUF inference, you should skip the BOS since itâ€™ll auto add it.

`<ï½œUserï½œ>What is 1+1?<ï½œAssistantï½œ>`

The <think> and </think> tokens get their own designated tokens. For the distilled versions for Qwen and Llama, some tokens are re-mapped, whilst Qwen for example did not have a BOS token, so <\|object\_ref\_start\|> had to be used instead.
**Tokenizer ID Mappings:**

Token

R1

Distill Qwen

Distill Llama

<think>

128798

151648

128013

</think>

128799

151649

128014

<\|begin\_of\_sentence\|>

0

151646

128000

<\|end\_of\_sentence\|>

1

151643

128001

<\|User\|>

128803

151644

128011

<\|Assistant\|>

128804

151645

128012

Padding token

2

151654

128004

Original tokens in models:

Token

Qwen 2.5 32B Base

Llama 3.3 70B Instruct

<think>

<\|box\_start\|>

<\|reserved\_special\_token\_5\|>

</think>

<\|box\_end\|>

<\|reserved\_special\_token\_6\|>

<ï½œbeginâ–ofâ–sentenceï½œ>

<\|object\_ref\_start\|>

<\|begin\_of\_text\|>

<ï½œendâ–ofâ–sentenceï½œ>

<\|endoftext\|>

<\|end\_of\_text\|>

<ï½œUserï½œ>

<\|im\_start\|>

<\|reserved\_special\_token\_3\|>

<ï½œAssistantï½œ>

<\|im\_end\|>

<\|reserved\_special\_token\_4\|>

Padding token

<\|vision\_pad\|>

<\|finetune\_right\_pad\_id\|>

All Distilled and the original R1 versions seem to have accidentally assigned the padding token to <ï½œendâ–ofâ–sentenceï½œ>, which is mostly not a good idea, especially if you want to further finetune on top of these reasoning models. This will cause endless infinite generations, since most frameworks will mask the EOS token out as -100.

We fixed all distilled and the original R1 versions with the correct padding token (Qwen uses <\|vision\_pad\|>, Llama uses <\|finetune\_right\_pad\_id\|>, and R1 uses <ï½œâ–padâ–ï½œ> or our own added <ï½œPADâ–TOKENï½œ>.

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally\#gguf-r1-table)    GGUF R1 Table

MoE Bits

Type

Disk Size

Accuracy

Link

Details

1.58bit

UD-IQ1\_S

**131GB**

Fair

[Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S)

MoE all 1.56bit. `down_proj` in MoE mixture of 2.06/1.56bit

1.73bit

UD-IQ1\_M

**158GB**

Good

[Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M)

MoE all 1.56bit. `down_proj` in MoE left at 2.06bit

2.22bit

UD-IQ2\_XXS

**183GB**

Better

[Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS)

MoE all 2.06bit. `down_proj` in MoE mixture of 2.5/2.06bit

2.51bit

UD-Q2\_K\_XL

**212GB**

Best

[Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL)

MoE all 2.5bit. `down_proj` in MoE mixture of 3.5/2.5bit

[PreviousReinforcement Learning - DPO, ORPO & KTO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto) [NextDeepSeek-R1 Dynamic 1.58-bit](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit)

Last updated 1 month ago

Was this helpful?

* * *

## Fine-Tuning for LLMs
## [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#understanding-fine-tuning)    Understanding Fine-Tuning

Fine-tuning an LLM customizes its behavior, deepens its domain expertise, and optimizes its performance for specific tasks. By refining a pre-trained model (e.g. _Llama-3.1-8B_) with specialized data, you can:

- **Update Knowledge** â€“ Introduce new, domain-specific information that the base model didnâ€™t originally include.

- **Customize Behavior** â€“ Adjust the modelâ€™s tone, personality, or response style to fit specific needs or a brand voice.

- **Optimize for Tasks** â€“ Improve accuracy and relevance on particular tasks or queries your use-case requires.


Think of fine-tuning as creating a specialized expert out of a generalist model. Some debate whether to use Retrieval-Augmented Generation (RAG) instead of fine-tuning, but fine-tuning can incorporate knowledge and behaviors directly into the model in ways RAG cannot. In practice, combining both approaches yields the best results - leading to greater accuracy, better usability, and fewer hallucinations.

### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#real-world-applications-of-fine-tuning)    Real-World Applications of Fine-Tuning

Fine-tuning can be applied across various domains and needs. Here are a few practical examples of how it makes a difference:

- **Sentiment Analysis for Finance** â€“ Train an LLM to determine if a news headline impacts a company positively or negatively, tailoring its understanding to financial context.

- **Customer Support Chatbots** â€“ Fine-tune on past customer interactions to provide more accurate and personalized responses in a companyâ€™s style and terminology.

- **Legal Document Assistance** â€“ Fine-tune on legal texts (contracts, case law, regulations) for tasks like contract analysis, case law research, or compliance support, ensuring the model uses precise legal language.


## [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#the-benefits-of-fine-tuning)    The Benefits of Fine-Tuning

Fine-tuning offers several notable benefits beyond what a base model or a purely retrieval-based system can provide:

#### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#fine-tuning-vs.-rag-whats-the-difference)    Fine-Tuning vs. RAG: Whatâ€™s the Difference?

Fine-tuning can do mostly everything RAG can - but not the other way around. During training, fine-tuning embeds external knowledge directly into the model. This allows the model to handle niche queries, summarize documents, and maintain context without relying on an outside retrieval system. Thatâ€™s not to say RAG lacks advantages as it is excels at accessing up-to-date information from external databases. It is in fact possible to retrieve fresh data with fine-tuning as well, however it is better to combine RAG with fine-tuning for efficiency.

#### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#task-specific-mastery)    Task-Specific Mastery

Fine-tuning deeply integrates domain knowledge into the model. This makes it highly effective at handling structured, repetitive, or nuanced queries, scenarios where RAG-alone systems often struggle. In other words, a fine-tuned model becomes a specialist in the tasks or content it was trained on.

#### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#independence-from-retrieval)    Independence from Retrieval

A fine-tuned model has no dependency on external data sources at inference time. It remains reliable even if a connected retrieval system fails or is incomplete, because all needed information is already within the modelâ€™s own parameters. This self-sufficiency means fewer points of failure in production.

#### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#faster-responses)    Faster Responses

Fine-tuned models donâ€™t need to call out to an external knowledge base during generation. Skipping the retrieval step means they can produce answers much more quickly. This speed makes fine-tuned models ideal for time-sensitive applications where every second counts.

#### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#custom-behavior-and-tone)    Custom Behavior and Tone

Fine-tuning allows precise control over how the model communicates. This ensures the modelâ€™s responses stay consistent with a brandâ€™s voice, adhere to regulatory requirements, or match specific tone preferences. You get a model that not only knows _what_ to say, but _how_ to say it in the desired style.

#### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#reliable-performance)    Reliable Performance

Even in a hybrid setup that uses both fine-tuning and RAG, the fine-tuned model provides a reliable fallback. If the retrieval component fails to find the right information or returns incorrect data, the modelâ€™s built-in knowledge can still generate a useful answer. This guarantees more consistent and robust performance for your system.

## [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#common-misconceptions)    Common Misconceptions

Despite fine-tuningâ€™s advantages, a few myths persist. Letâ€™s address two of the most common misconceptions about fine-tuning:

### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#does-fine-tuning-add-new-knowledge-to-a-model)    Does Fine-Tuning Add New Knowledge to a Model?

**Yes - it absolutely can.** A common myth suggests that fine-tuning doesnâ€™t introduce new knowledge, but in reality it does. If your fine-tuning dataset contains new domain-specific information, the model will learn that content during training and incorporate it into its responses. In effect, fine-tuning _can and does_ teach the model new facts and patterns from scratch.

### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#is-rag-always-better-than-fine-tuning)    Is RAG Always Better Than Fine-Tuning?

**Not necessarily.** Many assume RAG will consistently outperform a fine-tuned model, but thatâ€™s not the case when fine-tuning is done properly. In fact, a well-tuned model often matches or even surpasses RAG-based systems on specialized tasks. Claims that â€œRAG is always betterâ€ usually stem from fine-tuning attempts that werenâ€™t optimally configured - for example, using incorrect [LoRA parameters](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia) or insufficient training.

Unsloth takes care of these complexities by automatically selecting the best parameter configurations for you. All you need is a good-quality dataset, and you'll get a fine-tuned model that performs to its fullest potential.

### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#is-fine-tuning-expensive)    Is Fine-Tuning Expensive?

**Not at all!** While full fine-tuning or pretraining can be costly, these are not necessary (pretraining is especially not necessary). In most cases, LoRA or QLoRA fine-tuning can be done for minimal cost. In fact, with Unslothâ€™s [free notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) for Colab or Kaggle, you can fine-tune models without spending a dime. Better yet, you can even fine-tune locally on your own device.

## [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#faq)    FAQ:

### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#why-you-should-combine-rag-and-fine-tuning)    Why You Should Combine RAG & Fine-Tuning

Instead of choosing between RAG and fine-tuning, consider using **both** together for the best results. Combining a retrieval system with a fine-tuned model brings out the strengths of each approach. Hereâ€™s why:

- **Task-Specific Expertise** â€“ Fine-tuning excels at specialized tasks or formats (making the model an expert in a specific area), while RAG keeps the model up-to-date with the latest external knowledge.

- **Better Adaptability** â€“ A fine-tuned model can still give useful answers even if the retrieval component fails or returns incomplete information. Meanwhile, RAG ensures the system stays current without requiring you to retrain the model for every new piece of data.

- **Efficiency** â€“ Fine-tuning provides a strong foundational knowledge base within the model, and RAG handles dynamic or quickly-changing details without the need for exhaustive re-training from scratch. This balance yields an efficient workflow and reduces overall compute costs.


### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#lora-vs.-qlora-which-one-to-use)    LoRA vs. QLoRA: Which One to Use?

When it comes to implementing fine-tuning, two popular techniques can dramatically cut down the compute and memory requirements: **LoRA** and **QLoRA**. Hereâ€™s a quick comparison of each:

- **LoRA (Low-Rank Adaptation)** â€“ Fine-tunes only a small set of additional â€œadapterâ€ weight matrices (in 16-bit precision), while leaving most of the original model unchanged. This significantly reduces the number of parameters that need updating during training.

- **QLoRA (Quantized LoRA)** â€“ Combines LoRA with 4-bit quantization of the model weights, enabling efficient fine-tuning of very large models on minimal hardware. By using 4-bit precision where possible, it dramatically lowers memory usage and compute overhead.


We recommend starting with **QLoRA**, as itâ€™s one of the most efficient and accessible methods available. Thanks to Unslothâ€™s [dynamic 4-bit](https://unsloth.ai/blog/dynamic-4bit) quants, the accuracy loss compared to standard 16-bit LoRA fine-tuning is now negligible.

### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me\#experimentation-is-key)    Experimentation is Key

Thereâ€™s no single â€œbestâ€ approach to fine-tuning - only best practices for different scenarios. Itâ€™s important to experiment with different methods and configurations to find what works best for your dataset and use case. A great starting point is **QLoRA (4-bit)**, which offers a very cost-effective, resource-friendly way to fine-tune models without heavy computational requirements.

[ðŸ§ LoRA Parameters Encyclopedia](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia)

[PreviousUnsloth Requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements) [NextWhat Model Should I Use?](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use)

Last updated 1 day ago

Was this helpful?

* * *

## Unsloth AI Inference Guide
* * *

## Continued Pretraining Overview
- The [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for continued pretraining/raw text.

- The [continued pretraining notebook](https://colab.research.google.com/drive/1tEd1FrOXWMnCU9UIvdYhs61tkxdMuKZu?usp=sharing) is for learning another language.


You can read more about continued pretraining and our release in our [blog post](https://unsloth.ai/blog/contpretraining).

## [Direct link to heading](https://docs.unsloth.ai/basics/continued-pretraining\#what-is-continued-pretraining)    What is Continued Pretraining?

Continued or continual pretraining (CPT) is necessary to â€œsteerâ€ the language model to understand new domains of knowledge, or out of distribution domains. Base models like Llama-3 8b or Mistral 7b are first pretrained on gigantic datasets of trillions of tokens (Llama-3 for e.g. is 15 trillion).

But sometimes these models have not been well trained on other languages, or text specific domains, like law, medicine or other areas. So continued pretraining (CPT) is necessary to make the language model learn new tokens or datasets.

## [Direct link to heading](https://docs.unsloth.ai/basics/continued-pretraining\#advanced-features)    Advanced Features:

### [Direct link to heading](https://docs.unsloth.ai/basics/continued-pretraining\#loading-lora-adapters-for-continued-finetuning)    Loading LoRA adapters for continued finetuning

If you saved a LoRA adapter through Unsloth, you can also continue training using your LoRA weights. The optimizer state will be reset as well. To load even optimizer states to continue finetuning, see the next section.

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "LORA_MODEL_NAME",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)
trainer = Trainer(...)
trainer.train()
```

### [Direct link to heading](https://docs.unsloth.ai/basics/continued-pretraining\#continued-pretraining-and-finetuning-the-lm_head-and-embed_tokens-matrices)    Continued Pretraining & Finetuning the `lm_head` and `embed_tokens` matrices

Add `lm_head` and `embed_tokens`. For Colab, sometimes you will go out of memory for Llama-3 8b. If so, just add `lm_head`.

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",\
                      "gate_proj", "up_proj", "down_proj",\
                      "lm_head", "embed_tokens",],
    lora_alpha = 16,
)
```

Then use 2 different learning rates - a 2-10x smaller one for the `lm_head` or `embed_tokens` like so:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
from unsloth import UnslothTrainer, UnslothTrainingArguments

trainer = UnslothTrainer(
    ....
    args = UnslothTrainingArguments(
        ....
        learning_rate = 5e-5,
        embedding_learning_rate = 5e-6, # 2-10x smaller than learning_rate
    ),
)
```

[PreviousInference](https://docs.unsloth.ai/basics/running-and-saving-models/inference) [NextChat Templates](https://docs.unsloth.ai/basics/chat-templates)

Last updated 14 days ago

Was this helpful?

* * *

## Chat Templates Overview
### [Direct link to heading](https://docs.unsloth.ai/basics/chat-templates\#list-of-colab-chat-template-notebooks)    List of Colab chat template notebooks:

- [Conversational](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)

- [ChatML](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)

- [Ollama](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)

- [Text Classification](https://github.com/timothelaborie/text_classification_scripts/blob/main/unsloth_classification.ipynb) by Timotheeee

- [Multiple Datasets](https://colab.research.google.com/drive/1njCCbE1YVal9xC83hjdo2hiGItpY_D6t?usp=sharing) by Flail


## [Direct link to heading](https://docs.unsloth.ai/basics/chat-templates\#multi-turn-conversations)    Multi turn conversations

A bit issue if you didn't notice is the Alpaca dataset is single turn, whilst remember using ChatGPT was interactive and you can talk to it in multiple turns. For example, the left is what we want, but the right which is the Alpaca dataset only provides singular conversations. We want the finetuned language model to somehow learn how to do multi turn conversations just like ChatGPT.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FWCAN7bYUt6QWwCWUxisL%252Fdiff.png%3Falt%3Dmedia%26token%3D29821fd9-2181-4d1d-8b93-749b69bcf400&width=768&dpr=4&quality=100&sign=d4f1b675&sv=2)

So we introduced the `conversation_extension` parameter, which essentially selects some random rows in your single turn dataset, and merges them into 1 conversation! For example, if you set it to 3, we randomly select 3 rows and merge them into 1! Setting them too long can make training slower, but could make your chatbot and final finetune much better!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FWi1rRNBFC2iDmCvSJsZt%252Fcombine.png%3Falt%3Dmedia%26token%3Dbef37a55-b272-4be3-89b5-9767c219a380&width=768&dpr=4&quality=100&sign=ae98ba1b&sv=2)

Then set `output_column_name` to the prediction / output column. For the Alpaca dataset dataset, it would be the output column.

We then use the `standardize_sharegpt` function to just make the dataset in a correct format for finetuning! Always call this!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FE75C4Y848VNF6luLuPRR%252Fimage.png%3Falt%3Dmedia%26token%3Daac1d79b-ecca-4e56-939d-d97dcbbf30eb&width=768&dpr=4&quality=100&sign=d48e3c76&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/chat-templates\#customizable-chat-templates)    Customizable Chat Templates

We can now specify the chat template for finetuning itself. The very famous Alpaca format is below:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F8SWcsgH47Uhkm0IclDs5%252Fimage.png%3Falt%3Dmedia%26token%3Dfa03d7aa-d568-468d-9884-18e925a0551f&width=768&dpr=4&quality=100&sign=dff54efb&sv=2)

But remember we said this was a bad idea because ChatGPT style finetunes require only 1 prompt? Since we successfully merged all dataset columns into 1 using Unsloth, we essentially can create the below style chat template with 1 input column (instruction) and 1 output:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FyuMpSLIpPLEbcdh970UJ%252Fimage.png%3Falt%3Dmedia%26token%3D87c4d5e1-accf-4847-9971-63e3a47b4a5f&width=768&dpr=4&quality=100&sign=728095c1&sv=2)

We just require you must put a `{INPUT}` field for the instruction and an `{OUTPUT}` field for the model's output field. We in fact allow an optional `{SYSTEM}` field as well which is useful to customize a system prompt just like in ChatGPT. For example, below are some cool examples which you can customize the chat template to be:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fi6B8IP1OZmmxBYr6k4W3%252Fimage.png%3Falt%3Dmedia%26token%3D061d1b4c-4b22-4d1b-a423-8d4c15e40efa&width=768&dpr=4&quality=100&sign=dd8c7435&sv=2)

For the ChatML format used in OpenAI models:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F3OEJaXooJCICJR6DJIJP%252Fimage.png%3Falt%3Dmedia%26token%3D4fa85cf1-463d-4090-a838-591c4f94efea&width=768&dpr=4&quality=100&sign=a1f23ff9&sv=2)

Or you can use the Llama-3 template itself (which only functions by using the instruct version of Llama-3): We in fact allow an optional `{SYSTEM}` field as well which is useful to customize a system prompt just like in ChatGPT.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F4qQXd0hIvh9fJNO2cJ04%252Fimage.png%3Falt%3Dmedia%26token%3D614b9200-7375-47f5-ac15-ce9aa891ede4&width=768&dpr=4&quality=100&sign=c9811100&sv=2)

Or in the Titanic prediction task where you had to predict if a passenger died or survived in this Colab notebook which includes CSV and Excel uploading: [https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing)

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F1iQitC3PwcuV0LpHEhdP%252Fimage.png%3Falt%3Dmedia%26token%3Dd117f681-afb0-4d5f-b534-f51013fe772a&width=768&dpr=4&quality=100&sign=20577629&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/chat-templates\#more-information)    More Information

Assuming your dataset is a list of list of dictionaries like the below:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
[\
    [{'from': 'human', 'value': 'Hi there!'},\
     {'from': 'gpt', 'value': 'Hi how can I help?'},\
     {'from': 'human', 'value': 'What is 2+2?'}],\
    [{'from': 'human', 'value': 'What's your name?'},\
     {'from': 'gpt', 'value': 'I'm Daniel!'},\
     {'from': 'human', 'value': 'Ok! Nice!'},\
     {'from': 'gpt', 'value': 'What can I do for you?'},\
     {'from': 'human', 'value': 'Oh nothing :)'},],\
]
```

You can use our `get_chat_template` to format it. Select `chat_template` to be any of `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth`, and use `mapping` to map the dictionary values `from`, `value` etc. `map_eos_token` allows you to map `<|im_end|>` to EOS without any training.

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "chatml", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth
    mapping = {"role" : "from", "content" : "value", "user" : "human", "assistant" : "gpt"}, # ShareGPT style
    map_eos_token = True, # Maps <|im_end|> to </s> instead
)

def formatting_prompts_func(examples):
    convos = examples["conversations"]
    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]
    return { "text" : texts, }
pass

from datasets import load_dataset
dataset = load_dataset("philschmid/guanaco-sharegpt-style", split = "train")
dataset = dataset.map(formatting_prompts_func, batched = True,)
```

You can also make your own custom chat templates! For example our internal chat template we use is below. You must pass in a `tuple` of `(custom_template, eos_token)` where the `eos_token` must be used inside the template.

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
unsloth_template = \
    "{{ bos_token }}"\
    "{{ 'You are a helpful assistant to the user\n' }}"\
    "</div>"\
    "<div data-gb-custom-block data-tag="for">"\
        "<div data-gb-custom-block data-tag="if" data-0='role' data-1='role' data-2='] == ' data-3='user'>"\
            "{{ '>>> User: ' + message['content'] + '\n' }}"\
        "<div data-gb-custom-block data-tag="elif" data-0='role' data-1='role' data-2='] == ' data-3='assistant'></div>"\
            "{{ '>>> Assistant: ' + message['content'] + eos_token + '\n' }}"\
        "</div>"\
    "</div>"\
    "<div data-gb-custom-block data-tag="if">"\
        "{{ '>>> Assistant: ' }}"\
    "</div>"
unsloth_eos_token = "eos_token"

tokenizer = get_chat_template(
    tokenizer,
    chat_template = (unsloth_template, unsloth_eos_token,), # You must provide a template and EOS token
    mapping = {"role" : "from", "content" : "value", "user" : "human", "assistant" : "gpt"}, # ShareGPT style
    map_eos_token = True, # Maps <|im_end|> to </s> instead
)
```

[PreviousContinued Pretraining](https://docs.unsloth.ai/basics/continued-pretraining) [NextVision Fine-tuning](https://docs.unsloth.ai/basics/vision-fine-tuning)

Last updated 24 days ago

Was this helpful?

* * *

## Unsloth Errors Guide
## [Direct link to heading](https://docs.unsloth.ai/basics/errors-troubleshooting\#running-in-unsloth-works-well-but-after-exporting-and-running-on-other-platforms-the-results-are-poo)    Running in Unsloth works well, but after exporting & running on other platforms, the results are poor

You might sometimes encounter an issue where your model runs and produces good results on Unsloth, but when you use it on another platform like Ollama or vLLM, the results are poor or you might get gibberish, endless/infinite generations _or_ repeated outputs **.**

- The most common cause of this error is using an incorrect chat template. Itâ€™s essential to use the SAME chat template that was used when training the model in Unsloth and later when you run it in another framework, such as llama.cpp or Ollama. When inferencing from a saved model, it's crucial to apply the correct template.

- It might also be because your inference engine adds an unnecessary "start of sequence" token (or the lack of thereof on the contrary) so ensure you check both hypotheses!


## [Direct link to heading](https://docs.unsloth.ai/basics/errors-troubleshooting\#saving-to-gguf-vllm-16bit-crashes)    Saving to GGUF / vLLM 16bit crashes

You can try reducing the maximum GPU usage during saving by changing `maximum_memory_usage`.

The default is `model.save_pretrained(..., maximum_memory_usage = 0.75)`. Reduce it to say 0.5 to use 50% of GPU peak memory or lower. This can reduce OOM crashes during saving.

## [Direct link to heading](https://docs.unsloth.ai/basics/errors-troubleshooting\#evaluation-loop-also-oom-or-crashing)    Evaluation Loop - also OOM or crashing.

A common issue when you OOM is because you set your batch size too high. Set it lower than 3 to use less VRAM.

First split your training dataset into a train and test split. Set the trainer settings for evaluation to:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
new_dataset = dataset.train_test_split(test_size = 0.01)
SFTTrainer(
    args = TrainingArguments(
        fp16_full_eval = True,
        per_device_eval_batch_size = 2,
        eval_accumulation_steps = 4,
        eval_strategy = "steps",
        eval_steps = 1,
    ),
    train_dataset = new_dataset["train"],
    eval_dataset = new_dataset["test"],
```

This will cause no OOMs and make it somewhat faster with no upcasting to float32. Validation set.

## [Direct link to heading](https://docs.unsloth.ai/basics/errors-troubleshooting\#notimplementederror-a-utf-8-locale-is-required.-got-ansi)    NotImplementedError: A UTF-8 locale is required. Got ANSI

See https://github.com/googlecolab/colabtools/issues/3409

In a new cell, run the below:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
import locale
locale.getpreferredencoding = lambda: "UTF-8"
```

[PreviousFinetuning from Last Checkpoint](https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint) [NextUnsloth Benchmarks](https://docs.unsloth.ai/basics/unsloth-benchmarks)

Last updated 14 days ago

Was this helpful?

* * *

## Running and Saving Models
You can also run your fine-tuned models by using [Unsloth's 2x faster inference](https://docs.unsloth.ai/basics/running-and-saving-models/inference).

[Saving to GGUF](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf)

[Saving to Ollama](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama)

[Saving to VLLM](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-vllm)

[Troubleshooting](https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting)

[Inference](https://docs.unsloth.ai/basics/running-and-saving-models/inference)

[PreviousDatasets 101](https://docs.unsloth.ai/basics/datasets-101) [NextSaving to GGUF](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf)

Last updated 23 days ago

Was this helpful?

* * *

## Model Selection Guide
## [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use\#llama-qwen-mistral-phi-or)    Llama, Qwen, Mistral, Phi or?

When preparing for fine-tuning, one of the first decisions you'll face is selecting the right model. Here's a step-by-step guide to help you choose:

1

#### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use\#choose-a-model-that-aligns-with-your-usecase)    Choose a model that aligns with your usecase

- E.g. For image-based training, select a vision model such as _Llama 3.2 Vision_. For code datasets, opt for a specialized model like _Qwen Coder 2.5_.

- **Licensing and Requirements**: Different models may have specific licensing terms and [system requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements#system-requirements). Be sure to review these carefully to avoid compatibility issues.


2

#### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use\#assess-your-storage-compute-capacity-and-dataset)    **Assess your storage, compute capacity and dataset**

- Use our [VRAM guideline](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements#approximate-vram-requirements-based-on-model-parameters) to determine the VRAM requirements for the model youâ€™re considering.

- Your dataset will reflect the type of model you will use and amount of time it will take to train


3

#### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use\#select-a-model-and-parameters)    **Select a Model and Parameters**

- We recommend using the latest model for the best performance and capabilities. For instance, as of January 2025, the leading 70B model is _Llama 3.3_.

- You can stay up to date by exploring our catalog of [model uploads](https://docs.unsloth.ai/get-started/all-our-models) to find the most recent and relevant options.


4

#### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use\#choose-between-base-and-instruct-models)    **Choose Between Base and Instruct Models**

Further details below:

## [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use\#instruct-or-base-model)    Instruct or Base Model?

When preparing for fine-tuning, one of the first decisions you'll face is whether to use an instruct model or a base model.

### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use\#instruct-models)    Instruct Models

Instruct models are pre-trained with built-in instructions, making them ready to use without any fine-tuning. These models, including GGUFs and others commonly available, are optimized for direct usage and respond effectively to prompts right out of the box.

### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use\#base-models)    **Base Models**

Base models, on the other hand, are the original pre-trained versions without instruction fine-tuning. These are specifically designed for customization through fine-tuning, allowing you to adapt them to your unique needs.

### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use\#should-i-choose-instruct-or-base)    Should I Choose Instruct or Base?

The decision often depends on the quantity, quality, and type of your data:

- **1,000+ Rows of Data**: If you have a large dataset with over 1,000 rows, it's generally best to fine-tune the base model.

- **300â€“1,000 Rows of High-Quality Data**: With a medium-sized, high-quality dataset, fine-tuning the base or instruct model are both viable options.

- **Less than 300 Rows**: For smaller datasets, the instruct model is typically the better choice. Fine-tuning the instruct model enables it to align with specific needs while preserving its built-in instructional capabilities. This ensures it can follow general instructions without additional input unless you intend to significantly alter its functionality.

- For information how how big your dataset should be, [see here](https://docs.unsloth.ai/basics/datasets-101#how-big-should-my-dataset-be)


### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use\#experimentation-is-key)    Experimentation is Key

We recommend experimenting with both models when possible. Fine-tune each one and evaluate the outputs to see which aligns better with your goals.

[PreviousFAQ + Is Fine-tuning Right For Me?](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me) [NextLoRA Parameters Encyclopedia](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia)

Last updated 2 months ago

Was this helpful?

* * *

## LoRA Parameters Guide
## [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia\#key-fine-tuning-parameters)    Key Fine-tuning Parameters

### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia\#learning-rate)    **Learning Rate**

Defines how much the modelâ€™s weights adjust per training step.

- **Higher Learning Rates**: Faster training, risk of overfitting.

- **Lower Learning Rates**: More stable training, may require more epochs.

- **Typical Range**: 1e-4 (0.0001) to 5e-5 (0.00005).


### [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia\#epochs)    **Epochs**

Number of times the model sees the full training dataset.

- **Recommended:** 1-3 epochs (anything more than 3 is generally not optimal unless you want your model to have much less hallucinations but also less creativity)

- **More Epochs**: Better learning, higher risk of overfitting.

- **Fewer Epochs**: May undertrain the model.


## [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia\#advanced-parameters)    **Advanced Parameters:**

Parameter

Function

Recommended Settings

**LoRA Rank**

Controls the number of low-rank factors used for adaptation.

16-32

**LoRA Alpha**

Scaling factor for weight updates.

Ratio of 1-2 with LoRA Rank

**LoRA Dropout**

Dropout rate to prevent overfitting.

0.1-0.2

**Max Sequence Length**

Maximum number of tokens processed in one input.

Adjust based on dataset needs

**Warmup Steps**

Gradually increases learning rate at the start of training.

5-10% of total steps

**Scheduler Type**

Adjusts learning rate dynamically during training.

Linear Decay, Cosine Annealing

**Seed**

Ensures reproducibility of results.

Fixed number (e.g., 42)

**Batch Size**

Number of samples processed per training step.

Higher values require more VRAM

**Quantization**

Reduces precision of model weights for efficiency.

Q4\_K\_M for balance between performance & speed

**Weight Decay**

Penalizes large weight updates to prevent overfitting.

Start at 0.01, adjust as needed

## [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia\#lora-configuration-parameters)    **LoRA Configuration Parameters**

For more information, we would recommend you checking out our [tutorial](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-5.-parameters-for-finetuning). Written by [Sebastien](https://github.com/sebdg).

Tuning these parameters helps balance model performance and efficiency:

- **r** (Rank of decomposition): Controls the finetuning process.



- Suggested: 8, 16, 32, 64, or 128.

- **Higher**: Better accuracy on hard tasks but increases memory and risk of overfitting.

- **Lower**: Faster, memory-efficient but may reduce accuracy.


- **lora\_alpha** (Scaling factor): Determines the learning strength.



- Suggested: Equal to or double the rank ( `r`).

- **Higher**: Learns more but may overfit.

- **Lower**: Slower to learn, more generalizable.


- **lora\_dropout** (Default: 0): Dropout probability for regularization.



- **Higher**: More regularization, slower training.

- **Lower (0)**: Faster training, minimal impact on overfitting.


- **target\_modules**: Modules to fine-tune (default includes `"q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"`).



- Fine-tuning all modules is recommended for best results.


- **bias** (Default: `"none"`): Controls bias term updates.



- **Set to none** for optimized, faster training.


- **use\_gradient\_checkpointing**: Reduces memory usage for long contexts.



- Use `"unsloth"` to reduce memory by an extra 30% by using our [gradient checkpointing](https://unsloth.ai/blog/gradient) algorithm which you can read about [here](https://unsloth.ai/blog/gradient).


- **random\_state**: A seed for reproducible experiments.



- Suggested: Set to a fixed value like 3407.


- **use\_rslora**: Enables Rank-Stabilized LoRA.



- **True**: Automatically adjusts `lora_alpha`.


- **loftq\_config**: Applies quantization and advanced LoRA initialization.



- **None**: Default (no quantization).

- **Set**: Initializes LoRA using top singular vectorsâ€”improves accuracy but increases memory usage.


## [Direct link to heading](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia\#target-modules-explained)    **Target Modules Explained**

These components transform inputs for attention mechanisms:

- **q\_proj, k\_proj, v\_proj**: Handle queries, keys, and values.

- **o\_proj**: Integrates attention results into the model.

- **gate\_proj**: Manages flow in gated layers.

- **up\_proj, down\_proj**: Adjust dimensionality for efficiency.


[PreviousWhat Model Should I Use?](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use) [NextUnsloth Notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks)

Last updated 24 days ago

Was this helpful?

* * *

## Beginner's Fine-Tuning Guide
If you're a beginner, here might be the first questions you'll ask before your first fine-tune. You can also always ask our community by joining our [Discord server](https://discord.gg/unsloth).

[ðŸ§¬](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama) [Fine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-guide)

Step-by-step on how to fine-tune!

Learn the core basics of training.

[â“](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use) [What Model Should I Use?](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use)

Instruct or Base Model?

How big should my dataset be?

[ðŸ“¥](https://docs.unsloth.ai/get-started/installing-+-updating) [Installing + Updating](https://docs.unsloth.ai/get-started/installing-+-updating)

How do I install Unsloth locally?

How to update Unsloth?

[ðŸ¤”](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me) [FAQ + Is Fine-tuning Right For Me?](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me)

What can fine-tuning do for me?

RAG vs. Fine-tuning?

ðŸ“ˆ [Datasets 101](https://docs.unsloth.ai/basics/datasets-101)

How do I structure/prepare my dataset?

How do I collect data?

[ðŸ’¬](https://docs.unsloth.ai/basics/chat-templates) [Chat Templates](https://docs.unsloth.ai/basics/chat-templates)

How do I structure my Chat Template?

Which Chat Template should I use for my model?

[ðŸ› ï¸](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements) [Unsloth Requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements)

Does Unsloth work on my GPU?

How much VRAM will I need?

[ðŸ–¥ï¸](https://docs.unsloth.ai/basics/running-and-saving-models) [Running & Saving Models](https://docs.unsloth.ai/basics/running-and-saving-models)

How do I save my model locally?

How do I run my model via Ollama or vLLM?

ðŸ§  [LoRA Parameters Encyclopedia](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia)

What happens when I change a parameter?

What parameters should I change?

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FjT759hR4zq8ygzg1oEwI%252FLarge%2520sloth%2520Question%2520mark.png%3Falt%3Dmedia%26token%3Dca8d2f56-889a-4da8-8106-da88d22e69d2&width=768&dpr=4&quality=100&sign=1635f07f&sv=2)

[PreviousWelcome](https://docs.unsloth.ai/) [NextUnsloth Requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements)

Last updated 24 days ago

Was this helpful?

* * *

## Unsloth Installation Guide
Unsloth works on Linux, Windows directly, Kaggle, Google Colab and more. See our [system requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements).

**Recommended installation method:**

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install unsloth
```

[Pip Install](https://docs.unsloth.ai/get-started/installing-+-updating/pip-install)

[Windows Installation](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation)

[Updating](https://docs.unsloth.ai/get-started/installing-+-updating/updating)

[Conda Install](https://docs.unsloth.ai/get-started/installing-+-updating/conda-install)

[Google Colab](https://docs.unsloth.ai/get-started/installing-+-updating/google-colab)

[PreviousAll Our Models](https://docs.unsloth.ai/get-started/all-our-models) [NextUpdating](https://docs.unsloth.ai/get-started/installing-+-updating/updating)

Last updated 9 days ago

Was this helpful?

* * *

## GRPO and Reinforcement Learning
This article covers everything you need to know about GRPO, Reinforcement Learning (RL) and reward functions, along with tips, and the basics of using GRPO with Unsloth. If you're looking for a quickstart tutorial for using GRPO, see our guide [here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo):

[âš¡Tutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo)

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#grpo-notebooks)    GRPO notebooks:

- [Llama 3.1 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)

- [Phi-4 (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb)

- [Qwen2.5 (3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb)


DeepSeek developed [GRPO](https://unsloth.ai/blog/grpo) (Group Relative Policy Optimization) to train their R1 reasoning models. This RL technique optimizes responses efficiently without a value function model, reducing memory and computational costs compared to PPO (Proximal Policy Optimization).

- Usecases for GRPO isnâ€™t just for code or mathâ€”its reasoning process can enhance tasks like email automation, database retrieval, law, and medicine, greatly improving accuracy based on your dataset and reward function!

- With 15GB VRAM, Unsloth allows you to transform any model up to 17B parameters like Llama 3.1 (8B), Phi-4 (14B), Mistral (7B) or Qwen2.5 (7B) into a reasoning model

- **Minimum requirement:** Just â€¯5GB VRAM is enough to train your own reasoning model locally (for any model with 1.5B parameters or less)

- If you're not getting any reasoning, make sure you have enough training steps and ensure your [reward function/verifier](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier) is working. We provide examples for reward functions [here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-function-examples).

- Previous demonstrations show that you could achieve your own "aha" moment with Qwen2.5 (3B) - but it required 2xA100 GPUs (160GB VRAM). Now, with Unsloth, you can achieve the same "aha" moment using just a single 5GB VRAM GPU.

- Previously, GRPO was only supported for full fine-tuning, but we've made it work with QLoRA and LoRA

- On [**20K context lengths**](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#grpo-requirement-guidelines) for example with 8 generations per prompt, Unsloth uses only 54.3GB of VRAM for Llama 3.1 (8B), whilst standard implementations (+ Flash Attention 2) take **510.8GB (90% less for Unsloth)**.

- Please note, this isnâ€™t fine-tuning DeepSeekâ€™s R1 distilled models or using distilled data from R1 for tuning which Unsloth already supported. This is converting a standard model into a full-fledged reasoning model using GRPO.


In a test example, even though we only trained Phi-4 with 100 steps using GRPO, the results are already clear. The model without GRPO does not have the thinking token, whilst the one trained with GRPO does and also has the correct answer.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FyBeJAvfolzfEYyftji76%252Fprompt%2520only%2520example.png%3Falt%3Dmedia%26token%3D3903995a-d9d5-4cdc-9020-c4efe7fff651&width=768&dpr=4&quality=100&sign=80d59783&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#training-with-grpo)    Training with GRPO

For a tutorial on how to transform any open LLM into a reasoning model using Unsloth & GRPO, [see here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo).

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#how-grpo-trains-a-model)    **How GRPO Trains a Model**

1. For each question-answer pair, the model generates multiple possible responses (e.g., 8 variations).

2. Each response is evaluated using reward functions.

3. Training Steps:



- If you have 300 rows of data, that's 300 training steps (or 900 steps if trained for 3 epochs).

- You can increase the number of generated responses per question (e.g., from 8 to 16).


4. The model learns by updating its weights every step.


### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#basics-tips)    Basics/Tips

- Wait for at least **300 steps** for the reward to actually increase. In order to get decent results, you may need to trade for a minimum of 12 hours (this is how GRPO works), but keep in mind this isn't compulsory as you can stop at anytime.

- For optimal results have at least **500 rows of data**. You can try with even 10 rows of data but it's better to have more.

- Each training run will always be different depending on your model, data, reward function/verifier etc. so though 300 steps is what we wrote as the minimum, sometimes it might be 1000 steps or more. So, it depends on various factors.

- If you're using GRPO with Unsloth locally, please "pip install diffusers" as well if you get an error. Please also use the latest version of vLLM.

- Itâ€™s advised to apply GRPO to a model at least **1.5B in parameters** to correctly generate thinking tokens as smaller models may not.

- For GRPO's [**GPU VRAM requirements**](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#grpo-requirement-guidelines) **for QLoRA 4-bit**, the general rule is the model parameters = the amount of VRAM you will need (you can use less VRAM but this just to be safe). The more context length you set, the more VRAM. LoRA 16-bit will use at minimum 4x more VRAM.

- **Continuous fine-tuning is** possible and you can just leave GRPO running in the background.

- In the example notebooks, we use the [**GSM8K dataset**](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#gsm8k-reward-functions), the current most popular choice for R1-style training.

- If youâ€™re using a base model, ensure you have a chat template.

- The more you train with GRPO the better. The best part of GRPO is you don't even need that much data. All you need is a great reward function/verifier and the more time spent training, the better your model will get. Expect your reward vs step to increase as time progresses like this:





![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FUROleqJQ5aEp8MjTCWFf%252Funnamed.png%3Falt%3Dmedia%26token%3D12ca4975-7a0c-4d10-9178-20db28ad0451&width=768&dpr=4&quality=100&sign=a2046ca5&sv=2)

- Training loss tracking for GRPO is now built directly into Unsloth, eliminating the need for external tools like wandb etc. It contains full logging details for all reward functions now including the total aggregated reward function itself.


![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fjo7fVFoFG2xbZPgL45el%252FScreenshot%25202025-02-20%2520at%252004-52-52%2520Copy%2520of%2520Yet%2520another%2520copy%2520of%2520Llama3.1_%288B%29-GRPO.ipynb%2520-%2520Colab.png%3Falt%3Dmedia%26token%3D041c17b1-ab98-4ab6-b6fb-8c7e5a8c07df&width=768&dpr=4&quality=100&sign=b8126c85&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#reward-functions-verifier)    Reward Functions / Verifier

In Reinforcement Learning a **Reward Function** and a **Verifier** serve distinct roles in evaluating a modelâ€™s output. In general, you could interpret them as the same thing however, technically they're not but it does not matter as much as they are usually used in conjunction with each other.

**Verifier**:

- Determines whether the generated response is correct or incorrect.

- It does not assign a numerical scoreâ€”it simply verifies correctness.

- Example: If a model generates "5" for "2+2", the verifier checks and labels it as "wrong" (since the correct answer is 4).

- Verifiers can also execute code (e.g., in Python) to validate logic, syntax, and correctness without needing manual evaluation.


**Reward Function**:

- Converts verification results (or other criteria) into a numerical score.

- Example: If an answer is wrong, it might assign a penalty (-1, -2, etc.), while a correct answer could get a positive score (+1, +2).

- It can also penalize based on criteria beyond correctness, such as excessive length or poor readability.


**Key Differences**:

- A **Verifier** checks correctness but doesnâ€™t score.

- A **Reward Function** assigns a score but doesnâ€™t necessarily verify correctness itself.

- A Reward Function _can_ use a Verifier, but they are technically not the same.


### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#understanding-reward-functions)    **Understanding Reward Functions**

GRPO's primary goal is to maximize reward and learn how an answer was derived, rather than simply memorizing and reproducing responses from its training data.

- With every training step, GRPO **adjusts model weights** to maximize the reward. This process fine-tunes the model incrementally.

- **Regular fine-tuning** (without GRPO) only **maximizes next-word prediction probability** but does not optimize for a reward. GRPO **optimizes for a reward function** rather than just predicting the next word.

- You can **reuse data** across multiple epochs.

- **Default reward functions** can be predefined to be used on a wide array of use cases or you can ask ChatGPT/local model to generate them for you.

- Thereâ€™s no single correct way to design reward functions or verifiers - the possibilities are endless. However, they must be well-designed and meaningful, as poorly crafted rewards can unintentionally degrade model performance.


### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#reward-function-examples)    Reward Function Examples

You can refer to the examples below. You can input your generations into an LLM like ChatGPT 4o or Llama 3.1 (8B) and design a reward function and verifier to evaluate it. For example, feed your generations into a LLM of your choice and set a rule: "If the answer sounds too robotic, deduct 3 points." This helps refine outputs based on quality criteria

#### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#example-1-simple-arithmetic-task)    **Example \#1: Simple Arithmetic Task**

- **Question:** `"2 + 2"`

- **Answer:** `"4"`

- **Reward Function 1:**



- If a number is detected â†’ **+1**

- If no number is detected â†’ **-1**


- **Reward Function 2:**



- If the number matches the correct answer â†’ **+3**

- If incorrect â†’ **-3**


- **Total Reward:** _Sum of all reward functions_


#### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#example-2-email-automation-task)    **Example \#2: Email Automation Task**

- **Question:** Inbound email

- **Answer:** Outbound email

- **Reward Functions:**



- If the answer contains a required keyword â†’ **+1**

- If the answer exactly matches the ideal response â†’ **+1**

- If the response is too long â†’ **-1**

- If the recipient's name is included â†’ **+1**

- If a signature block (phone, email, address) is present â†’ **+1**


### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#gsm8k-reward-functions)    GSM8K Reward Functions

In our examples, we've built on existing GSM8K reward functions by [@willccbb](https://x.com/willccbb) which is popular and shown to be quite effective:

- **correctness\_reward\_func** â€“ Rewards exact label matches.

- **int\_reward\_func** â€“ Encourages integer-only answers.

- **soft\_format\_reward\_func** â€“ Checks structure but allows minor newline mismatches.

- **strict\_format\_reward\_func** â€“ Ensures response structure matches the prompt, including newlines.

- **xmlcount\_reward\_func** â€“ Ensures exactly one of each XML tag in the response.


## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#using-vllm)    Using vLLM

You can now use [vLLM](https://github.com/vllm-project/vllm/) directly in your finetuning stack, which allows for much more throughput and allows you to finetune and do inference on the model at the same time! On 1x A100 40GB, expect 4000 tokens / s or so with Unslothâ€™s dynamic 4bit quant of Llama 3.2 3B Instruct. On a 16GB Tesla T4 (free Colab GPU), you can get 300 tokens / s.

We also magically removed double memory usage when loading vLLM and Unsloth together, allowing for savings of 5GB or so for Llama 3.1 8B and 3GB for Llama 3.2 3B. Unsloth could originally finetune Llama 3.3 70B Instruct in 1x 48GB GPU with Llama 3.3 70B weights taking 40GB of VRAM. If we do not remove double memory usage, then weâ€™ll need >= 80GB of VRAM when loading Unsloth and vLLM together.

But with Unsloth, you can still finetune and get the benefits of fast inference in one package in under 48GB of VRAM! To use fast inference, first install vllm, and instantiate Unsloth with fast\_inference:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install unsloth vllm
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct",
    fast_inference = True,
)
model.fast_generate(["Hello!"])
```

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#grpo-requirement-guidelines)    GRPO Requirement Guidelines

When youâ€™re using Unsloth to do GRPO, we smartly reduce VRAM usage by over 90% when compared to standard implementations with Flash Attention 2 by using multiple tricks! On 20K context lengths for example with 8 generations per prompt, Unsloth uses only **54.3GB of VRAM for Llama 3.1 8B**, whilst standard implementations take **510.8GB (90% less for Unsloth)**.

1. For GRPO's **GPU VRAM requirements for QLoRA 4-bit**, the general rule is the model parameters = the amount of VRAM you will need (you can use less VRAM but this just to be safe). The more context length you set, the more VRAM. LoRA 16-bit will use at minimum 4x more VRAM.

2. Our new memory efficient linear kernels for GRPO slashes memory usage by 8x or more. This shaves 68.5GB of memory, whilst being actually faster through the help of torch.compile!

3. We leverage our smart [Unsloth gradient checkpointing](https://unsloth.ai/blog/long-context) algorithm which we released a while ago. It smartly offloads intermediate activations to system RAM asynchronously whilst being only 1% slower. This shaves 52GB of memory.

4. Unsloth also uses the same GPU / CUDA memory space as the underlying inference engine (vLLM), unlike implementations in other packages. This shaves 16GB of memory.


Metrics

Unsloth

Standard + FA2

Training Memory Cost (GB)

42GB

414GB

GRPO Memory Cost (GB)

9.8GB

78.3GB

Inference Cost (GB)

0GB

16GB

Inference KV Cache for 20K context length (GB)

2.5GB

2.5GB

Total Memory Usage

54.33GB (90% less)

510.8GB

In typical standard GRPO implementations, you need to create 2 logits of size (8. 20K) to calculate the GRPO loss. This takes 2 \* 2 bytes \* 8 (num generations) \* 20K (context length) \* 128256 (vocabulary size) = 78.3GB in VRAM.

Unsloth shaves 8x memory usage for long context GRPO, so we need only an extra 9.8GB in extra VRAM for 20K context lengths!

We also need to from the KV Cache in 16bit. Llama 3.1 8B has 32 layers, and both K and V are 1024 in size. So memory usage for 20K context length = 2 \* 2 bytes \* 32 layers \* 20K context length \* 1024 = 2.5GB per batch. We would set the batch size for vLLM to 8, but we shall leave it at 1 for our calculations to save VRAM. Otherwise you will need 20GB for the KV cache.

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#how-grpo-works)    How GRPO Works:

DeepSeekâ€™s researchers observed an "aha moment" when training R1-Zero with pure reinforcement learning (RL). The model learned to extend its thinking time by reevaluating its initial approach, without any human guidance or predefined instructions.

1. The model generates groups of responses.

2. Each response is scored based on correctness or another metric created by some set reward function rather than an LLM reward model.

3. The average score of the group is computed.

4. Each response's score is compared to the group average.

5. The model is reinforced to favor higher-scoring responses.


As an example, assume we want a model to solve:

What is 1+1? >> Chain of thought/working out >> The answer is 2.

What is 2+2? >> Chain of thought/working out >> The answer is 4.

Originally, one had to collect large swathes of data to fill the working out / chain of thought process. But GRPO (the algorithm DeepSeek uses) or other RL algorithms can steer the model to automatically exhibit reasoning capabilities and create the reasoning trace. Instead, we need to create good reward functions or verifiers. For example, if it gets the correct answer, give it a score of 1. If some words are mis-spelt, minus 0.1. And so on! We can provide many many functions to reward the process.

[PreviousFine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-guide) [NextTutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo)

Last updated 2 days ago

Was this helpful?

* * *

## Unsloth Benchmark Insights
- For our most detailed benchmarks, read our [Llama 3.3 Blog](https://unsloth.ai/blog/llama3-3).

- Benchmarking of Unsloth was also conducted by [ðŸ¤—Hugging Face](https://huggingface.co/blog/unsloth-trl).


We tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):

Model

VRAM

ðŸ¦¥Unsloth speed

ðŸ¦¥VRAM reduction

ðŸ¦¥Longer context

ðŸ˜ŠHugging Face + FA2

Llama 3.3 (70B)

80GB

2x

>75%

13x longer

1x

Llama 3.1 (8B)

80GB

2x

>70%

12x longer

1x

## [Direct link to heading](https://docs.unsloth.ai/basics/unsloth-benchmarks\#context-length-benchmarks)    Context length benchmarks

The more data you have, the less VRAM Unsloth uses due to our [gradient checkpointing](https://unsloth.ai/blog/long-context) algorithm + Apple's CCE algorithm!

### [Direct link to heading](https://docs.unsloth.ai/basics/unsloth-benchmarks\#llama-3.1-8b-max.-context-length)    **Llama 3.1 (8B) max. context length**

We tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.

GPU VRAM

ðŸ¦¥Unsloth context length

Hugging Face + FA2

8 GB

2,972

OOM

12 GB

21,848

932

16 GB

40,724

2,551

24 GB

78,475

5,789

40 GB

153,977

12,264

48 GB

191,728

15,502

80 GB

342,733

28,454

### [Direct link to heading](https://docs.unsloth.ai/basics/unsloth-benchmarks\#llama-3.3-70b-max.-context-length)    **Llama 3.3 (70B) max. context length**

We tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.

GPU VRAM

ðŸ¦¥Unsloth context length

Hugging Face + FA2

48 GB

12,106

OOM

80 GB

89,389

6,916

[PreviousErrors/Troubleshooting](https://docs.unsloth.ai/basics/errors-troubleshooting)

Last updated 1 month ago

Was this helpful?

* * *

## Unsloth Notebooks Overview
â€¢ Google Colabâ€¢ Kaggle

#### [Direct link to heading](https://docs.unsloth.ai/get-started/unsloth-notebooks\#main-notebooks)    Main notebooks:

- Gemma 3 (12B) - we're still working on it

- [Llama 3.1 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb) \- GRPO reasoning

- [Phi-4 (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb) \- GRPO reasoning

- [Qwen2.5 (3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb) \- GRPO reasoning

- [Phi-4 (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)

- [Llama 3.1 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)

- [Llama 3.2 (1B + 3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)

- [Mistral v0.3 Instruct (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)

- [Gemma 2 (9B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb)

- [Qwen 2.5 (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)


#### [Direct link to heading](https://docs.unsloth.ai/get-started/unsloth-notebooks\#vision-notebooks)    Vision notebooks:

- [Llama 3.2 Vision (11B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)

- [Qwen2-VL (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb)

- [Pixtral (12B) 2409](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb)


#### [Direct link to heading](https://docs.unsloth.ai/get-started/unsloth-notebooks\#other-important-notebooks)    Other important notebooks:

- [Ollama](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)

- [ORPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb)

- [Continued Pretraining](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-CPT.ipynb)

- [DPO Zephyr](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb)

- [_**Inference only**_](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Inference.ipynb)

- [Phi-3.5 (mini)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb)

- [Llama 3 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Alpaca.ipynb)

- [Phi-3 (medium)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3_Medium-Conversational.ipynb)

- [Mistral NeMo (12B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Nemo_(12B)-Alpaca.ipynb)


#### [Direct link to heading](https://docs.unsloth.ai/get-started/unsloth-notebooks\#specific-use-case-notebooks)    Specific use-case notebooks:

- [_**Inference chat UI**_](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb)

- [Text Classification](https://github.com/timothelaborie/text_classification_scripts/blob/main/unsloth_classification.ipynb) by Timotheeee

- [Multiple Datasets](https://colab.research.google.com/drive/1njCCbE1YVal9xC83hjdo2hiGItpY_D6t?usp=sharing) by Flail

- [KTO](https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing) by Jeffrey

- [Conversational](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)

- [ChatML](https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing)

- [Text Completion](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)


#### [Direct link to heading](https://docs.unsloth.ai/get-started/unsloth-notebooks\#rest-of-notebooks)    Rest of notebooks:

- [Gemm](https://colab.research.google.com/drive/1weTpKOjBZxZJ5PQ-Ql8i6ptAY2x-FWVA?usp=sharing) [a 2 (2B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(2B)-Alpaca.ipynb)

- [Qwen 2.5 Coder (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(14B)-Conversational.ipynb)

- [Mistral Small (22B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb)

- [TinyLlama](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/TinyLlama_(1.1B)-Alpaca.ipynb)

- [CodeGemma (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/CodeGemma_(7B)-Conversational.ipynb)

- [Mistral v0.3 (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Alpaca.ipynb)

- [Qwen2 (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_(7B)-Alpaca.ipynb)


- [Llama 3.1 (8B)](https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Phi_4_(14B)-GRPO.ipynb&accelerator=nvidiaTeslaT4) \- GRPO reasoning

- [Phi-4 (14B)](https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Phi_4_(14B)-GRPO.ipynb&accelerator=nvidiaTeslaT4) \- GRPO reasoning

- [Qwen2.5 (3B)](https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Qwen2.5_(3B)-GRPO.ipynb&accelerator=nvidiaTeslaT4) \- GRPO reasoning

- [Phi-4 (14B)](https://www.kaggle.com/code/danielhanchen/phi-4-finetuning-unsloth-notebook)

- [Llama 3.1 (8B)](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook)

- [Llama 3.2 (1B + 3B)](https://www.kaggle.com/code/danielhanchen/fixed-kaggle-llama-3-2-1b-3b-conversation)

- [Llama 3.2 Vision](https://www.kaggle.com/code/danielhanchen/llama-3-2-vision-finetuning-unsloth-kaggle)

- [Mistral NeMo (12B)](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-nemo-12b-unsloth-notebook)

- [Qwen 2.5 (14B)](https://www.kaggle.com/code/danielhanchen/kaggle-qwen-2-5-conversational-unsloth)

- [Gemma 2 (9B)](https://www.kaggle.com/code/danielhanchen/kaggle-gemma2-9b-unsloth-notebook)

- [Phi-3 (medium)](https://www.kaggle.com/code/danielhanchen/kaggle-phi-3-medium-unsloth-notebook)

- [Qwen2-VL (7B)](https://www.kaggle.com/code/danielhanchen/qwen2-vision-finetuning-unsloth-kaggle)

- [Qwen2.5-Coder (14B)](https://www.kaggle.com/code/danielhanchen/kaggle-qwen-2-5-coder-14b-conversational)

- [Llama 3 (8B)](https://www.kaggle.com/code/danielhanchen/kaggle-llama-3-8b-unsloth-notebook)

- [Mistral v0.3 (7B)](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)

- [Qwen2 (7B)](https://docs.unsloth.ai/)


To view a complete list of all our Kaggle notebooks, [click here](https://github.com/unslothai/notebooks#-kaggle-notebooks).

Feel free to contribute to the notebooks by visiting our [repo](https://github.com/unslothai/notebooks)!

[PreviousLoRA Parameters Encyclopedia](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia) [NextAll Our Models](https://docs.unsloth.ai/get-started/all-our-models)

Last updated 15 hours ago

Was this helpful?

* * *

## Google Colab Installation
![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FQzuUQL60uFWHpaAvDPYD%252FColab%2520Options.png%3Falt%3Dmedia%26token%3Dfb808ec5-20c5-4f42-949e-14ed26a44987&width=768&dpr=4&quality=100&sign=be097a14&sv=2)

If you have never used a Colab notebook, a quick primer on the notebook itself:

1. **Play Button at each "cell".** Click on this to run that cell's code. You must not skip any cells and you must run every cell in chronological order. If you encounter errors, simply rerun the cell you did not run. Another option is to click CTRL + ENTER if you don't want to click the play button.

2. **Runtime Button in the top toolbar.** You can also use this button and hit "Run all" to run the entire notebook in 1 go. This will skip all the customization steps, but is a good first try.

3. **Connect / Reconnect T4 button.** T4 is the free GPU Google is providing. It's quite powerful!


The first installation cell looks like below: Remember to click the PLAY button in the brackets \[ \]. We grab our open source Github package, and install some other packages.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FIz2XUXhcmjheDtxfvbLA%252Fimage.png%3Falt%3Dmedia%26token%3Db9da0e5c-075c-48f8-8abb-5db6fdf9866b&width=768&dpr=4&quality=100&sign=e33e1780&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/google-colab\#undefined)

[PreviousConda Install](https://docs.unsloth.ai/get-started/installing-+-updating/conda-install) [NextFine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-guide)

Last updated 8 months ago

Was this helpful?

* * *

## Model Export Troubleshooting
## [Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting\#running-in-unsloth-works-well-but-after-exporting-and-running-on-other-platforms-the-results-are-poo)    Running in Unsloth works well, but after exporting & running on other platforms, the results are poor

You might sometimes encounter an issue where your model runs and produces good results on Unsloth, but when you use it on another platform like Ollama or vLLM, the results are poor or you might get gibberish, endless/infinite generations _or_ repeated outputs **.**

- The most common cause of this error is using an incorrect chat template. Itâ€™s essential to use the SAME chat template that was used when training the model in Unsloth and later when you run it in another framework, such as llama.cpp or Ollama. When inferencing from a saved model, it's crucial to apply the correct template.

- It might also be because your inference engine adds an unnecessary "start of sequence" token (or the lack of thereof on the contrary) so ensure you check both hypotheses!


## [Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting\#saving-to-safetensors-not-bin-format-in-colab)    Saving to `safetensors`, not `bin` format in Colab

We save to `.bin` in Colab so it's like 4x faster, but set `safe_serialization = None` to force saving to `.safetensors`. So `model.save_pretrained(..., safe_serialization = None)` or `model.push_to_hub(..., safe_serialization = None)`

## [Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting\#if-saving-to-gguf-or-vllm-16bit-crashes)    If saving to GGUF or vLLM 16bit crashes

You can try reducing the maximum GPU usage during saving by changing `maximum_memory_usage`.

The default is `model.save_pretrained(..., maximum_memory_usage = 0.75)`. Reduce it to say 0.5 to use 50% of GPU peak memory or lower. This can reduce OOM crashes during saving.

[PreviousSaving to VLLM](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-vllm) [NextInference](https://docs.unsloth.ai/basics/running-and-saving-models/inference)

Last updated 9 days ago

Was this helpful?

* * *

## Unsloth Update Instructions
## [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/updating\#standard-updating)    Standard Updating:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo
```

### [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/updating\#updating-without-dependency-updates)    Updating without dependency updates:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install --upgrade --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
pip install --upgrade --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth-zoo.git
```

## [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/updating\#to-use-an-old-version-of-unsloth)    To use an old version of Unsloth:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install --force-reinstall --no-cache-dir --no-deps unsloth==2025.1.5
```

'2025.1.5' is one of the previous old versions of Unsloth. Change it to a specific release listed on our [Github here](https://github.com/unslothai/unsloth/releases).

[PreviousInstalling + Updating](https://docs.unsloth.ai/get-started/installing-+-updating) [NextPip Install](https://docs.unsloth.ai/get-started/installing-+-updating/pip-install)

Last updated 11 days ago

Was this helpful?

* * *

## Unsloth Windows Installation Guide
## [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation\#method-1-windows-directly)    Method \#1 - Windows directly:

Python 3.13 does not support Unsloth. Use 3.12, 3.11 or 3.10.

Need help or experiencing an error? Ask on our [GitHub Discussions](https://github.com/unslothai/unsloth/discussions/1849) thread for Windows support!

1

**Install NVIDIA Video Driver**

You should install the latest version of your GPUs driver. Download drivers here: [NVIDIA GPU Drive](https://www.nvidia.com/Download/index.aspx)

2

**Install Visual Studio C++**

You will need Visual Studio, with C++ installed. By default, C++ is not installed with Visual Studio, so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK.

- Launch the Installer here: [Visual Studio Community Edition](https://visualstudio.microsoft.com/vs/community/)

- In the installer, navigate to individual components and select all the options listed here:



- **.NET Framework 4.8 SDK**

- **.NET Framework 4.7.2 targeting pack**

- **C# and Visual Basic Roslyn compilers**

- **MSBuild**

- **MSVC v143 - VS 2022 C++ x64/x86 build tools**

- **C++ 2022 Redistributable Update**

- **C++ CMake tools for Windows**

- **C++/CLI support for v143 build tools (Latest)**

- **MSBuild support for LLVM (clang-cl) toolset**

- **C++ Clang Compiler for Windows (19.1.1)**

- **Windows 11 SDK (10.0.22621.0)**

- **Windows Universal CRT SDK**

- **C++ 2022 Redistributable MSMs**


**Easier method:** Or you can open an elevated Command Prompt or PowerShell:

- Search for "cmd" or "PowerShell", right-click it, and choose "Run as administrator."

- Paste and run this command (update the Visual Studio path if necessary):


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
"C:\Program Files (x86)\Microsoft Visual Studio\Installer\vs_installer.exe" modify ^
--installPath "C:\Program Files\Microsoft Visual Studio\2022\Community" ^
--add Microsoft.Net.Component.4.8.SDK ^
--add Microsoft.Net.Component.4.7.2.TargetingPack ^
--add Microsoft.VisualStudio.Component.Roslyn.Compiler ^
--add Microsoft.Component.MSBuild ^
--add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 ^
--add Microsoft.VisualStudio.Component.VC.Redist.14.Latest ^
--add Microsoft.VisualStudio.Component.VC.CMake.Project ^
--add Microsoft.VisualStudio.Component.VC.CLI.Support ^
--add Microsoft.VisualStudio.Component.VC.Llvm.Clang ^
--add Microsoft.VisualStudio.ComponentGroup.ClangCL ^
--add Microsoft.VisualStudio.Component.Windows11SDK.22621 ^
--add Microsoft.VisualStudio.Component.Windows10SDK.19041 ^
--add Microsoft.VisualStudio.Component.UniversalCRT.SDK ^
--add Microsoft.VisualStudio.Component.VC.Redist.MSM
```

3

**Install Python and CUDA Toolkit**

Follow the instructions to install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive).

Then install Miniconda (which has Python) here: [https://www.anaconda.com/docs/getting-started/miniconda/install](https://www.anaconda.com/docs/getting-started/miniconda/install#quickstart-install-instructions)

4

**Install PyTorch**

You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully. [Install PyTorch](https://pytorch.org/get-started/locally/)

5

**Install Unsloth**

Open Conda command prompt or your terminal with Python and run the command:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install "unsloth[windows] @ git+https://github.com/unslothai/unsloth.git"
```

If you're using GRPO or plan to use vLLM, currently vLLM does not support Windows directly but only via WSL or Linux.

### [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation\#notes)    **Notes**

To run Unsloth directly on Windows:

- Install Triton from this Windows fork and follow the instructions [here](https://github.com/woct0rdho/triton-windows) (be aware that the Windows fork requires PyTorch >= 2.4 and CUDA 12)

- In the SFTTrainer, set `dataset_num_proc=1` to avoid a crashing issue:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
trainer = SFTTrainer(
    dataset_num_proc=1,
    ...
)
```

### [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation\#advanced-troubleshooting)    **Advanced/Troubleshooting**

For **advanced installation instructions** or if you see weird errors during installations:

1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`

2. Confirm if CUDA is installated correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.

3. Install `xformers` manually. You can try installing `vllm` and seeing if `vllm` succeeds. Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs.

4. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful.

5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`


* * *

## [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation\#method-2-windows-using-powershell)    Method \#2 - Windows using PowerShell:

#### [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation\#step-1-install-prerequisites)    **Step 1: Install Prerequisites**

1. **Install NVIDIA CUDA Toolkit**:



- Download and install the appropriate version of the **NVIDIA CUDA Toolkit** from [CUDA Downloads](https://developer.nvidia.com/cuda-downloads).

- Reboot your system after installation if prompted.

- **Note**: No additional setup is required after installation for Unsloth.


2. **Install Microsoft C++ Build Tools**:



- Download and install **Microsoft Build Tools for Visual Studio** from the [official website](https://visualstudio.microsoft.com/visual-cpp-build-tools/).

- During installation, select the **C++ build tools** workload.
Ensure the **MSVC compiler toolset** is included.


3. **Set Environment Variables for the C++ Compiler**:



- Open the **System Properties** window (search for "Environment Variables" in the Start menu).

- Click **"Environment Variablesâ€¦"**.

- Add or update the following under **System variables**:



- **CC**:
Path to the `cl.exe` C++ compiler.
Example (adjust if your version differs):







Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
C:\Program Files\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.34.31933\bin\Hostx64\x64\cl.exe
```

- **CXX**:
Same path as `CC`.


- Click **OK** to save changes.

- Verify: Open a new terminal and type `cl`. It should show version info.


4. **Install Conda**



1. Download and install **Miniconda** from the [official website](https://docs.anaconda.com/miniconda/install/#quick-command-line-install)

2. Follow installation instruction from the website

3. To check whether `conda` is already installed, you can test it with `conda` in your PowerShell


#### [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation\#step-2-run-the-unsloth-installation-script)    **Step 2: Run the Unsloth Installation Script**

1. **Download the** [**unsloth\_windows.ps1**](https://github.com/unslothai/notebooks/blob/main/unsloth_windows.ps1) **PowerShell script by going through this link**.

2. **Open PowerShell as Administrator**:



- Right-click Start and select **"Windows PowerShell (Admin)"**.


3. **Navigate to the scriptâ€™s location** using `cd`:







Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
cd path\to\script\folder
```

4. **Run the script**:







Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
powershell.exe -ExecutionPolicy Bypass -File .\unsloth_windows.ps1
```


#### [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation\#step-3-using-unsloth)    **Step 3: Using Unsloth**

Activate the environment after the installation completes:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
conda activate unsloth_env
```

**Unsloth and its dependencies are now ready!**

* * *

## [Direct link to heading](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation\#method-3-windows-via-wsl)    Method \#3 - Windows via WSL:

WSL is Window's subsystem for Linux.

1. Install python though [Python's official site](https://www.python.org/downloads/windows/).

2. Start WSL (Should already be preinstalled). Open command prompt as admin then run:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
wsl -d ubuntu
```

Optional: If WSL is not preinstalled, go to the Microsoft store and search "Ubuntu" and the app that says Ubuntu will be WSL. Install it and run it and continue from there.

1. Update WSL:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
sudo apt update && sudo apt upgrade -y
```

1. Install pip:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
sudo apt install python3-pip
```

1. Install unsloth:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install unsloth
```

1. Optional: Install Jupyter Notebook to run in a Colab like environment:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip3 install notebook
```

1. Launch Jupyter Notebook:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
jupyter notebook
```

1. Download any Colab notebook from Unsloth, import it into your Jupyter Notebook, adjust the parameters as needed, and execute the script.


[PreviousPip Install](https://docs.unsloth.ai/get-started/installing-+-updating/pip-install) [NextConda Install](https://docs.unsloth.ai/get-started/installing-+-updating/conda-install)

Last updated 6 days ago

Was this helpful?

* * *

## Reinforcement Learning Techniques
DPO (Direct Preference Optimization), ORPO (Odds Ratio Preference Optimization), PPO, KTO Reward Modelling all work with Unsloth.

We have Google Colab notebooks for reproducing ORPO, DPO Zephyr, KTO and SimPO:

- [ORPO notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb)

- [DPO Zephyr notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb)

- [KTO notebook](https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing)

- [SimPO notebook](https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing)


We're also in ðŸ¤—Hugging Face's official docs! We're on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto\#dpo-code)    DPO Code

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
python
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0" # Optional set GPU device ID

from unsloth import FastLanguageModel, PatchDPOTrainer
from unsloth import is_bfloat16_supported
PatchDPOTrainer()
import torch
from transformers import TrainingArguments
from trl import DPOTrainer

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/zephyr-sft-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 64,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",\
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 64,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
)

dpo_trainer = DPOTrainer(
    model = model,
    ref_model = None,
    args = TrainingArguments(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 8,
        warmup_ratio = 0.1,
        num_train_epochs = 3,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        seed = 42,
        output_dir = "outputs",
    ),
    beta = 0.1,
    train_dataset = YOUR_DATASET_HERE,
    # eval_dataset = YOUR_DATASET_HERE,
    tokenizer = tokenizer,
    max_length = 1024,
    max_prompt_length = 512,
)
dpo_trainer.train()
```

[PreviousTutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo) [NextTutorial: How to Run DeepSeek-R1 Locally](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally)

Last updated 14 days ago

Was this helpful?

* * *

## DeepSeek-R1 Local Setup
Read our full DeepSeek-R1 blogpost here: [unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)

### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit\#id-1-bit-small-dynamic-vs.-basic)    1-bit (Small) - Dynamic vs. Basic

GGUF Type

Quant

Size (GB)

Seed

Pygame

Background

Accelerate SPACE

Bird shape

Land

Top right score

Pipes

Best Score

Quit

Runnable

Score

Avg Score

Errors

Notes

Dynamic

IQ1\_S

131

3407

1

0.5

1

0.5

0.5

1

0.5

1

1

0

7

score =!inc SyntaxError: invalid syntax

Selects random shapes and colors at the start, but doesn't rotate across trials

Dynamic

IQ1\_S

131

3408

1

1

0.25

1

0.5

1

0.5

1

1

0

7.25

score =B4 NameError: name 'B4' is not defined

Better - selects pipe colors randomnly, but all are just 1 color - should be different. Dropping to ground fails to reset acceleration.

Dynamic

IQ1\_S

131

3409

1

0.5

0.5

0.5

0

1

1

1

1

0

6.5

6.92

score =3D 0 SyntaxError: invalid decimal literal

Too hard to play - acceleration too fast. Pipe colors now are random, but bird shape not changing. Land collison fails.

Basic

IQ1\_S

133

3407

0

0

0

0

0

0

0

0

0

0

0

No code

Fully failed. Repeats "with Dark Colurs" forever

Basic

IQ1\_S

133

3408

0

0

0

0

0

0

0

0

0

0

0

No code

Fully failed. Repeats "Pygame's" forever

Basic

IQ1\_S

133

3409

0

0

0

0

0

0

0

0

0

0

0

0

No code

Fully failed. Repeats "pipe\_x = screen\_height
pipe\_x = screen\_height
pipe\_height = screen\_height - Pipe\_height" forever.

### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit\#id-1-bit-medium-dynamic-vs.-basic)    1-bit (Medium) - Dynamic vs. Basic

GGUF Type

Quant

Size (GB)

Seed

Pygame

Background

Accelerate SPACE

Bird shape

Land

Top right score

Pipes

Best Score

Quit

Runnable

Score

Avg Score

Errors

Notes

Dynamic

IQ1\_M

158

3407

1

1

0.75

1

1

1

1

1

1

1

9.75

None

A bit fast and hard to play.

Dynamic

IQ1\_M

158

3408

1

1

0.5

1

1

1

1

1

1

1

9.5

None

Very good - land should be clearer. Acceleration should be slower.

Dynamic

IQ1\_M

158

3409

1

0.5

1

0.5

0.5

1

0.5

1

1

1

8

9.08

None

Background color does not change across trials.Pipes do not touch the top. No land is seen.

Basic

IQ1\_M

149

3407

1

0

0

0

0

0

0

0

1

0

2

if game\_over: NameError: name 'game\_over' is not defined

Fully failed. Black screen only

Basic

IQ1\_M

149

3408

1

0

0

0

0

0

0

0

1

0

2

No code

Fully failed. Black screen then closes.

Basic

IQ1\_M

149

3409

1

0

0

0

0

0

0

0

0

0

1

1.67

window.fill((100, 100, 255)) Light Blue SyntaxError: invalid syntax && main() NameError: name 'main' is not defined.

Fully failed.

### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit\#id-2-bit-extra-extra-small-dynamic-vs.-basic)    2-bit (Extra extra Small) - Dynamic vs. Basic

GGUF Type

Quant

Size (GB)

Seed

Pygame

Background

Accelerate SPACE

Bird shape

Land

Top right score

Pipes

Best Score

Quit

Runnable

Score

Avg Score

Errors

Notes

Dynamic

IQ2\_XXS

183

3407

1

1

0.5

1

1

1

1

1

1

1

9.5

None

Too hard to play - acceleration too slow. Lags

Dynamic

IQ2\_XXS

183

3408

1

1

1

1

1

1

0.5

0.5

1

0

8

global best\_score SyntaxError: name 'best\_score' is assigned to before global declaration

Had to edit 2 lines - remove global best\_score, and set pipe\_list = \[\]

Dynamic

IQ2\_XXS

183

3409

1

1

1

1

1

1

1

1

1

1

10

9.17

None

Extremely good. Even makes pipes have random distances between them.

Basic

IQ2\_XXS

175

3407

1

0.5

0.5

0.5

1

0

0.5

1

0

0

5

pipe\_color = random.choice(\[(34, 139, 34), (139, 69, 19), (47, 47, 47)) SyntaxError: closing parenthesis ')' does not match opening parenthesis '\[' && pygame.draw.polygon(screen, bird\_color, points) ValueError: points argument must contain more than 2 points\
\
Fails quiting. Same color. Collison detection a bit off. No score\
\
Basic\
\
IQ2\_XXS\
\
175\
\
3408\
\
1\
\
0.5\
\
0.5\
\
0.5\
\
1\
\
1\
\
0.5\
\
1\
\
0\
\
0\
\
6\
\
pipes.append({'x': SCREEN\_WIDTH, 'gap\_y': random.randint(50, SCREEN\_HEIGHT - 150)) SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\
\
Acceleration weird. Chooses 1 color per round. Cannot quit.\
\
Basic\
\
IQ2\_XXS\
\
175\
\
3409\
\
1\
\
1\
\
1\
\
1\
\
1\
\
1\
\
1\
\
0\
\
0.5\
\
0\
\
7.5\
\
6.17\
\
screen = pygame.display.set\_mode((SCREEN\_WIDTH, SCREENHEIGHT)) NameError: name 'SCREENHEIGHT' is not defined. Did you mean: 'SCREEN\_HEIGHT'?\
\
OK. Colors change. Best score does not update. Quit only ESC not Q.\
\
### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit\#dynamic-quantization-trial-output)    **Dynamic Quantization trial output**\
\
IQ1\_S codeIQ1\_M codeIQ2\_XXS code\
\
[12KB\\
\\
inference\_UD-IQ1\_S\_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FqpBdpW55h5mNAzVoTxPI%2Finference_UD-IQ1_S_3407.txt?alt=media&token=37b19689-73e5-46d0-98be-352e515dfdf8)\
\
[11KB\\
\\
inference\_UD-IQ1\_S\_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FTdIrJSqc2VbNJy1bf3w5%2Finference_UD-IQ1_S_3408.txt?alt=media&token=e11f73bb-80be-49e5-91e2-f3a1f5495dcd)\
\
[10KB\\
\\
inference\_UD-IQ1\_S\_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FBk2ZwEIcLmvZQ3jlMLzw%2Finference_UD-IQ1_S_3409.txt?alt=media&token=052885f5-bee9-420d-a9c0-827412ac17c8)\
\
[10KB\\
\\
inference\_UD-IQ1\_M\_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Ft7YmT1H3Nflcy5kAp1LE%2Finference_UD-IQ1_M_3407.txt?alt=media&token=6f62f911-3364-4f92-b311-c1fa9b759370)\
\
[30KB\\
\\
inference\_UD-IQ1\_M\_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FH6BCTeWlJpUkfeEmeqpu%2Finference_UD-IQ1_M_3408.txt?alt=media&token=7727a999-8c0a-4baf-8542-be8686a01630)\
\
[9KB\\
\\
inference\_UD-IQ1\_M\_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FvVJI0H2F9KTNj5kwUCtC%2Finference_UD-IQ1_M_3409.txt?alt=media&token=0f863d41-53d6-4c94-8d57-bf1eeb79ead5)\
\
[29KB\\
\\
inference\_UD-IQ2\_XXS\_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2F26jxRY5mWuon67OfvGtq%2Finference_UD-IQ2_XXS_3407.txt?alt=media&token=daf9bf7d-245e-4b54-b0c0-a6273833835a)\
\
[34KB\\
\\
inference\_UD-IQ2\_XXS\_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FEhjjYN7vAh7gbmR8oXbS%2Finference_UD-IQ2_XXS_3408.txt?alt=media&token=4b50d6dd-2798-44c7-aa92-7e67c09868a4)\
\
[42KB\\
\\
inference\_UD-IQ2\_XXS\_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FXwCSfIf16nTwHzcWepoV%2Finference_UD-IQ2_XXS_3409.txt?alt=media&token=2f7539c9-026d-41e7-b7c7-5738a89ae5d4)\
\
### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit\#non-dynamic-quantization-trial-output)    Non Dynamic Quantization trial output\
\
IQ1\_S basic codeIQ1\_M basic codeIQ2\_XXS basic code\
\
[25KB\\
\\
inference\_basic-IQ1\_S\_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FFtAMzAucSfKMkkmXItTj%2Finference_basic-IQ1_S_3407.txt?alt=media&token=76bfcf47-e1ce-442b-af49-6bfb6af7d046)\
\
[15KB\\
\\
inference\_basic-IQ1\_S\_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2F4NhjCVFMwCwT2OCj0IJ5%2Finference_basic-IQ1_S_3408.txt?alt=media&token=d4715674-3347-400b-9eb6-ae5d4470feeb)\
\
[14KB\\
\\
inference\_basic-IQ1\_S\_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fb0ZW3xs7R7IMryO7n7Yp%2Finference_basic-IQ1_S_3409.txt?alt=media&token=64b8825b-7103-4708-9d12-12770e43b546)\
\
[7KB\\
\\
inference\_basic-IQ1\_M\_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FmZ2TsQEzoGjhGlqUjtmj%2Finference_basic-IQ1_M_3407.txt?alt=media&token=975a30d6-2d90-47eb-9d68-b50fd47337f7)\
\
[7KB\\
\\
inference\_basic-IQ1\_M\_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FIx9TQ99Qpmk7BViNLFBl%2Finference_basic-IQ1_M_3408.txt?alt=media&token=b88e1e5b-4535-4d93-bd67-f81def7377d5)\
\
[12KB\\
\\
inference\_basic-IQ1\_M\_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FDX7XYpJPxXKAMZeGhSrr%2Finference_basic-IQ1_M_3409.txt?alt=media&token=6da9127e-272b-4e74-b990-6657e25eea6b)\
\
[25KB\\
\\
inference\_basic-IQ2\_XXS\_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FajsVHsVqlWpwHk7mY32t%2Finference_basic-IQ2_XXS_3407.txt?alt=media&token=cbbf36a2-0d6a-4a87-8232-45b0b7fcc588)\
\
[34KB\\
\\
inference\_basic-IQ2\_XXS\_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2F4vjncPu2r2D7F5jVOC7I%2Finference_basic-IQ2_XXS_3408.txt?alt=media&token=9ed635a2-bf97-4f49-b26f-6e985d0ab1b7)\
\
[34KB\\
\\
inference\_basic-IQ2\_XXS\_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FJmVOFgrRyXjY4lYZXE96%2Finference_basic-IQ2_XXS_3409.txt?alt=media&token=faad5bff-ba7f-41f1-abd5-7896f17a5b25)\
\
[PreviousTutorial: How to Run DeepSeek-R1 Locally](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally) [NextTutorial: How to Run QwQ-32B effectively](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively)\
\
Last updated 1 month ago\
\
Was this helpful?\
\
* * *

## Vision Model Fine-Tuning
Fine-tuning vision models has numerous use cases across various industries, enabling models to adapt to specific tasks and datasets. We provided 3 example notebooks for vision finetuning.

1. **Llama 3.2 Vision** finetuning for radiography: [Notebook](https://colab.research.google.com/drive/1j0N4XTY1zXXy7mPAhOC1_gMYZ2F2EBlk?usp=sharing)
How can we assist medical professionals in analyzing Xrays, CT Scans & ultrasounds faster.

2. **Qwen 2 VL** finetuning for converting handwriting to LaTeX: [Notebook](https://colab.research.google.com/drive/1whHb54GNZMrNxIsi2wm2EY_-Pvo2QyKh?usp=sharing)
This allows complex math formulas to be easily transcribed as LaTeX without manually writing it.

3. **Pixtral 12B 2409** vision finetuning for general Q&A: [Notebook](https://colab.research.google.com/drive/1K9ZrdwvZRE96qGkCq_e88FgV3MLnymQq?usp=sharing)
One can concatenate general Q&A datasets with more niche datasets to make the finetune not forget base model skills.


It is best to ensure your dataset has images of all the same size/dimensions. Use dimensions of 300-1000px to ensure your training does not take too long or use too many resources.

To finetune vision models, we now allow you to select which parts of the mode to finetune. You can select to only finetune the vision layers, or the language layers, or the attention / MLP layers! We set them all on by default!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fhn0GA65x39KB93DZ4SzY%252Fvision%2520finetuning%2520template.png%3Falt%3Dmedia%26token%3D4a201a6c-53f5-4798-9a1b-0e18d0924d8a&width=768&dpr=4&quality=100&sign=a7bf3919&sv=2)

[PreviousChat Templates](https://docs.unsloth.ai/basics/chat-templates) [NextFinetuning from Last Checkpoint](https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint)

Last updated 24 days ago

Was this helpful?

* * *

## Datasets for LLMs
## [Direct link to heading](https://docs.unsloth.ai/basics/datasets-101\#what-is-a-dataset)    What is a Dataset?

For LLMs, datasets are collections of data that can be used to train our models. In order to be useful for training, text data needs to be in a format that can be tokenized.

## [Direct link to heading](https://docs.unsloth.ai/basics/datasets-101\#tokenization)    Tokenization

Tokenization is the process of breaking text into units called **tokens.** These units can be representative of words, sub-words or even characters.

The typical approach to tokenization with large language models is to tokenize text into sub-word chunks, this allows models to handle inputs that are out of vocabulary.

During training, tokens are embedded in a high-dimensional latent space. By utilizing attention mechanisms, the model fine-tunes these embeddings to produce contextually relevant outputs.

**In Summary:** Tokenization turns raw text into a format that is both machine-readable and retains meaningful information.

## [Direct link to heading](https://docs.unsloth.ai/basics/datasets-101\#data-format)    Data Format

To enable the process of tokenization, datasets need to be in a format that can be read by a tokenizer.

Format

Description

Training Type

Raw Corpus

Raw text from a source such as a website, book, or article.

Continued Pretraining

Instruct

Instructions for the model to follow and an example of the output to aim for.

Supervised fine-tuning (SFT)

Conversation

Multiple-turn conversation between a user and an AI assistant.

Supervised fine-tuning (SFT)

RLHF

Conversation between a user and an AI assistant, with the assistant's responses being ranked by a human evaluator.

Reinforcement Learning

It's worth noting that different styles of format exist for each of these types.

## [Direct link to heading](https://docs.unsloth.ai/basics/datasets-101\#getting-started)    Getting Started

Before we format our data, we want to identify the following:

1

Purpose of dataset

Knowing the purpose of the dataset will help us determine what data we need and format to use.

The purpose could be, adapting a model to a new task such as summarization or improving a model's ability to role-play a specific character.

2

Style of output

The style of output will let us know what sources of data we will use to reach our desired output.

For example, the type of output you want to achieve could be JSON, HTML, text or code. Or perhaps you want it to be Spanish, English or German etc.

3

Data source

When we know the purpose and style of the data we need, we can look for a data source to collect our data from.

The Source of data can be a CSV file, PDF or even a website. You can also synthetically generate data but extra care is required to make sure each example is high quality and relevant.

## [Direct link to heading](https://docs.unsloth.ai/basics/datasets-101\#formatting-our-data)    Formatting Our Data

When we have identified the relevant criteria, and collected the necessary data, we can then format our data into a machine readable format that is ready for training.

For continued pretraining, we use raw text format without specific structure:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
  "text": "Pasta carbonara is a traditional Roman pasta dish. The sauce is made by mixing raw eggs with grated Pecorino Romano cheese and black pepper. The hot pasta is then tossed with crispy guanciale (cured pork cheek) and the egg mixture, creating a creamy sauce from the residual heat. Despite popular belief, authentic carbonara never contains cream or garlic. The dish likely originated in Rome in the mid-20th century, though its exact origins are debated..."
```

This format preserves natural language flow and allows the model to learn from continuous text.

If we are adapting a model to a new task, and intend for the model to output text in a single turn based on a specific set of instructions, we can use **Instruction** format in [Alpaca style](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-6.-alpaca-dataset)

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
"Instruction": "Task we want the model to perform."

"Input": "Optional, but useful, it will essentially be the user's query."

"Output": "The expected result of the task and the output of the model."
```

When we want multiple turns of conversation we can use sharegpt **conversational** format

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
{
  "conversations": [\
    {\
      "from": "human",\
      "value": "Can you help me make pasta carbonara?"\
    },\
    {\
      "from": "assistant",\
      "value": "Would you like the traditional Roman recipe, or a simpler version?"\
    },\
    {\
      "from": "human",\
      "value": "The traditional version please"\
    },\
    {\
      "from": "assistant",\
      "value": "The authentic Roman carbonara uses just a few ingredients: pasta, guanciale, eggs, Pecorino Romano, and black pepper. Would you like the detailed recipe?"\
    }\
  ]
}
```

Each message alternates between `human` and `assistant`, allowing for natural dialogue flow.

**Q:**How can I format my raw data into Alpaca instruct?

**A:** One approach is to create a Python script to process your raw data. If you're working on a summarization task, you can use a local LLM to generate instructions and outputs for each example.

## [Direct link to heading](https://docs.unsloth.ai/basics/datasets-101\#how-big-should-my-dataset-be)    How big should my dataset be?

We generally recommend using at least 100 rows of data for fine-tuning to achieve reasonable results. For optimal performance, a dataset with over 300 rows is preferable, and in this case, more data usually leads to better outcomes. However, the effectiveness of your fine-tuned model depends heavily on the quality of the dataset, so be sure to thoroughly clean and prepare your data.

## [Direct link to heading](https://docs.unsloth.ai/basics/datasets-101\#multiple-datasets)    Multiple datasets

If you have multiple datasets for fine-tuning, you can either:

- Standardize the format of all datasets, combine them into a single dataset, and fine-tune on this unified dataset.

- Use the [Multiple Datasets](https://colab.research.google.com/drive/1njCCbE1YVal9xC83hjdo2hiGItpY_D6t?usp=sharing) notebook to fine-tune on multiple datasets directly.


### [Direct link to heading](https://docs.unsloth.ai/basics/datasets-101\#can-i-fine-tune-the-same-model-multiple-times)    Can I fine-tune the same model multiple times?

You can fine-tune an already fine-tuned model multiple times, but it's best to combine all the datasets and perform the fine-tuning in a single process instead. Training an already fine-tuned model can potentially alter the quality and knowledge acquired during the previous fine-tuning process.

[PreviousTutorial: How to Finetune Llama-3 and Use In Ollama](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama) [NextRunning & Saving Models](https://docs.unsloth.ai/basics/running-and-saving-models)

Last updated 25 days ago

Was this helpful?

* * *

## Saving to Ollama
See our guide below for the complete process on how to save to [Ollama](https://github.com/ollama/ollama):

[ðŸ¦™Tutorial: How to Finetune Llama-3 and Use In Ollama](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama)

## [Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama\#saving-on-google-colab)    Saving on Google Colab

You can save the finetuned model as a small 100MB file called a LoRA adapter like below. You can instead push to the Hugging Face hub as well if you want to upload your model! Remember to get a Hugging Face token via: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) and add your token!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FBz0YDi6Sc2oEP5QWXgSz%252Fimage.png%3Falt%3Dmedia%26token%3D33d9e4fd-e7dc-4714-92c5-bfa3b00f86c4&width=768&dpr=4&quality=100&sign=d6933a01&sv=2)

After saving the model, we can again use Unsloth to run the model itself! Use `FastLanguageModel` again to call it for inference!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FzymBQrqwt4GUmCIN0Iec%252Fimage.png%3Falt%3Dmedia%26token%3D41a110e4-8263-426f-8fa7-cdc295cc8210&width=768&dpr=4&quality=100&sign=b2a207c3&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama\#exporting-to-ollama)    Exporting to Ollama

Finally we can export our finetuned model to Ollama itself! First we have to install Ollama in the Colab notebook:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FqNvGTAGwZKXxkMQqzloS%252Fimage.png%3Falt%3Dmedia%26token%3Ddb503499-0c74-4281-b3bf-400fa20c9ce2&width=768&dpr=4&quality=100&sign=6d57e83a&sv=2)

Then we export the finetuned model we have to llama.cpp's GGUF formats like below:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FZduLjedyfUbTmYqF85pa%252Fimage.png%3Falt%3Dmedia%26token%3Df5bac541-b99f-4d9b-82f7-033f8de780f2&width=768&dpr=4&quality=100&sign=1fdb7647&sv=2)

Reminder to convert `False` to `True` for 1 row, and not change every row to `True`, or else you'll be waiting for a very time! We normally suggest the first row getting set to `True`, so we can export the finetuned model quickly to `Q8_0` format (8 bit quantization). We also allow you to export to a whole list of quantization methods as well, with a popular one being `q4_k_m`.

Head over to [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp) to learn more about GGUF. We also have some manual instructions of how to export to GGUF if you want here: [https://github.com/unslothai/unsloth/wiki#manually-saving-to-gguf](https://github.com/unslothai/unsloth/wiki#manually-saving-to-gguf)

You will see a long list of text like below - please wait 5 to 10 minutes!!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FcuUAx0RNtrQACvU7uWCL%252Fimage.png%3Falt%3Dmedia%26token%3Ddc67801a-a363-48e2-8572-4c6d0d8d0d93&width=768&dpr=4&quality=100&sign=cc7f7372&sv=2)

And finally at the very end, it'll look like below:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FxRh07PEQjAmmz3s2HJUP%252Fimage.png%3Falt%3Dmedia%26token%3D3552a3c9-4d4f-49ee-a31e-0a64327419f0&width=768&dpr=4&quality=100&sign=1e9c9f0d&sv=2)

Then, we have to run Ollama itself in the background. We use `subprocess` because Colab doesn't like asynchronous calls, but normally one just runs `ollama serve` in the terminal / command prompt.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FszDuikrg4HY8lGefwpRQ%252Fimage.png%3Falt%3Dmedia%26token%3Dec1c8762-661d-4b13-ab4f-ed1a7b9fda00&width=768&dpr=4&quality=100&sign=fc72e538&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama\#automatic-modelfile-creation)    Automatic `Modelfile` creation

The trick Unsloth provides is we automatically create a `Modelfile` which Ollama requires! This is a just a list of settings and includes the chat template which we used for the finetune process! You can also print the `Modelfile` generated like below:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fh6inH6k5ggxUP80Gltgj%252Fimage.png%3Falt%3Dmedia%26token%3D805bafb1-2795-4743-9bd2-323ab4f0881e&width=768&dpr=4&quality=100&sign=456e8653&sv=2)

We then ask Ollama to create a model which is Ollama compatible, by using the `Modelfile`

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F1123bSSwmjWXliaRUL5U%252Fimage.png%3Falt%3Dmedia%26token%3D2e72f1a0-1ff8-4189-8d9c-d31e39385555&width=768&dpr=4&quality=100&sign=52a4fd99&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama\#ollama-inference)    Ollama Inference

And we can now call the model for inference if you want to do call the Ollama server itself which is running on your own local machine / in the free Colab notebook in the background. Remember you can edit the yellow underlined part.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fk5mdsJ57hQ1Ar3KY6VXY%252FInference.png%3Falt%3Dmedia%26token%3D8cf0cbf9-0534-4bae-a887-89f45a3de771&width=768&dpr=4&quality=100&sign=8489fe55&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama\#undefined)

[PreviousSaving to GGUF](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf) [NextSaving to VLLM](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-vllm)

Last updated 8 months ago

Was this helpful?

* * *

## Saving Models to VLLM
To save to 16bit for VLLM, use:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
model.save_pretrained_merged("model", tokenizer, save_method = "merged_16bit",)
model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_16bit", token = "")
```

To merge to 4bit to load on HuggingFace, first call `merged_4bit`. Then use `merged_4bit_forced` if you are certain you want to merge to 4bit. I highly discourage you, unless you know what you are going to do with the 4bit model (ie for DPO training for eg or for HuggingFace's online inference engine)

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
model.save_pretrained_merged("model", tokenizer, save_method = "merged_4bit",)
model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_4bit", token = "")
```

To save just the LoRA adapters, either use:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
model.save_pretrained(...) AND tokenizer.save_pretrained(...)
```

Or just use our builtin function to do that:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
model.save_pretrained_merged("model", tokenizer, save_method = "lora",)
model.push_to_hub_merged("hf/model", tokenizer, save_method = "lora", token = "")
```

[PreviousSaving to Ollama](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama) [NextTroubleshooting](https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting)

Last updated 8 months ago

Was this helpful?

* * *

## GRPO and Reinforcement Learning
This article covers everything you need to know about GRPO, Reinforcement Learning (RL) and reward functions, along with tips, and the basics of using GRPO with Unsloth. If you're looking for a quickstart tutorial for using GRPO, see our guide [here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo):

[âš¡Tutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo)

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#grpo-notebooks)    GRPO notebooks:

- [Llama 3.1 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)

- [Phi-4 (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb)

- [Qwen2.5 (3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb)


DeepSeek developed [GRPO](https://unsloth.ai/blog/grpo) (Group Relative Policy Optimization) to train their R1 reasoning models. This RL technique optimizes responses efficiently without a value function model, reducing memory and computational costs compared to PPO (Proximal Policy Optimization).

- Usecases for GRPO isnâ€™t just for code or mathâ€”its reasoning process can enhance tasks like email automation, database retrieval, law, and medicine, greatly improving accuracy based on your dataset and reward function!

- With 15GB VRAM, Unsloth allows you to transform any model up to 17B parameters like Llama 3.1 (8B), Phi-4 (14B), Mistral (7B) or Qwen2.5 (7B) into a reasoning model

- **Minimum requirement:** Just â€¯5GB VRAM is enough to train your own reasoning model locally (for any model with 1.5B parameters or less)

- If you're not getting any reasoning, make sure you have enough training steps and ensure your [reward function/verifier](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier) is working. We provide examples for reward functions [here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-function-examples).

- Previous demonstrations show that you could achieve your own "aha" moment with Qwen2.5 (3B) - but it required 2xA100 GPUs (160GB VRAM). Now, with Unsloth, you can achieve the same "aha" moment using just a single 5GB VRAM GPU.

- Previously, GRPO was only supported for full fine-tuning, but we've made it work with QLoRA and LoRA

- On [**20K context lengths**](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#grpo-requirement-guidelines) for example with 8 generations per prompt, Unsloth uses only 54.3GB of VRAM for Llama 3.1 (8B), whilst standard implementations (+ Flash Attention 2) take **510.8GB (90% less for Unsloth)**.

- Please note, this isnâ€™t fine-tuning DeepSeekâ€™s R1 distilled models or using distilled data from R1 for tuning which Unsloth already supported. This is converting a standard model into a full-fledged reasoning model using GRPO.


In a test example, even though we only trained Phi-4 with 100 steps using GRPO, the results are already clear. The model without GRPO does not have the thinking token, whilst the one trained with GRPO does and also has the correct answer.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FyBeJAvfolzfEYyftji76%252Fprompt%2520only%2520example.png%3Falt%3Dmedia%26token%3D3903995a-d9d5-4cdc-9020-c4efe7fff651&width=768&dpr=4&quality=100&sign=80d59783&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#training-with-grpo)    Training with GRPO

For a tutorial on how to transform any open LLM into a reasoning model using Unsloth & GRPO, [see here](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo).

### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#how-grpo-trains-a-model)    **How GRPO Trains a Model**

1. For each question-answer pair, the model generates multiple possible responses (e.g., 8 variations).

2. Each response is evaluated using reward functions.

3. Training Steps:



- If you have 300 rows of data, that's 300 training steps (or 900 steps if trained for 3 epochs).

- You can increase the number of generated responses per question (e.g., from 8 to 16).


4. The model learns by updating its weights every step.


### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#basics-tips)    Basics/Tips

- Wait for at least **300 steps** for the reward to actually increase. In order to get decent results, you may need to trade for a minimum of 12 hours (this is how GRPO works), but keep in mind this isn't compulsory as you can stop at anytime.

- For optimal results have at least **500 rows of data**. You can try with even 10 rows of data but it's better to have more.

- Each training run will always be different depending on your model, data, reward function/verifier etc. so though 300 steps is what we wrote as the minimum, sometimes it might be 1000 steps or more. So, it depends on various factors.

- If you're using GRPO with Unsloth locally, please "pip install diffusers" as well if you get an error. Please also use the latest version of vLLM.

- Itâ€™s advised to apply GRPO to a model at least **1.5B in parameters** to correctly generate thinking tokens as smaller models may not.

- For GRPO's [**GPU VRAM requirements**](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#grpo-requirement-guidelines) **for QLoRA 4-bit**, the general rule is the model parameters = the amount of VRAM you will need (you can use less VRAM but this just to be safe). The more context length you set, the more VRAM. LoRA 16-bit will use at minimum 4x more VRAM.

- **Continuous fine-tuning is** possible and you can just leave GRPO running in the background.

- In the example notebooks, we use the [**GSM8K dataset**](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#gsm8k-reward-functions), the current most popular choice for R1-style training.

- If youâ€™re using a base model, ensure you have a chat template.

- The more you train with GRPO the better. The best part of GRPO is you don't even need that much data. All you need is a great reward function/verifier and the more time spent training, the better your model will get. Expect your reward vs step to increase as time progresses like this:





![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FUROleqJQ5aEp8MjTCWFf%252Funnamed.png%3Falt%3Dmedia%26token%3D12ca4975-7a0c-4d10-9178-20db28ad0451&width=768&dpr=4&quality=100&sign=a2046ca5&sv=2)

- Training loss tracking for GRPO is now built directly into Unsloth, eliminating the need for external tools like wandb etc. It contains full logging details for all reward functions now including the total aggregated reward function itself.


![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252Fjo7fVFoFG2xbZPgL45el%252FScreenshot%25202025-02-20%2520at%252004-52-52%2520Copy%2520of%2520Yet%2520another%2520copy%2520of%2520Llama3.1_%288B%29-GRPO.ipynb%2520-%2520Colab.png%3Falt%3Dmedia%26token%3D041c17b1-ab98-4ab6-b6fb-8c7e5a8c07df&width=768&dpr=4&quality=100&sign=b8126c85&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#reward-functions-verifier)    Reward Functions / Verifier

In Reinforcement Learning a **Reward Function** and a **Verifier** serve distinct roles in evaluating a modelâ€™s output. In general, you could interpret them as the same thing however, technically they're not but it does not matter as much as they are usually used in conjunction with each other.

**Verifier**:

- Determines whether the generated response is correct or incorrect.

- It does not assign a numerical scoreâ€”it simply verifies correctness.

- Example: If a model generates "5" for "2+2", the verifier checks and labels it as "wrong" (since the correct answer is 4).

- Verifiers can also execute code (e.g., in Python) to validate logic, syntax, and correctness without needing manual evaluation.


**Reward Function**:

- Converts verification results (or other criteria) into a numerical score.

- Example: If an answer is wrong, it might assign a penalty (-1, -2, etc.), while a correct answer could get a positive score (+1, +2).

- It can also penalize based on criteria beyond correctness, such as excessive length or poor readability.


**Key Differences**:

- A **Verifier** checks correctness but doesnâ€™t score.

- A **Reward Function** assigns a score but doesnâ€™t necessarily verify correctness itself.

- A Reward Function _can_ use a Verifier, but they are technically not the same.


### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#understanding-reward-functions)    **Understanding Reward Functions**

GRPO's primary goal is to maximize reward and learn how an answer was derived, rather than simply memorizing and reproducing responses from its training data.

- With every training step, GRPO **adjusts model weights** to maximize the reward. This process fine-tunes the model incrementally.

- **Regular fine-tuning** (without GRPO) only **maximizes next-word prediction probability** but does not optimize for a reward. GRPO **optimizes for a reward function** rather than just predicting the next word.

- You can **reuse data** across multiple epochs.

- **Default reward functions** can be predefined to be used on a wide array of use cases or you can ask ChatGPT/local model to generate them for you.

- Thereâ€™s no single correct way to design reward functions or verifiers - the possibilities are endless. However, they must be well-designed and meaningful, as poorly crafted rewards can unintentionally degrade model performance.


### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#reward-function-examples)    Reward Function Examples

You can refer to the examples below. You can input your generations into an LLM like ChatGPT 4o or Llama 3.1 (8B) and design a reward function and verifier to evaluate it. For example, feed your generations into a LLM of your choice and set a rule: "If the answer sounds too robotic, deduct 3 points." This helps refine outputs based on quality criteria

#### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#example-1-simple-arithmetic-task)    **Example \#1: Simple Arithmetic Task**

- **Question:** `"2 + 2"`

- **Answer:** `"4"`

- **Reward Function 1:**



- If a number is detected â†’ **+1**

- If no number is detected â†’ **-1**


- **Reward Function 2:**



- If the number matches the correct answer â†’ **+3**

- If incorrect â†’ **-3**


- **Total Reward:** _Sum of all reward functions_


#### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#example-2-email-automation-task)    **Example \#2: Email Automation Task**

- **Question:** Inbound email

- **Answer:** Outbound email

- **Reward Functions:**



- If the answer contains a required keyword â†’ **+1**

- If the answer exactly matches the ideal response â†’ **+1**

- If the response is too long â†’ **-1**

- If the recipient's name is included â†’ **+1**

- If a signature block (phone, email, address) is present â†’ **+1**


### [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#gsm8k-reward-functions)    GSM8K Reward Functions

In our examples, we've built on existing GSM8K reward functions by [@willccbb](https://x.com/willccbb) which is popular and shown to be quite effective:

- **correctness\_reward\_func** â€“ Rewards exact label matches.

- **int\_reward\_func** â€“ Encourages integer-only answers.

- **soft\_format\_reward\_func** â€“ Checks structure but allows minor newline mismatches.

- **strict\_format\_reward\_func** â€“ Ensures response structure matches the prompt, including newlines.

- **xmlcount\_reward\_func** â€“ Ensures exactly one of each XML tag in the response.


## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#using-vllm)    Using vLLM

You can now use [vLLM](https://github.com/vllm-project/vllm/) directly in your finetuning stack, which allows for much more throughput and allows you to finetune and do inference on the model at the same time! On 1x A100 40GB, expect 4000 tokens / s or so with Unslothâ€™s dynamic 4bit quant of Llama 3.2 3B Instruct. On a 16GB Tesla T4 (free Colab GPU), you can get 300 tokens / s.

We also magically removed double memory usage when loading vLLM and Unsloth together, allowing for savings of 5GB or so for Llama 3.1 8B and 3GB for Llama 3.2 3B. Unsloth could originally finetune Llama 3.3 70B Instruct in 1x 48GB GPU with Llama 3.3 70B weights taking 40GB of VRAM. If we do not remove double memory usage, then weâ€™ll need >= 80GB of VRAM when loading Unsloth and vLLM together.

But with Unsloth, you can still finetune and get the benefits of fast inference in one package in under 48GB of VRAM! To use fast inference, first install vllm, and instantiate Unsloth with fast\_inference:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
pip install unsloth vllm
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct",
    fast_inference = True,
)
model.fast_generate(["Hello!"])
```

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#grpo-requirement-guidelines)    GRPO Requirement Guidelines

When youâ€™re using Unsloth to do GRPO, we smartly reduce VRAM usage by over 90% when compared to standard implementations with Flash Attention 2 by using multiple tricks! On 20K context lengths for example with 8 generations per prompt, Unsloth uses only **54.3GB of VRAM for Llama 3.1 8B**, whilst standard implementations take **510.8GB (90% less for Unsloth)**.

1. For GRPO's **GPU VRAM requirements for QLoRA 4-bit**, the general rule is the model parameters = the amount of VRAM you will need (you can use less VRAM but this just to be safe). The more context length you set, the more VRAM. LoRA 16-bit will use at minimum 4x more VRAM.

2. Our new memory efficient linear kernels for GRPO slashes memory usage by 8x or more. This shaves 68.5GB of memory, whilst being actually faster through the help of torch.compile!

3. We leverage our smart [Unsloth gradient checkpointing](https://unsloth.ai/blog/long-context) algorithm which we released a while ago. It smartly offloads intermediate activations to system RAM asynchronously whilst being only 1% slower. This shaves 52GB of memory.

4. Unsloth also uses the same GPU / CUDA memory space as the underlying inference engine (vLLM), unlike implementations in other packages. This shaves 16GB of memory.


Metrics

Unsloth

Standard + FA2

Training Memory Cost (GB)

42GB

414GB

GRPO Memory Cost (GB)

9.8GB

78.3GB

Inference Cost (GB)

0GB

16GB

Inference KV Cache for 20K context length (GB)

2.5GB

2.5GB

Total Memory Usage

54.33GB (90% less)

510.8GB

In typical standard GRPO implementations, you need to create 2 logits of size (8. 20K) to calculate the GRPO loss. This takes 2 \* 2 bytes \* 8 (num generations) \* 20K (context length) \* 128256 (vocabulary size) = 78.3GB in VRAM.

Unsloth shaves 8x memory usage for long context GRPO, so we need only an extra 9.8GB in extra VRAM for 20K context lengths!

We also need to from the KV Cache in 16bit. Llama 3.1 8B has 32 layers, and both K and V are 1024 in size. So memory usage for 20K context length = 2 \* 2 bytes \* 32 layers \* 20K context length \* 1024 = 2.5GB per batch. We would set the batch size for vLLM to 8, but we shall leave it at 1 for our calculations to save VRAM. Otherwise you will need 20GB for the KV cache.

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl\#how-grpo-works)    How GRPO Works:

DeepSeekâ€™s researchers observed an "aha moment" when training R1-Zero with pure reinforcement learning (RL). The model learned to extend its thinking time by reevaluating its initial approach, without any human guidance or predefined instructions.

1. The model generates groups of responses.

2. Each response is scored based on correctness or another metric created by some set reward function rather than an LLM reward model.

3. The average score of the group is computed.

4. Each response's score is compared to the group average.

5. The model is reinforced to favor higher-scoring responses.


As an example, assume we want a model to solve:

What is 1+1? >> Chain of thought/working out >> The answer is 2.

What is 2+2? >> Chain of thought/working out >> The answer is 4.

Originally, one had to collect large swathes of data to fill the working out / chain of thought process. But GRPO (the algorithm DeepSeek uses) or other RL algorithms can steer the model to automatically exhibit reasoning capabilities and create the reasoning trace. Instead, we need to create good reward functions or verifiers. For example, if it gets the correct answer, give it a score of 1. If some words are mis-spelt, minus 0.1. And so on! We can provide many many functions to reward the process.

[PreviousFine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-guide) [NextTutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo)

Last updated 2 days ago

Was this helpful?

* * *

## Unsloth AI Documentation
* * *

## Fine-tuning Guide
## [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#id-1.-understand-fine-tuning)    1\. Understand Fine-tuning

Fine-tuning an LLM customizes its behavior, enhances + injects knowledge, and optimizes performance for domains/specific tasks. For example:

- **GPT-4** serves as a base model; however, OpenAI fine-tuned it to better comprehend instructions and prompts, leading to the creation of ChatGPT-4 which everyone uses today.

- â€‹ **DeepSeek-R1-Distill-Llama-8B** is a fine-tuned version of Llama-3.1-8B. DeepSeek utilized data generated by DeepSeek-R1, to fine-tune Llama-3.1-8B. This process, known as **distillation**, injects the data into the Llama model to learn reasoning capabilities.


With [Unsloth](https://github.com/unslothai/unsloth), you can fine-tune for free on Colab, Kaggle, or locally with just 3GB VRAM by using our [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks). By fine-tuning a pre-trained model (e.g. Llama-3.1-8B) on a specialized dataset, you can:

- **Update Knowledge**: Introduce new domain-specific information.

- **Customize Behavior**: Adjust the modelâ€™s tone, personality, or response style.

- **Optimize for Tasks**: Improve accuracy and relevance for specific use cases.


**Example usecases**:

- Train LLM to predict if a headline impacts a company positively or negatively.

- Use historical customer interactions for more accurate and custom responses.

- Fine-tune LLM on legal texts for contract analysis, case law research, and compliance.


You can think of a fine-tuned model as a specialized agent designed to do specific tasks more effectively and efficiently. **Fine-tuning can replicate all of RAG's capabilities**, but not vice versa. Read more FAQ here:

[ðŸ¤”FAQ + Is Fine-tuning Right For Me?](https://docs.unsloth.ai/get-started/beginner-start-here/faq-+-is-fine-tuning-right-for-me)

## [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#id-2.-choose-the-right-model--method)    2\. Choose the Right Model + Method

If you're a beginner, it is best to start with a small instruct model like Llama 3.1 (8B) and experiment from there. You'll also need to decide between QLoRA and LoRA training:

- **LoRA:** Fine-tunes small, trainable matrices in 16-bit without updating all model weights.

- **QLoRA:** Combines LoRA with 4-bit quantization to handle very large models with minimal resources.


You can change the model name to whichever model you like by matching it with model's name on Hugging Face e.g. 'unsloth/llama-3.1-8b-bnb-4bit'.

There are 3 other settings which you can toggle:

- `max_seq_length = 2048` â€“ Controls context length. While Llama-3 supports 8192, we recommend 2048 for testing. Unsloth enables 4Ã— longer context fine-tuning.

- `dtype = None` â€“ Defaults to None; use `torch.float16` or `torch.bfloat16` for newer GPUs.

- `load_in_4bit = True` â€“ Enables 4-bit quantization, reducing memory use 4Ã— for fine-tuning on 16GB GPUs. Disabling it on larger GPUs (e.g., H100) slightly improves accuracy (1â€“2%).


We recommend starting with QLoRA, as it is one of the most accessible and effective methods for training models. Our [dynamic 4-bit](https://unsloth.ai/blog/phi4) quants, the accuracy loss for QLoRA compared to LoRA is now largely recovered.

You can also do [reasoning (GRPO)](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl), [vision](https://docs.unsloth.ai/basics/vision-fine-tuning), [reward modelling](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto) (DPO, ORPO, KTO), [continued pretraining](https://docs.unsloth.ai/basics/continued-pretraining), text completion and other training methodologies with Unsloth.

Read our detailed guide on choosing the right model:

[â“What Model Should I Use?](https://docs.unsloth.ai/get-started/beginner-start-here/what-model-should-i-use)

## [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#id-3.-your-dataset)    3\. Your Dataset

For LLMs, datasets are collections of data that can be used to train our models. In order to be useful for training, text data needs to be in a format that can be tokenized.

- You will need to create a dataset usually with 2 columns - question and answer. The quality and amount will largely reflect the end result of your fine-tune so it's imperative to get this part right.

- You can synthetically generate data and structure your dataset (into QA pairs) using ChatGPT or local LLMs.

- Fine-tuning can permanently incorporate an existing repository of documents and continuously expand its knowledge base, but just dumping data alone wonâ€™t work as well. For optimal results, curate a well-structured dataset, ideally as question-answer pairs. This enhances learning, understanding, and response accuracy.

- But, that's not always the case, e.g. if you just dump all your code data for fine-tuning, your model can yield significant performance improvements, even without structured formatting. So it really depends on your use case.


_**Read more about creating your dataset:**_

[ðŸ“ˆDatasets 101](https://docs.unsloth.ai/basics/datasets-101)

For most of our notebook examples, we utilize the [Alpaca dataset](https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama#id-6.-alpaca-dataset) however other notebooks like Vision will use different datasets which may need images in the answer ouput as well.

## [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#id-4.-understand-model-parameters)    4\. Understand Model Parameters

There are millions of hyperparameters combinations and choosing the right numbers are crucial to a good result. You can edit the parameters (numbers) below, but you can ignore it, since we already select quite reasonable numbers.

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FW1P2qmzGQGDAXQ0pXhRq%252Fparameters.png%3Falt%3Dmedia%26token%3Df146c646-ca31-4459-b1de-499bd1d23fd1&width=768&dpr=4&quality=100&sign=3bae97dd&sv=2)

The goal is to change these numbers to increase accuracy, but also **counteract over-fitting**. Over-fitting is when you make the language model memorize a dataset, and not be able to answer novel new questions. We want to a final model to answer unseen questions, and not do memorization. Here are the key parameters:

#### [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#learning-rate)    **Learning Rate**

Defines how much the modelâ€™s weights adjust per training step.

- **Higher Learning Rates**: Faster training, risk of overfitting.

- **Lower Learning Rates**: More stable training, may require more epochs.

- **Typical Range**: 1e-4 (0.0001) to 5e-5 (0.00005).


#### [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#epochs)    **Epochs**

Number of times the model sees the full training dataset.

- **Recommended:** 1-3 epochs (anything more than 3 is generally not optimal unless you want your model to have much less hallucinations but also less creativity)

- **More Epochs**: Better learning, higher risk of overfitting.

- **Fewer Epochs**: May undertrain the model.


_**For a complete guide on how hyperparameters affect training, see:**_

[ðŸ§ LoRA Parameters Encyclopedia](https://docs.unsloth.ai/get-started/beginner-start-here/lora-parameters-encyclopedia)

### [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#avoiding-overfitting-and-underfitting)    **Avoiding Overfitting & Underfitting**

#### [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#overfitting-too-specialized)    **Overfitting** (Too Specialized)

The model memorizes training data, failing to generalize to unseen inputs. Solution:

- Reduce learning rate.

- Lower the number of training epochs.

- Combine your dataset with a generic dataset e.g. ShareGPT

- Increase dropout rate to introduce regularization.


#### [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#underfitting-too-generic)    **Underfitting** (Too Generic)

Though quite rare, sometimes your model may fail to learn from training data, providing responses similar to the base model. Solution:

- Increase learning rate.

- Train for more epochs.

- Use a more domain-relevant dataset.


Fine-tuning has no single "best" approach, only best practices. Experimentation is key to finding what works for your needs. Our notebooks auto-set optimal parameters based on evidence from research papers and past experiments.

## [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#id-5.-installing--requirements)    5\. Installing + Requirements

We would recommend beginners to utilise our pre-made [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) first as it's the easiest way to get started with guided steps. However, if installing locally is a must, you can install and use Unsloth - just make sure you have all the right requirements necessary. Also depending on the model and quantization you're using, you'll need enough VRAM and resources. See all the details here:

[ðŸ› ï¸Unsloth Requirements](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements)

Next, you'll need to install Unsloth. Unsloth currently only supports Windows and Linux devices. Once you install Unsloth, you can copy and paste our notebooks and use them in your own local environment. We have many installation methods:

[ðŸ“¥Installing + Updating](https://docs.unsloth.ai/get-started/installing-+-updating)

## [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#id-6.-training--evaluation)    6\. Training + Evaluation

Once you have everything set, it's time to train! If something's not working, remember you can always change hyperparameters, your dataset etc.

You will see a log of some numbers whilst training! This is the training loss, and your job is to set parameters to make this go to as close to 0.5 as possible! If your finetune is not reaching 1, 0.8 or 0.5, you might have to adjust some numbers. If your loss goes to 0, that's probably not a good sign as well!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FxwOA09mtcimcQOCjP4PG%252Fimage.png%3Falt%3Dmedia%26token%3D39a0f525-6d4e-4c3b-af0d-82d8960d87be&width=768&dpr=4&quality=100&sign=853c0062&sv=2)

The training loss will appear as numbers

We generally recommend keeping the default settings unless you need longer training or larger batch sizes.

- `per_device_train_batch_size = 2` â€“ Increase for better GPU utilization but beware of slower training due to padding. Instead, increase `gradient_accumulation_steps` for smoother training.

- `gradient_accumulation_steps = 4` â€“ Simulates a larger batch size without increasing memory usage.

- `max_steps = 60` â€“ Speeds up training. For full runs, replace with `num_train_epochs = 1` (1â€“3 epochs recommended to avoid overfitting).

- `learning_rate = 2e-4` â€“ Lower for slower but more precise fine-tuning. Try values like `1e-4`, `5e-5`, or `2e-5`.


### [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#evaluation)    Evaluation

In order to evaluate, you could do manually evaluation by just chatting with the model and see if it's to your liking. You can also enable evaluation for Unsloth, but keep in mind it can be time-consuming depending on the dataset size. To speed up evaluation you can: reduce the evaluation dataset size or set `evaluation_steps = 100`.

For testing, you can also take 20% of your training data and use that for testing. If you already used all of the training data, then you have to manually evaluate it. You can also use automatic eval tools like EleutherAIâ€™s [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness). Keep in mind that automated tools may not perfectly align with your evaluation criteria.

## [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#id-7.-running--saving-the-model)    7\. Running + Saving the model

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FRX9Byv1hlSpvmonT1PLw%252Fimage.png%3Falt%3Dmedia%26token%3D6043cd8c-c6a3-4cc5-a019-48baeed3b5a2&width=768&dpr=4&quality=100&sign=7c7ce43f&sv=2)

Now let's run the model after we completed the training process! You can edit the yellow underlined part! In fact, because we created a multi turn chatbot, we can now also call the model as if it saw some conversations in the past like below:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F6DXSlsHkN8cZiiAxAV0Z%252Fimage.png%3Falt%3Dmedia%26token%3D846307de-7386-4bbe-894e-7d9e572244fe&width=768&dpr=4&quality=100&sign=6482b95b&sv=2)

Reminder Unsloth itself provides **2x faster inference** natively as well, so always do not forget to call `FastLanguageModel.for_inference(model)`. If you want the model to output longer responses, set `max_new_tokens = 128` to some larger number like 256 or 1024. Notice you will have to wait longer for the result as well!

### [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#saving-the-model)    Saving the model

For saving and using your model in desired inference engines like Ollama, vLLM, Open WebUI, we can have more information here:

[ðŸ–¥ï¸Running & Saving Models](https://docs.unsloth.ai/basics/running-and-saving-models)

We can now save the finetuned model as a small 100MB file called a LoRA adapter like below. You can instead push to the Hugging Face hub as well if you want to upload your model! Remember to get a Hugging Face token via: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) and add your token!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FBz0YDi6Sc2oEP5QWXgSz%252Fimage.png%3Falt%3Dmedia%26token%3D33d9e4fd-e7dc-4714-92c5-bfa3b00f86c4&width=768&dpr=4&quality=100&sign=d6933a01&sv=2)

After saving the model, we can again use Unsloth to run the model itself! Use `FastLanguageModel` again to call it for inference!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FzymBQrqwt4GUmCIN0Iec%252Fimage.png%3Falt%3Dmedia%26token%3D41a110e4-8263-426f-8fa7-cdc295cc8210&width=768&dpr=4&quality=100&sign=b2a207c3&sv=2)

## [Direct link to heading](https://docs.unsloth.ai/get-started/fine-tuning-guide\#id-8.-were-done)    8\. We're done!

You've successfully finetuned a language model and exported it to your desired inference engine with Unsloth!

To learn more about finetuning tips and tricks, head over to our blogs which provide tremendous and educational value: [https://unsloth.ai/blog/](https://unsloth.ai/blog/)

If you need any help on finetuning, you can also join our Discord server [here](https://discord.gg/unsloth). Thanks for reading and hopefully this was helpful!

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252FPEvp4xsbVObJZ1lawDj8%252Fsloth%2520sparkling%2520square.png%3Falt%3Dmedia%26token%3D876bf67d-7470-4977-a6cc-3ee02cc9440b&width=768&dpr=4&quality=100&sign=d5ba19e6&sv=2)

[PreviousGoogle Colab](https://docs.unsloth.ai/get-started/installing-+-updating/google-colab) [NextReasoning - GRPO & RL](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl)

Last updated 15 hours ago

Was this helpful?

* * *

## Faster Inference Guide
Unsloth supports natively 2x faster inference. For our inference only notebook, click [here](https://colab.research.google.com/drive/1aqlNQi7MMJbynFDyOQteD2t0yVfjb9Zh?usp=sharing).

All QLoRA, LoRA and non LoRA inference paths are 2x faster. This requires no change of code or any new dependencies.

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "lora_model", # YOUR MODEL YOU USED FOR TRAINING
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)
FastLanguageModel.for_inference(model) # Enable native 2x faster inference
text_streamer = TextStreamer(tokenizer)
_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 64)
```

#### [Direct link to heading](https://docs.unsloth.ai/basics/running-and-saving-models/inference\#notimplementederror-a-utf-8-locale-is-required.-got-ansi)    NotImplementedError: A UTF-8 locale is required. Got ANSI

Sometimes when you execute a cell [this error](https://github.com/googlecolab/colabtools/issues/3409) can appear. To solve this, in a new cell, run the below:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
import locale
locale.getpreferredencoding = lambda: "UTF-8"
```

[PreviousTroubleshooting](https://docs.unsloth.ai/basics/running-and-saving-models/troubleshooting) [NextContinued Pretraining](https://docs.unsloth.ai/basics/continued-pretraining)

Last updated 2 months ago

Was this helpful?

* * *

## Page Not Found
## Page not found

The page you are looking for doesn't exist.

* * *

## Reinforcement Learning Techniques
DPO (Direct Preference Optimization), ORPO (Odds Ratio Preference Optimization), PPO, KTO Reward Modelling all work with Unsloth.

We have Google Colab notebooks for reproducing ORPO, DPO Zephyr, KTO and SimPO:

- [ORPO notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb)

- [DPO Zephyr notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb)

- [KTO notebook](https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing)

- [SimPO notebook](https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing)


We're also in ðŸ¤—Hugging Face's official docs! We're on the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth).

## [Direct link to heading](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto\#dpo-code)    DPO Code

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
python
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0" # Optional set GPU device ID

from unsloth import FastLanguageModel, PatchDPOTrainer
from unsloth import is_bfloat16_supported
PatchDPOTrainer()
import torch
from transformers import TrainingArguments
from trl import DPOTrainer

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/zephyr-sft-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 64,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",\
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 64,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
)

dpo_trainer = DPOTrainer(
    model = model,
    ref_model = None,
    args = TrainingArguments(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 8,
        warmup_ratio = 0.1,
        num_train_epochs = 3,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        seed = 42,
        output_dir = "outputs",
    ),
    beta = 0.1,
    train_dataset = YOUR_DATASET_HERE,
    # eval_dataset = YOUR_DATASET_HERE,
    tokenizer = tokenizer,
    max_length = 1024,
    max_prompt_length = 512,
)
dpo_trainer.train()
```

[PreviousTutorial: Train your own Reasoning model with GRPO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/tutorial-train-your-own-reasoning-model-with-grpo) [NextTutorial: How to Run DeepSeek-R1 Locally](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally)

Last updated 14 days ago

Was this helpful?

* * *

## Error Troubleshooting Guide
* * *

## Run DeepSeek-R1 Locally
A guide on how you can run our 1.58-bit Dynamic Quants for DeepSeek-R1 using llama.cpp.

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally\#using-llama.cpp-recommended)    Using llama.cpp (recommended)

1. Do not forget about `<ï½œUserï½œ>` and `<ï½œAssistantï½œ>` tokens! - Or use a chat template formatter

2. Obtain the latest `llama.cpp` at: [github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp). You can follow the build instructions below as well:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
apt-get update
apt-get install build-essential cmake curl libcurl4-openssl-dev -y
git clone https://github.com/ggerganov/llama.cpp
cmake llama.cpp -B llama.cpp/build \
    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON
cmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split
cp llama.cpp/build/bin/llama-* llama.cpp
```

1. It's best to use `--min-p 0.05` to counteract very rare token predictions - I found this to work well especially for the 1.58bit model.

2. Download the model via:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
# pip install huggingface_hub hf_transfer
# import os # Optional for faster downloading
# os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

from huggingface_hub import snapshot_download
snapshot_download(
  repo_id = "unsloth/DeepSeek-R1-GGUF",
  local_dir = "DeepSeek-R1-GGUF",
  allow_patterns = ["*UD-IQ1_S*"], # Select quant type UD-IQ1_S for 1.58bit
)
```

1. Example with Q4\_0 K quantized cache **Notice -no-cnv disables auto conversation mode**


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
   ./llama.cpp/llama-cli \
	  --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
	  --cache-type-k q4_0 \
	  --threads 12 -no-cnv --prio 2 \
	  --temp 0.6 \
	  --ctx-size 8192 \
	  --seed 3407 \
	  --prompt "<ï½œUserï½œ>Create a Flappy Bird game in Python.<ï½œAssistantï½œ>"
```

Example output:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
 <think>
 Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.
 Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.
 Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.
 I also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.
 Is there a scenario where 1 plus 1 wouldn't be 2? I can't think of any...
```

1. If you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
  ./llama.cpp/llama-cli \
    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --cache-type-k q4_0 \
    --threads 12 -no-cnv --prio 2 \
    --n-gpu-layers 7 \
    --temp 0.6 \
    --ctx-size 8192 \
    --seed 3407 \
    --prompt "<ï½œUserï½œ>Create a Flappy Bird game in Python.<ï½œAssistantï½œ>"
```

1. If you want to merge the weights together, use this script:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
./llama.cpp/llama-gguf-split --merge \
    DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    merged_file.gguf
```

1. DeepSeek R1 has 61 layers. For example with a 24GB GPU or 80GB GPU, you can expect to offload after rounding down (reduce by 1 if it goes out of memory):


Quant

File Size

24GB GPU

80GB GPU

2x80GB GPU

1.58bit

131GB

7

33

All layers 61

1.73bit

158GB

5

26

57

2.22bit

183GB

4

22

49

2.51bit

212GB

2

19

32

### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally\#running-on-mac-apple-devices)    Running on Mac / Apple devices

For Apple Metal devices, be careful of --n-gpu-layers. If you find the machine going out of memory, reduce it. For a 128GB unified memory machine, you should be able to offload 59 layers or so.

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
./llama.cpp/llama-cli \
    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --cache-type-k q4_0 \
    --threads 16 \
    --prio 2 \
    --temp 0.6 \
    --ctx-size 8192 \
    --seed 3407 \
    --n-gpu-layers 59 \
    -no-cnv \
    --prompt "<ï½œUserï½œ>Create a Flappy Bird game in Python.<ï½œAssistantï½œ>"
```

### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally\#run-in-ollama-open-webui)    Run in Ollama/Open WebUI

Open WebUI has made an step-by-step tutorial on how to run R1 here: [docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/](https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/)

If you want to use Ollama for inference on GGUFs, you need to first merge the 3 GGUF split files into 1 like the code below. Then you will need to run the model locally.

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
./llama.cpp/llama-gguf-split --merge \
  DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
	merged_file.gguf
```

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally\#deepseek-chat-template)    DeepSeek Chat Template

All distilled versions and the main 671B R1 model use the same chat template:

`<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>What is 1+1?<ï½œAssistantï½œ>It's 2.<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>Explain more!<ï½œAssistantï½œ>`

A BOS is forcibly added, and an EOS separates each interaction. To counteract double BOS tokens during inference, you should only call _tokenizer.encode(..., add\_special\_tokens = False)_ since the chat template auto adds a BOS token as well.
For llama.cpp / GGUF inference, you should skip the BOS since itâ€™ll auto add it.

`<ï½œUserï½œ>What is 1+1?<ï½œAssistantï½œ>`

The <think> and </think> tokens get their own designated tokens. For the distilled versions for Qwen and Llama, some tokens are re-mapped, whilst Qwen for example did not have a BOS token, so <\|object\_ref\_start\|> had to be used instead.
**Tokenizer ID Mappings:**

Token

R1

Distill Qwen

Distill Llama

<think>

128798

151648

128013

</think>

128799

151649

128014

<\|begin\_of\_sentence\|>

0

151646

128000

<\|end\_of\_sentence\|>

1

151643

128001

<\|User\|>

128803

151644

128011

<\|Assistant\|>

128804

151645

128012

Padding token

2

151654

128004

Original tokens in models:

Token

Qwen 2.5 32B Base

Llama 3.3 70B Instruct

<think>

<\|box\_start\|>

<\|reserved\_special\_token\_5\|>

</think>

<\|box\_end\|>

<\|reserved\_special\_token\_6\|>

<ï½œbeginâ–ofâ–sentenceï½œ>

<\|object\_ref\_start\|>

<\|begin\_of\_text\|>

<ï½œendâ–ofâ–sentenceï½œ>

<\|endoftext\|>

<\|end\_of\_text\|>

<ï½œUserï½œ>

<\|im\_start\|>

<\|reserved\_special\_token\_3\|>

<ï½œAssistantï½œ>

<\|im\_end\|>

<\|reserved\_special\_token\_4\|>

Padding token

<\|vision\_pad\|>

<\|finetune\_right\_pad\_id\|>

All Distilled and the original R1 versions seem to have accidentally assigned the padding token to <ï½œendâ–ofâ–sentenceï½œ>, which is mostly not a good idea, especially if you want to further finetune on top of these reasoning models. This will cause endless infinite generations, since most frameworks will mask the EOS token out as -100.

We fixed all distilled and the original R1 versions with the correct padding token (Qwen uses <\|vision\_pad\|>, Llama uses <\|finetune\_right\_pad\_id\|>, and R1 uses <ï½œâ–padâ–ï½œ> or our own added <ï½œPADâ–TOKENï½œ>.

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally\#gguf-r1-table)    GGUF R1 Table

MoE Bits

Type

Disk Size

Accuracy

Link

Details

1.58bit

UD-IQ1\_S

**131GB**

Fair

[Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S)

MoE all 1.56bit. `down_proj` in MoE mixture of 2.06/1.56bit

1.73bit

UD-IQ1\_M

**158GB**

Good

[Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M)

MoE all 1.56bit. `down_proj` in MoE left at 2.06bit

2.22bit

UD-IQ2\_XXS

**183GB**

Better

[Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS)

MoE all 2.06bit. `down_proj` in MoE mixture of 2.5/2.06bit

2.51bit

UD-Q2\_K\_XL

**212GB**

Best

[Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL)

MoE all 2.5bit. `down_proj` in MoE mixture of 3.5/2.5bit

[PreviousReinforcement Learning - DPO, ORPO & KTO](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl/reinforcement-learning-dpo-orpo-and-kto) [NextDeepSeek-R1 Dynamic 1.58-bit](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit)

Last updated 1 month ago

Was this helpful?

* * *

## DeepSeek-R1 Local Setup
Read our full DeepSeek-R1 blogpost here: [unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)

### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit\#id-1-bit-small-dynamic-vs.-basic)    1-bit (Small) - Dynamic vs. Basic

GGUF Type

Quant

Size (GB)

Seed

Pygame

Background

Accelerate SPACE

Bird shape

Land

Top right score

Pipes

Best Score

Quit

Runnable

Score

Avg Score

Errors

Notes

Dynamic

IQ1\_S

131

3407

1

0.5

1

0.5

0.5

1

0.5

1

1

0

7

score =!inc SyntaxError: invalid syntax

Selects random shapes and colors at the start, but doesn't rotate across trials

Dynamic

IQ1\_S

131

3408

1

1

0.25

1

0.5

1

0.5

1

1

0

7.25

score =B4 NameError: name 'B4' is not defined

Better - selects pipe colors randomnly, but all are just 1 color - should be different. Dropping to ground fails to reset acceleration.

Dynamic

IQ1\_S

131

3409

1

0.5

0.5

0.5

0

1

1

1

1

0

6.5

6.92

score =3D 0 SyntaxError: invalid decimal literal

Too hard to play - acceleration too fast. Pipe colors now are random, but bird shape not changing. Land collison fails.

Basic

IQ1\_S

133

3407

0

0

0

0

0

0

0

0

0

0

0

No code

Fully failed. Repeats "with Dark Colurs" forever

Basic

IQ1\_S

133

3408

0

0

0

0

0

0

0

0

0

0

0

No code

Fully failed. Repeats "Pygame's" forever

Basic

IQ1\_S

133

3409

0

0

0

0

0

0

0

0

0

0

0

0

No code

Fully failed. Repeats "pipe\_x = screen\_height
pipe\_x = screen\_height
pipe\_height = screen\_height - Pipe\_height" forever.

### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit\#id-1-bit-medium-dynamic-vs.-basic)    1-bit (Medium) - Dynamic vs. Basic

GGUF Type

Quant

Size (GB)

Seed

Pygame

Background

Accelerate SPACE

Bird shape

Land

Top right score

Pipes

Best Score

Quit

Runnable

Score

Avg Score

Errors

Notes

Dynamic

IQ1\_M

158

3407

1

1

0.75

1

1

1

1

1

1

1

9.75

None

A bit fast and hard to play.

Dynamic

IQ1\_M

158

3408

1

1

0.5

1

1

1

1

1

1

1

9.5

None

Very good - land should be clearer. Acceleration should be slower.

Dynamic

IQ1\_M

158

3409

1

0.5

1

0.5

0.5

1

0.5

1

1

1

8

9.08

None

Background color does not change across trials.Pipes do not touch the top. No land is seen.

Basic

IQ1\_M

149

3407

1

0

0

0

0

0

0

0

1

0

2

if game\_over: NameError: name 'game\_over' is not defined

Fully failed. Black screen only

Basic

IQ1\_M

149

3408

1

0

0

0

0

0

0

0

1

0

2

No code

Fully failed. Black screen then closes.

Basic

IQ1\_M

149

3409

1

0

0

0

0

0

0

0

0

0

1

1.67

window.fill((100, 100, 255)) Light Blue SyntaxError: invalid syntax && main() NameError: name 'main' is not defined.

Fully failed.

### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit\#id-2-bit-extra-extra-small-dynamic-vs.-basic)    2-bit (Extra extra Small) - Dynamic vs. Basic

GGUF Type

Quant

Size (GB)

Seed

Pygame

Background

Accelerate SPACE

Bird shape

Land

Top right score

Pipes

Best Score

Quit

Runnable

Score

Avg Score

Errors

Notes

Dynamic

IQ2\_XXS

183

3407

1

1

0.5

1

1

1

1

1

1

1

9.5

None

Too hard to play - acceleration too slow. Lags

Dynamic

IQ2\_XXS

183

3408

1

1

1

1

1

1

0.5

0.5

1

0

8

global best\_score SyntaxError: name 'best\_score' is assigned to before global declaration

Had to edit 2 lines - remove global best\_score, and set pipe\_list = \[\]

Dynamic

IQ2\_XXS

183

3409

1

1

1

1

1

1

1

1

1

1

10

9.17

None

Extremely good. Even makes pipes have random distances between them.

Basic

IQ2\_XXS

175

3407

1

0.5

0.5

0.5

1

0

0.5

1

0

0

5

pipe\_color = random.choice(\[(34, 139, 34), (139, 69, 19), (47, 47, 47)) SyntaxError: closing parenthesis ')' does not match opening parenthesis '\[' && pygame.draw.polygon(screen, bird\_color, points) ValueError: points argument must contain more than 2 points\
\
Fails quiting. Same color. Collison detection a bit off. No score\
\
Basic\
\
IQ2\_XXS\
\
175\
\
3408\
\
1\
\
0.5\
\
0.5\
\
0.5\
\
1\
\
1\
\
0.5\
\
1\
\
0\
\
0\
\
6\
\
pipes.append({'x': SCREEN\_WIDTH, 'gap\_y': random.randint(50, SCREEN\_HEIGHT - 150)) SyntaxError: closing parenthesis ')' does not match opening parenthesis '{'\
\
Acceleration weird. Chooses 1 color per round. Cannot quit.\
\
Basic\
\
IQ2\_XXS\
\
175\
\
3409\
\
1\
\
1\
\
1\
\
1\
\
1\
\
1\
\
1\
\
0\
\
0.5\
\
0\
\
7.5\
\
6.17\
\
screen = pygame.display.set\_mode((SCREEN\_WIDTH, SCREENHEIGHT)) NameError: name 'SCREENHEIGHT' is not defined. Did you mean: 'SCREEN\_HEIGHT'?\
\
OK. Colors change. Best score does not update. Quit only ESC not Q.\
\
### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit\#dynamic-quantization-trial-output)    **Dynamic Quantization trial output**\
\
IQ1\_S codeIQ1\_M codeIQ2\_XXS code\
\
[12KB\\
\\
inference\_UD-IQ1\_S\_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FqpBdpW55h5mNAzVoTxPI%2Finference_UD-IQ1_S_3407.txt?alt=media&token=37b19689-73e5-46d0-98be-352e515dfdf8)\
\
[11KB\\
\\
inference\_UD-IQ1\_S\_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FTdIrJSqc2VbNJy1bf3w5%2Finference_UD-IQ1_S_3408.txt?alt=media&token=e11f73bb-80be-49e5-91e2-f3a1f5495dcd)\
\
[10KB\\
\\
inference\_UD-IQ1\_S\_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FBk2ZwEIcLmvZQ3jlMLzw%2Finference_UD-IQ1_S_3409.txt?alt=media&token=052885f5-bee9-420d-a9c0-827412ac17c8)\
\
[10KB\\
\\
inference\_UD-IQ1\_M\_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Ft7YmT1H3Nflcy5kAp1LE%2Finference_UD-IQ1_M_3407.txt?alt=media&token=6f62f911-3364-4f92-b311-c1fa9b759370)\
\
[30KB\\
\\
inference\_UD-IQ1\_M\_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FH6BCTeWlJpUkfeEmeqpu%2Finference_UD-IQ1_M_3408.txt?alt=media&token=7727a999-8c0a-4baf-8542-be8686a01630)\
\
[9KB\\
\\
inference\_UD-IQ1\_M\_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FvVJI0H2F9KTNj5kwUCtC%2Finference_UD-IQ1_M_3409.txt?alt=media&token=0f863d41-53d6-4c94-8d57-bf1eeb79ead5)\
\
[29KB\\
\\
inference\_UD-IQ2\_XXS\_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2F26jxRY5mWuon67OfvGtq%2Finference_UD-IQ2_XXS_3407.txt?alt=media&token=daf9bf7d-245e-4b54-b0c0-a6273833835a)\
\
[34KB\\
\\
inference\_UD-IQ2\_XXS\_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FEhjjYN7vAh7gbmR8oXbS%2Finference_UD-IQ2_XXS_3408.txt?alt=media&token=4b50d6dd-2798-44c7-aa92-7e67c09868a4)\
\
[42KB\\
\\
inference\_UD-IQ2\_XXS\_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FXwCSfIf16nTwHzcWepoV%2Finference_UD-IQ2_XXS_3409.txt?alt=media&token=2f7539c9-026d-41e7-b7c7-5738a89ae5d4)\
\
### [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit\#non-dynamic-quantization-trial-output)    Non Dynamic Quantization trial output\
\
IQ1\_S basic codeIQ1\_M basic codeIQ2\_XXS basic code\
\
[25KB\\
\\
inference\_basic-IQ1\_S\_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FFtAMzAucSfKMkkmXItTj%2Finference_basic-IQ1_S_3407.txt?alt=media&token=76bfcf47-e1ce-442b-af49-6bfb6af7d046)\
\
[15KB\\
\\
inference\_basic-IQ1\_S\_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2F4NhjCVFMwCwT2OCj0IJ5%2Finference_basic-IQ1_S_3408.txt?alt=media&token=d4715674-3347-400b-9eb6-ae5d4470feeb)\
\
[14KB\\
\\
inference\_basic-IQ1\_S\_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fb0ZW3xs7R7IMryO7n7Yp%2Finference_basic-IQ1_S_3409.txt?alt=media&token=64b8825b-7103-4708-9d12-12770e43b546)\
\
[7KB\\
\\
inference\_basic-IQ1\_M\_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FmZ2TsQEzoGjhGlqUjtmj%2Finference_basic-IQ1_M_3407.txt?alt=media&token=975a30d6-2d90-47eb-9d68-b50fd47337f7)\
\
[7KB\\
\\
inference\_basic-IQ1\_M\_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FIx9TQ99Qpmk7BViNLFBl%2Finference_basic-IQ1_M_3408.txt?alt=media&token=b88e1e5b-4535-4d93-bd67-f81def7377d5)\
\
[12KB\\
\\
inference\_basic-IQ1\_M\_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FDX7XYpJPxXKAMZeGhSrr%2Finference_basic-IQ1_M_3409.txt?alt=media&token=6da9127e-272b-4e74-b990-6657e25eea6b)\
\
[25KB\\
\\
inference\_basic-IQ2\_XXS\_3407.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FajsVHsVqlWpwHk7mY32t%2Finference_basic-IQ2_XXS_3407.txt?alt=media&token=cbbf36a2-0d6a-4a87-8232-45b0b7fcc588)\
\
[34KB\\
\\
inference\_basic-IQ2\_XXS\_3408.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2F4vjncPu2r2D7F5jVOC7I%2Finference_basic-IQ2_XXS_3408.txt?alt=media&token=9ed635a2-bf97-4f49-b26f-6e985d0ab1b7)\
\
[34KB\\
\\
inference\_basic-IQ2\_XXS\_3409.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FJmVOFgrRyXjY4lYZXE96%2Finference_basic-IQ2_XXS_3409.txt?alt=media&token=faad5bff-ba7f-41f1-abd5-7896f17a5b25)\
\
[PreviousTutorial: How to Run DeepSeek-R1 Locally](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally) [NextTutorial: How to Run QwQ-32B effectively](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively)\
\
Last updated 1 month ago\
\
Was this helpful?\
\
* * *

## Run QwQ-32B Effectively
Qwen released QwQ-32B - a reasoning model with performance comparable to DeepSeek-R1 on many [benchmarks](https://qwenlm.github.io/blog/qwq-32b/). However, people have been experiencing **infinite generations**, **many repetitions**, <think> token issues and finetuning issues. We hope this guide will help debug and fix most issues!

Our model uploads with our bug fixes work great for fine-tuning, vLLM and Transformers. If you're using llama.cpp and engines that use llama.cpp as backend, follow our [instructions here](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#tutorial-how-to-run-qwq-32b) to fix endless generations.

**Unsloth QwQ-32B uploads with our bug fixes:**

[GGUF](https://huggingface.co/unsloth/QwQ-32B-GGUF)

[Dynamic 4-bit](https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit)

[BnB 4-bit](https://huggingface.co/unsloth/QwQ-32B-bnb-4bit)

[16-bit](https://huggingface.co/unsloth/QwQ-32B)

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively\#official-recommended-settings)    âš™ï¸ Official Recommended Settings

According to [Qwen](https://huggingface.co/Qwen/QwQ-32B), these are the recommended settings for inference:

- Temperature of 0.6

- Top\_K of 40 (or 20 to 40)

- Min\_P of 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1)

- Top\_P of 0.95

- Repetition Penalty of 1.0. (1.0 means disabled in llama.cpp and transformers)

- Chat template: `<|im_start|>user\nCreate a Flappy Bird game in Python.<|im_end|>\n<|im_start|>assistant\n<think>\n`


`llama.cpp` uses `min_p = 0.1` by default, which might cause issues. Force it to 0.0.

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively\#recommended-settings-for-llama.cpp)    ðŸ‘ Recommended settings for llama.cpp

We noticed many people use a `Repetition Penalty` greater than 1.0. For example 1.1 to 1.5. This actually interferes with llama.cpp's sampling mechanisms. The goal of a repetition penalty is to penalize repeated generations, but we found this doesn't work as expected.

Turning off `Repetition Penalty` also works (ie setting it to 1.0), but we found using it to be useful to penalize endless generations.

To use it, we found you must also edit the ordering of samplers in llama.cpp to before applying `Repetition Penalty`, otherwise there will be endless generations. So add this:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
--samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc"
```

By default, llama.cpp uses this ordering:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
--samplers "dry;top_k;typ_p;top_p;min_p;xtc;temperature"
```

We reorder essentially temperature and dry, and move min\_p forward. This means we apply samplers in this order:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
top_k=40
top_p=0.95
min_p=0.0
temperature=0.6
dry
typ_p
xtc
```

If you still encounter issues, you can increase the `--repeat-penalty 1.0 to 1.2 or 1.3.`

Courtesy to [@krist486](https://x.com/krist486/status/1897885598196654180) for bringing llama.cpp sampling directions to my attention.

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively\#dry-repetition-penalty)    â˜€ï¸ Dry Repetition Penalty

We investigated usage of `dry penalty` as suggested in [https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md](https://github.com/ggml-org/llama.cpp/blob/master/examples/main/README.md) using a value of 0.8, but we actually found this to **rather cause syntax issues especially for coding**. If you still encounter issues, you can increase the `dry penalty to 0.8.`

Utilizing our swapped sampling ordering can also help if you decide to use `dry penalty`.

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively\#tutorial-how-to-run-qwq-32b-in-ollama)    ðŸ¦™ Tutorial: How to Run QwQ-32B in Ollama

1. Install `ollama` if you haven't already!


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
apt-get update
apt-get install pciutils -y
curl -fsSL https://ollama.com/install.sh | sh
```

1. Run run the model! Note you can call `ollama serve` in another terminal if it fails! We include all our fixes and suggested parameters (temperature, min\_p etc) in `param` in our Hugging Face upload!


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
ollama run hf.co/unsloth/QwQ-32B-GGUF:Q4_K_M
```

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively\#tutorial-how-to-run-qwq-32b-in-llama.cpp)    ðŸ“– Tutorial: How to Run QwQ-32B in llama.cpp

1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
apt-get update
apt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y
git clone https://github.com/ggerganov/llama.cpp
cmake llama.cpp -B llama.cpp/build \
    -DBUILD_SHARED_LIBS=ON -DGGML_CUDA=ON -DLLAMA_CURL=ON
cmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split
cp llama.cpp/build/bin/llama-* llama.cpp
```

1. Download the model via (after installing `pip install huggingface_hub hf_transfer` ). You can choose Q4\_K\_M, or other quantized versions (like BF16 full precision). More versions at: [https://huggingface.co/unsloth/QwQ-32B-GGUF](https://huggingface.co/unsloth/QwQ-32B-GGUF)


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
# !pip install huggingface_hub hf_transfer
import os
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id = "unsloth/QwQ-32B-GGUF",
    local_dir = "unsloth-QwQ-32B-GGUF",
    allow_patterns = ["*Q4_K_M*"], # For Q4_K_M
)
```

1. Run Unsloth's Flappy Bird test, which will save the output to `Q4_K_M_yes_samplers.txt`

2. Edit `--threads 32` for the number of CPU threads, `--ctx-size 16384` for context length, `--n-gpu-layers 99` for GPU offloading on how many layers. Try adjusting it if your GPU goes out of memory. Also remove it if you have CPU only inference.

3. We use `--repeat-penalty 1.1` and `--dry-multiplier 0.5` which you can adjust.


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
./llama.cpp/llama-cli \
    --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \
    --threads 32 \
    --ctx-size 16384 \
    --n-gpu-layers 99 \
    --seed 3407 \
    --prio 2 \
    --temp 0.6 \
    --repeat-penalty 1.1 \
    --dry-multiplier 0.5 \
    --min-p 0.01 \
    --top-k 40 \
    --top-p 0.95 \
    -no-cnv \
    --samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc" \
    --prompt "<|im_start|>user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>\n<|im_start|>assistant\n<think>\n"  \
        2>&1 | tee Q4_K_M_yes_samplers.txt
```

The full input from our [https://unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic) 1.58bit blog is:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
<|im_start|>user
Create a Flappy Bird game in Python. You must include these things:
1. You must use pygame.
2. The background color should be randomly chosen and is a light shade. Start with a light blue color.
3. Pressing SPACE multiple times will accelerate the bird.
4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.
5. Place on the bottom some land colored as dark brown or yellow chosen randomly.
6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.
7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.
8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.
The final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>
<|im_start|>assistant
<think>
```

The beginning and the end of the final Python output after removing the thinking parts:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
import pygame
import random
import sys

pygame.init()
### Continues

class Bird:
    def __init__(self):
        ### Continues

def main():
    best_score = 0
    current_score = 0
    game_over = False
    pipes = []
    first_time = True  # Track first game play

    # Initial setup
    background_color = (173, 216, 230)  # Light blue initially
    land_color = random.choice(land_colors)
    bird = Bird()

    while True:
        for event in pygame.event.get():
            ### Continues

        if not game_over:
            # Update bird and pipes
            bird.update()
            ### Continues

        # Drawing
        ### Continues
        pygame.display.flip()
        clock.tick(60)

if __name__ == "__main__":
    main()
```

Full final Python output (removed thinking parts): [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#full-final-python-output-removed-thinking-parts)

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
import pygame
import random
import sys

pygame.init()
WIDTH, HEIGHT = 800, 600
screen = pygame.display.set_mode((WIDTH, HEIGHT))
pygame.display.set_caption("Flappy Bird Clone")
clock = pygame.time.Clock()

# Colors
pipe_colors = [(0, 100, 0), (210, 180, 140), (50, 50, 50)]
land_colors = [(139, 69, 19), (255, 255, 0)]

# Game constants
GRAVITY = 0.5
PIPE_SPEED = 5
BIRD_SIZE = 30
LAND_HEIGHT = 50
PIPE_WIDTH = 50
PIPE_GAP = 150

class Bird:
    def __init__(self):
        self.x = WIDTH // 2
        self.y = HEIGHT // 2
        self.velocity = 0
        self.shape = random.choice(['square', 'circle', 'triangle'])
        self.color = (random.randint(0, 100), random.randint(0, 100), random.randint(0, 100))
        self.rect = pygame.Rect(self.x - BIRD_SIZE//2, self.y - BIRD_SIZE//2, BIRD_SIZE, BIRD_SIZE)

    def update(self):
        self.velocity += GRAVITY
        self.y += self.velocity
        self.rect.y = self.y - BIRD_SIZE//2
        self.rect.x = self.x - BIRD_SIZE//2  # Keep x centered

    def draw(self):
        if self.shape == 'square':
            pygame.draw.rect(screen, self.color, self.rect)
        elif self.shape == 'circle':
            pygame.draw.circle(screen, self.color, (self.rect.centerx, self.rect.centery), BIRD_SIZE//2)
        elif self.shape == 'triangle':
            points = [\
                (self.rect.centerx, self.rect.top),\
                (self.rect.left, self.rect.bottom),\
                (self.rect.right, self.rect.bottom)\
            ]
            pygame.draw.polygon(screen, self.color, points)

def spawn_pipe():
    pipe_x = WIDTH
    top_height = random.randint(50, HEIGHT - PIPE_GAP - LAND_HEIGHT)
    rect_top = pygame.Rect(pipe_x, 0, PIPE_WIDTH, top_height)
    bottom_y = top_height + PIPE_GAP
    bottom_height = (HEIGHT - LAND_HEIGHT) - bottom_y
    rect_bottom = pygame.Rect(pipe_x, bottom_y, PIPE_WIDTH, bottom_height)
    color = random.choice(pipe_colors)
    return {
        'rect_top': rect_top,
        'rect_bottom': rect_bottom,
        'color': color,
        'scored': False
    }

def main():
    best_score = 0
    current_score = 0
    game_over = False
    pipes = []
    first_time = True  # Track first game play

    # Initial setup
    background_color = (173, 216, 230)  # Light blue initially
    land_color = random.choice(land_colors)
    bird = Bird()

    while True:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                sys.exit()
            if event.type == pygame.KEYDOWN:
                if event.key == pygame.K_ESCAPE or event.key == pygame.K_q:
                    pygame.quit()
                    sys.exit()
                if event.key == pygame.K_SPACE:
                    if game_over:
                        # Reset the game
                        bird = Bird()
                        pipes.clear()
                        current_score = 0
                        if first_time:
                            # First restart after initial game over
                            background_color = (random.randint(200, 255), random.randint(200, 255), random.randint(200, 255))
                            first_time = False
                        else:
                            background_color = (random.randint(200, 255), random.randint(200, 255), random.randint(200, 255))
                        land_color = random.choice(land_colors)
                        game_over = False
                    else:
                        # Jump the bird
                        bird.velocity = -15  # Initial upward velocity

        if not game_over:
            # Update bird and pipes
            bird.update()

            # Move pipes left
            remove_pipes = []
            for pipe in pipes:
                pipe['rect_top'].x -= PIPE_SPEED
                pipe['rect_bottom'].x -= PIPE_SPEED
                # Check if bird passed the pipe
                if not pipe['scored'] and bird.rect.x > pipe['rect_top'].right:
                    current_score += 1
                    pipe['scored'] = True
                # Check if pipe is offscreen
                if pipe['rect_top'].right < 0:
                    remove_pipes.append(pipe)
            # Remove offscreen pipes
            for p in remove_pipes:
                pipes.remove(p)

            # Spawn new pipe if needed
            if not pipes or pipes[-1]['rect_top'].x < WIDTH - 200:
                pipes.append(spawn_pipe())

            # Check collisions
            land_rect = pygame.Rect(0, HEIGHT - LAND_HEIGHT, WIDTH, LAND_HEIGHT)
            bird_rect = bird.rect
            # Check pipes
            for pipe in pipes:
                if bird_rect.colliderect(pipe['rect_top']) or bird_rect.colliderect(pipe['rect_bottom']):
                    game_over = True
                    break
            # Check land and top
            if bird_rect.bottom >= land_rect.top or bird_rect.top <= 0:
                game_over = True

            if game_over:
                if current_score > best_score:
                    best_score = current_score

        # Drawing
        screen.fill(background_color)
        # Draw pipes
        for pipe in pipes:
            pygame.draw.rect(screen, pipe['color'], pipe['rect_top'])
            pygame.draw.rect(screen, pipe['color'], pipe['rect_bottom'])
        # Draw land
        pygame.draw.rect(screen, land_color, (0, HEIGHT - LAND_HEIGHT, WIDTH, LAND_HEIGHT))
        # Draw bird
        bird.draw()
        # Draw score
        font = pygame.font.SysFont(None, 36)
        score_text = font.render(f'Score: {current_score}', True, (0, 0, 0))
        screen.blit(score_text, (WIDTH - 150, 10))
        # Game over screen
        if game_over:
            over_text = font.render('Game Over!', True, (255, 0, 0))
            best_text = font.render(f'Best: {best_score}', True, (255, 0, 0))
            restart_text = font.render('Press SPACE to restart', True, (255, 0, 0))
            screen.blit(over_text, (WIDTH//2 - 70, HEIGHT//2 - 30))
            screen.blit(best_text, (WIDTH//2 - 50, HEIGHT//2 + 10))
            screen.blit(restart_text, (WIDTH//2 - 100, HEIGHT//2 + 50))

        pygame.display.flip()
        clock.tick(60)

if __name__ == "__main__":
    main()
```

1. When running it, we get a runnable game!


![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F7qQoA6yrMWUVrwIhLbGu%252Fimage.png%3Falt%3Dmedia%26token%3D6d99c8ce-567a-4144-bd7e-fa57e96b5284&width=768&dpr=4&quality=100&sign=911446a1&sv=2)

1. Now try the same without our fixes! So remove `--samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc"` This will save the output to `Q4_K_M_no_samplers.txt`


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
./llama.cpp/llama-cli \
    --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \
    --threads 32 \
    --ctx-size 16384 \
    --n-gpu-layers 99 \
    --seed 3407 \
    --prio 2 \
    --temp 0.6 \
    --repeat-penalty 1.1 \
    --dry-multiplier 0.5 \
    --min-p 0.01 \
    --top-k 40 \
    --top-p 0.95 \
    -no-cnv \
    --prompt "<|im_start|>user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>\n<|im_start|>assistant\n<think>\n"  \
        2>&1 | tee Q4_K_M_no_samplers.txt
```

You will get some looping, but **problematically incorrect Python syntax** and many other issues. For example the below looks correct, but is wrong! Ie line 39 `pipes.clear() ### <<< NameError: name 'pipes' is not defined. Did you forget to import 'pipes'?`

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap
import pygame
import random

pygame.init()

# Constants
WIDTH, HEIGHT = 800, 600
GROUND_HEIGHT = 20
GRAVITY = 0.7
PIPE_SPEED = -3
BIRD_SIZE = 45
MIN_GAP = 130
MAX_GAP = 200
PIPE_COLORS = [(0, 96, 0), (205, 133, 63), (89, 97, 107)]
DARK_BROWN = (94, 72, 4)
YELLOW = (252, 228, 6)

screen = pygame.display.set_mode((WIDTH, HEIGHT))
clock = pygame.time.Clock()

def random_light_color():
    return (
        random.randint(180, 230),
        random.randint(190, 300),
        random.randint(250, 255)
    )

def reset_game():
    global bird_x, bird_y
    global pipes, score
    global background_color, land_color
    global bird_shape, bird_color

    # Bird properties
    bird_x = WIDTH * 0.3
    bird_y = HEIGHT // 2
    bird_vel = -5  # Initial upward thrust

    pipes.clear() ### <<< NameError: name 'pipes' is not defined. Did you forget to import 'pipes'?
```

1. If you use `--repeat-penalty 1.5`, it gets even worse and more obvious, with actually totally incorrect syntax.


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
import pygame
from random import randint  # For generating colors/shapes/positions randomly
pygame.init()

# Constants:
WIDTH, HEIGHT =456 ,702   #
BACKGROUND_COLOR_LIGHTS=['lightskyblue']
GAP_SIZE=189           #

BIRD_RADIUS=3.
PIPE_SPEED=- ( )    ?
class Game():
def __init__(self):
        self.screen_size=( )

def reset_game_vars():
    global current_scor e
   # set to zero and other initial states.

# Main game loop:
while running :
     for event in pygame.event.get() :
        if quit ... etc

pygame.quit()
print("Code is simplified. Due time constraints, full working version requires further implementation.")
```

1. You might be wondering maybe it's Q4\_K\_M? B16 ie full precision should work fine right? Incorrect - the outputs again fail if we do not use our fix of - `-samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc"` when using a Repetition Penalty.


## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively\#still-doesnt-work-try-min_p-0.1-temperature-1.5)    ðŸŒ„ Still doesn't work? Try Min\_p = 0.1, Temperature = 1.5

According to the Min\_p paper [https://arxiv.org/pdf/2407.01082](https://arxiv.org/pdf/2407.01082), for more creative and diverse outputs, and if you still see repetitions, try disabling top\_p and top\_k!

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
./llama.cpp/llama-cli --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \
    --threads 32 --n-gpu-layers 99 \
    --ctx-size 16384 \
    --temp 1.5 \
    --min-p 0.1 \
    --top-k 0 \
    --top-p 1.0 \
    -no-cnv \
    --prompt "<|im_start|>user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>\n<|im_start|>assistant\n<think>\n"
```

Another approach is to disable `min_p` directly, since llama.cpp by default uses `min_p = 0.1`!

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
./llama.cpp/llama-cli --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \
    --threads 32 --n-gpu-layers 99 \
    --ctx-size 16384 \
    --temp 0.6 \
    --min-p 0.0 \
    --top-k 40 \
    --top-p 0.95 \
    -no-cnv \
    --prompt "<|im_start|>user\nCreate a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>\n<|im_start|>assistant\n<think>\n"
```

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively\#less-than-think-greater-than-token-not-shown)    ðŸ¤” <think> token not shown?

Some people are reporting that because <think> is default added in the chat template, some systems are not outputting the thinking traces correctly. You will have to manually edit the Jinja template from:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap
{%- if tools %} {{- '<|im_start|>system\n' }} {%- if messages[0]['role'] == 'system' %} {{- messages[0]['content'] }} {%- else %} {{- '' }} {%- endif %} {{- "\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }} {%- for tool in tools %} {{- "\n" }} {{- tool | tojson }} {%- endfor %} {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }} {%- else %} {%- if messages[0]['role'] == 'system' %} {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }} {%- endif %} {%- endif %} {%- for message in messages %} {%- if (message.role == "user") or (message.role == "system" and not loop.first) %} {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }} {%- elif message.role == "assistant" and not message.tool_calls %} {%- set content = message.content.split('</think>')[-1].lstrip('\n') %} {{- '<|im_start|>' + message.role + '\n' + content + '<|im_end|>' + '\n' }} {%- elif message.role == "assistant" %} {%- set content = message.content.split('</think>')[-1].lstrip('\n') %} {{- '<|im_start|>' + message.role }} {%- if message.content %} {{- '\n' + content }} {%- endif %} {%- for tool_call in message.tool_calls %} {%- if tool_call.function is defined %} {%- set tool_call = tool_call.function %} {%- endif %} {{- '\n<tool_call>\n{"name": "' }} {{- tool_call.name }} {{- '", "arguments": ' }} {{- tool_call.arguments | tojson }} {{- '}\n</tool_call>' }} {%- endfor %} {{- '<|im_end|>\n' }} {%- elif message.role == "tool" %} {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %} {{- '<|im_start|>user' }} {%- endif %} {{- '\n<tool_response>\n' }} {{- message.content }} {{- '\n</tool_response>' }} {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %} {{- '<|im_end|>\n' }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- '<|im_start|>assistant\n<think>\n' }} {%- endif %}
```

to another by removing the `<think>\n` at the end. The model will now have to manually add `<think>\n` during inference, which might not always succeed. DeepSeek also edited all models to default add a `<think>` token to force the model to go into reasoning model.

So change `{%- if add_generation_prompt %} {{- '<|im_start|>assistant\n<think>\n' }} {%- endif %} ` to `{%- if add_generation_prompt %} {{- '<|im_start|>assistant\n' }} {%- endif %}` ie remove `<think>\n`

Full jinja template with removed <think>\\n part [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively#full-jinja-template-with-removed-less-than-think-greater-than-n-part)

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap
{%- if tools %} {{- '<|im_start|>system\n' }} {%- if messages[0]['role'] == 'system' %} {{- messages[0]['content'] }} {%- else %} {{- '' }} {%- endif %} {{- "\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }} {%- for tool in tools %} {{- "\n" }} {{- tool | tojson }} {%- endfor %} {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }} {%- else %} {%- if messages[0]['role'] == 'system' %} {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }} {%- endif %} {%- endif %} {%- for message in messages %} {%- if (message.role == "user") or (message.role == "system" and not loop.first) %} {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }} {%- elif message.role == "assistant" and not message.tool_calls %} {%- set content = message.content.split('</think>')[-1].lstrip('\n') %} {{- '<|im_start|>' + message.role + '\n' + content + '<|im_end|>' + '\n' }} {%- elif message.role == "assistant" %} {%- set content = message.content.split('</think>')[-1].lstrip('\n') %} {{- '<|im_start|>' + message.role }} {%- if message.content %} {{- '\n' + content }} {%- endif %} {%- for tool_call in message.tool_calls %} {%- if tool_call.function is defined %} {%- set tool_call = tool_call.function %} {%- endif %} {{- '\n<tool_call>\n{"name": "' }} {{- tool_call.name }} {{- '", "arguments": ' }} {{- tool_call.arguments | tojson }} {{- '}\n</tool_call>' }} {%- endfor %} {{- '<|im_end|>\n' }} {%- elif message.role == "tool" %} {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %} {{- '<|im_start|>user' }} {%- endif %} {{- '\n<tool_response>\n' }} {{- message.content }} {{- '\n</tool_response>' }} {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %} {{- '<|im_end|>\n' }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- '<|im_start|>assistant\n' }} {%- endif %}
```

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively\#extra-notes)    Extra Notes

We first thought maybe:

1. QwQ's context length was not natively 128K, but rather 32K with YaRN extension. For example in the readme file for [https://huggingface.co/Qwen/QwQ-32B](https://huggingface.co/Qwen/QwQ-32B), we see:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
{
  ...,
  "rope_scaling": {
    "factor": 4.0,
    "original_max_position_embeddings": 32768,
    "type": "yarn"
  }
}
```

We tried overriding llama.cpp's YaRN handling, but nothing changed.

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap
--override-kv qwen2.context_length=int:131072 \
--override-kv qwen2.rope.scaling.type=str:yarn \
--override-kv qwen2.rope.scaling.factor=float:4 \
--override-kv qwen2.rope.scaling.original_context_length=int:32768 \
--override-kv qqwen2.rope.scaling.attn_factor=float:1.13862943649292 \
```

1. We also thought maybe the RMS Layernorm epsilon was wrong - not 1e-5 but maybe 1e-6. For example [this](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct/blob/main/config.json) has `rms_norm_eps=1e-06`, whilst [this](https://huggingface.co/Qwen/Qwen2.5-32B/blob/main/config.json) has `rms_norm_eps=1e-05` . We also overrided it, but it did not work:


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line] whitespace-pre-wrap
--override-kv qwen2.attention.layer_norm_rms_epsilon=float:0.000001 \
```

1. We also tested if tokenizer IDs matched between llama.cpp and normal Transformers courtesy of [@kalomaze](https://x.com/kalomaze/status/1897875332230779138). They matched, so this was not the culprit.


We provide our experimental results below:

[61KB\\
\\
file\_BF16\_no\_samplers.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FeABgnEXerhmNw1jzUmrr%2Ffile_BF16_no_samplers.txt?alt=media&token=d11aa8f8-0ff7-4370-9412-6129bd980a42)

BF16 full precision with no sampling fix

[55KB\\
\\
file\_BF16\_yes\_samplers.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fv01qqEwj6PHVE9VvPzfg%2Ffile_BF16_yes_samplers.txt?alt=media&token=d8ecf5bf-b4f2-4abe-a0b4-26d7e8e862f9)

BF16 full precision with sampling fix

[71KB\\
\\
final\_Q4\_K\_M\_no\_samplers.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fi3eSz0NWvc44CkRUanrY%2Ffinal_Q4_K_M_no_samplers.txt?alt=media&token=deca70bd-fc21-44a9-b42c-87837ac3a8ce)

Q4\_K\_M precision with no sampling fix

[65KB\\
\\
final\_Q4\_K\_M\_yes\_samplers.txt](https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FBtdJmKQjMZVlpO1HfWE7%2Ffinal_Q4_K_M_yes_samplers.txt?alt=media&token=f266d668-71ab-436d-8c05-b720e56e348e)

Q4\_K\_M precision with sampling fix

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively\#tokenizer-bug-fixes)    âœï¸ Tokenizer Bug Fixes

- We found a few issues as well specifically impacting finetuning! The EOS token is correct, but the PAD token should probably rather be `"<|vision_pad|>`" We updated it in: [https://huggingface.co/unsloth/QwQ-32B/blob/main/tokenizer\_config.json](https://huggingface.co/unsloth/QwQ-32B/blob/main/tokenizer_config.json)


Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
"eos_token": "<|im_end|>",
"pad_token": "<|endoftext|>",
```

## [Direct link to heading](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively\#dynamic-4-bit-quants)    ðŸ› ï¸ Dynamic 4-bit Quants

We also uploaded dynamic 4bit quants which increase accuracy vs naive 4bit quantizations! We attach the QwQ quantization error plot analysis for both activation and weight quantization errors:

![](https://docs.unsloth.ai/~gitbook/image?url=https%3A%2F%2F3215535692-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FxhOjnexMCB3dmuQFQ2Zq%252Fuploads%252F32wjrIWeUEQTMq9PhmbS%252FQwQ%2520quantization%2520errors.png%3Falt%3Dmedia%26token%3D0733fd33-9fe9-4aad-812c-75dbad00373f&width=768&dpr=4&quality=100&sign=aafe447c&sv=2)

We uploaded dynamic 4-bit quants to: [https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit](https://huggingface.co/unsloth/QwQ-32B-unsloth-bnb-4bit)

Since vLLM 0.7.3 (2025 February 20th) [https://github.com/vllm-project/vllm/releases/tag/v0.7.3](https://github.com/vllm-project/vllm/releases/tag/v0.7.3), vLLM now supports loading Unsloth dynamic 4bit quants!

All our GGUFs are at [https://huggingface.co/unsloth/QwQ-32B-GGUF](https://huggingface.co/unsloth/QwQ-32B-GGUF)!

[PreviousDeepSeek-R1 Dynamic 1.58-bit](https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-locally/deepseek-r1-dynamic-1.58-bit) [NextTutorial: How to Run Gemma 3 effectively](https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively)

Last updated 4 days ago

Was this helpful?

* * *

## Unsloth Errors Guide
## [Direct link to heading](https://docs.unsloth.ai/basics/errors-troubleshooting\#running-in-unsloth-works-well-but-after-exporting-and-running-on-other-platforms-the-results-are-poo)    Running in Unsloth works well, but after exporting & running on other platforms, the results are poor

You might sometimes encounter an issue where your model runs and produces good results on Unsloth, but when you use it on another platform like Ollama or vLLM, the results are poor or you might get gibberish, endless/infinite generations _or_ repeated outputs **.**

- The most common cause of this error is using an incorrect chat template. Itâ€™s essential to use the SAME chat template that was used when training the model in Unsloth and later when you run it in another framework, such as llama.cpp or Ollama. When inferencing from a saved model, it's crucial to apply the correct template.

- It might also be because your inference engine adds an unnecessary "start of sequence" token (or the lack of thereof on the contrary) so ensure you check both hypotheses!


## [Direct link to heading](https://docs.unsloth.ai/basics/errors-troubleshooting\#saving-to-gguf-vllm-16bit-crashes)    Saving to GGUF / vLLM 16bit crashes

You can try reducing the maximum GPU usage during saving by changing `maximum_memory_usage`.

The default is `model.save_pretrained(..., maximum_memory_usage = 0.75)`. Reduce it to say 0.5 to use 50% of GPU peak memory or lower. This can reduce OOM crashes during saving.

## [Direct link to heading](https://docs.unsloth.ai/basics/errors-troubleshooting\#evaluation-loop-also-oom-or-crashing)    Evaluation Loop - also OOM or crashing.

A common issue when you OOM is because you set your batch size too high. Set it lower than 3 to use less VRAM.

First split your training dataset into a train and test split. Set the trainer settings for evaluation to:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
new_dataset = dataset.train_test_split(test_size = 0.01)
SFTTrainer(
    args = TrainingArguments(
        fp16_full_eval = True,
        per_device_eval_batch_size = 2,
        eval_accumulation_steps = 4,
        eval_strategy = "steps",
        eval_steps = 1,
    ),
    train_dataset = new_dataset["train"],
    eval_dataset = new_dataset["test"],
```

This will cause no OOMs and make it somewhat faster with no upcasting to float32. Validation set.

## [Direct link to heading](https://docs.unsloth.ai/basics/errors-troubleshooting\#notimplementederror-a-utf-8-locale-is-required.-got-ansi)    NotImplementedError: A UTF-8 locale is required. Got ANSI

See https://github.com/googlecolab/colabtools/issues/3409

In a new cell, run the below:

Copy

```inline-grid min-w-full grid-cols-[auto_1fr] p-2 [count-reset:line]
import locale
locale.getpreferredencoding = lambda: "UTF-8"
```

[PreviousFinetuning from Last Checkpoint](https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint) [NextUnsloth Benchmarks](https://docs.unsloth.ai/basics/unsloth-benchmarks)

Last updated 14 days ago

Was this helpful?

* * *

## QWQ-32B Running Guide
* * *

## Run QWQ-32B Tutorial
* * *


