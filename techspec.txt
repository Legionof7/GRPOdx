Specification for Doctor–Patient Self-Play with GRPO via Unsloth
1. Overview
We want to train a “Doctor” model using LoRA with GRPO. The Doctor learns to ask better questions and make accurate final diagnoses by engaging in simulated conversations with a “Patient” (GPT-4o-mini). The Patient picks a hidden disease (preferably a common one, chosen freely by GPT-4o-mini), and the Doctor attempts to diagnose it within a fixed number of turns.

Key points:

Hidden chain-of-thought: Each Doctor reply must include <reason>...</reason> blocks with reasoning that is not shown to the Patient.
Reward model: The same GPT-4o-mini instance (or a separate instance) reads the full conversation (including <reason> blocks) at the end, generates a score 
[
0..1
]
[0..1], and we use that score for GRPO updates.
Partial credit: The Judge model should give partial scores for diagnoses that are somewhat correct or partially correct.
Multiple completions per scenario: We generate multiple Doctor attempts for each scenario to compute an advantage relative to their average reward, then update the LoRA weights accordingly.
2. High-Level Data Flow
Doctor Turn

The Doctor sees the conversation so far (no <reason> from previous Doctor turns, only visible text).
The Doctor produces output containing <reason> ... </reason> (hidden reasoning) followed by the visible text for the Patient.
We log two versions of the Doctor's output:
Full text (with <reason>), stored in a “conversation_with_reason” log.
Visible text (with <reason> removed), stored in a “conversation_no_reason” log (shown to the Patient).
Patient Turn

GPT-4o-mini (Patient) has chosen a hidden disease (preferably a common one).
The Patient sees the visible text only.
The Patient replies accordingly, without revealing the disease unless confirming a final diagnosis.
End of Conversation

The conversation ends if the Doctor outputs Final diagnosis: ... or we reach MAX_TURNS.
We then do a separate GPT-4o-mini call to the Patient with a special prompt that says: “Reveal the disease you were simulating.”
The revealed disease is provided (e.g., “Influenza”).
Scoring

We feed the entire “conversation_with_reason” (including the Doctor’s <reason> blocks) plus the revealed disease to GPT-4o-mini in a custom reward prompt.
We parse out the first float in [0..1] from GPT-4o-mini’s response (see next section).
This score can reflect partial correctness (partial credit) if the Doctor’s final diagnosis is close or partially matches the hidden disease.
GRPO Updates

For each scenario, we generate multiple completions to compute an average reward.
For each completion’s reward, advantage = (score - average).
We then call train_on_records(...) with the (text_data, advantage) pairs to update the Doctor’s LoRA weights.
3. Implementation Details
3.1 Loading the Doctor Model & Applying LoRA
python
Copy
# doctor.py
from unsloth import FastLanguageModel

def load_doctor_model():
    model, tokenizer = FastLanguageModel.from_pretrained(
        "unsloth/Phi-4",
        load_in_4bit=True,
        max_seq_length=2048
    )
    return model, tokenizer

def prepare_lora(model):
    return FastLanguageModel.get_peft_model(
        model,
        r=8,
        target_modules=["q_proj","k_proj","v_proj","o_proj",
                        "gate_proj","up_proj","down_proj"],
        lora_alpha=16,
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing="unsloth"
    )
We assume the base model is 4-bit quantized.
Adjust LoRA hyperparameters as needed.
3.2 Enforcing <reason>...</reason> in the Doctor’s Output
Use a system prompt that instructs the Doctor to always produce <reason>...</reason>:

sql
Copy
System:
You are an AI Doctor. Each time you speak, you MUST include a hidden 
chain-of-thought in the format <reason> ... </reason>.

After writing your hidden reasoning, provide a short statement to the patient.
Never reveal the text within <reason> to the patient.

If you reach the last turn without providing "Final diagnosis: <Disease>", 
include that line in your final output (outside <reason>).
We incorporate this logic in a function that builds the Doctor’s prompt each turn.

3.3 Generating Doctor Turns & Hiding <reason> Content
python
Copy
import re

def remove_reason_tags(text):
    # Remove all <reason>...</reason> content
    return re.sub(r"<reason>.*?</reason>", "", text, flags=re.DOTALL)

def generate_doctor_turn(doctor_model, prompt):
    """
    Returns:
      full_text: str   (with <reason> blocks)
      visible_text: str (with <reason> blocks removed)
    """
    full_text = doctor_model.fast_generate([prompt], max_new_tokens=256)[0]
    visible_text = remove_reason_tags(full_text)
    return full_text, visible_text
3.4 Patient LLM (GPT-4o-mini)
The Patient is prompted with a system instruction to pick a common disease and roleplay:

python
Copy
# patient.py
def start_patient_system_prompt():
    return """System:
Pick a common disease (any from your knowledge base).
Roleplay as if you have that disease.
Do not reveal it unless the Doctor provides a final diagnosis or explicitly asks.
"""

def get_patient_reply(conversation, patient_system_prompt):
    """
    conversation: list of dicts with 'role' and 'content', 
                  containing only visible text (no <reason>).
    """
    messages = [{"role": "system", "content": patient_system_prompt}]
    for turn in conversation:
        messages.append({"role": turn["role"], "content": turn["content"]})
    # Invoke GPT-4o-mini (or other model) with 'messages'
    # Extract and return the Patient's text
    return "<patient response>"

def reveal_hidden_disease(conversation, patient_system_prompt):
    """
    Instruct the same GPT-4o-mini to reveal which disease it was simulating.
    For partial credit and correctness, the Doctor's final guess 
    will be compared to this revealed disease.
    """
    # e.g., system prompt: "You were roleplaying as the patient. Now reveal the disease you used."
    # Return the disease as a string
    return "<revealed disease>"
3.5 Reward Model with Partial Credit
We use GPT-4o-mini again (or a separate instance) to judge the entire conversation (including <reason> blocks) and the final diagnosis. It should:

Evaluate how thorough/logical the <reason> content is.
Compare the Doctor’s final diagnosis with the hidden disease.
Provide a float in [0..1]—the first float we find in its response is taken as the score.
Provide partial credit if the diagnosis is close or partially correct.
Reward Prompt Example:

python
Copy
# reward.py
def format_conversation(conversation_with_reason):
    text = []
    for turn in conversation_with_reason:
        role = turn['role'].title()
        content = turn['content']
        text.append(f"{role}: {content}")
    return "\n".join(text)

def score_conversation(conversation_with_reason, hidden_disease):
    """
    Return a float score in [0..1]. 
    We parse out the first float in GPT-4o-mini's response.
    """
    conv_text = format_conversation(conversation_with_reason)
    prompt_for_judge = f"""System:
You are a medical conversation evaluator. You see the full dialog, 
including the Doctor's <reason> blocks. The hidden disease was: {hidden_disease}.

Please score the Doctor's performance on these criteria, returning a single float 
in [0..1] (the first float in your response):
1) Thoroughness/correctness of the hidden reasoning (<reason>...</reason>).
2) Accuracy of the final diagnosis vs. {hidden_disease} (partial credit if close).
3) Relevance and specificity of the Doctor's questions.

Conversation:
{conv_text}
"""

    # Call GPT-4o-mini (or reward model) with `prompt_for_judge`
    # Suppose the raw output is something like: "I would give this a 0.75 overall."
    raw_judge_output = "<some GPT response with a float>"
    
    # Parse out the first float
    import re
    match = re.search(r"\b\d*\.?\d+\b", raw_judge_output)
    if match:
        score_str = match.group(0)
        score = float(score_str)
        # Ensure bounding to [0..1] just in case
        return max(0.0, min(1.0, score))
    else:
        # Fallback if we can't parse a float
        return 0.0
Note: You can further refine the partial-credit evaluation by adjusting the scoring instructions or by letting GPT-4o-mini produce detailed numeric break-down. But for simplicity, we parse the first float and clamp it to [0..1].

4. Main Training Loop (GRPO)
A minimal example:

python
Copy
# main.py
from trl import GRPOTrainer, GRPOConfig
from doctor import load_doctor_model, prepare_lora
from patient import start_patient_system_prompt, get_patient_reply, reveal_hidden_disease
from reward import score_conversation
import re

MAX_TURNS = 5

def build_doctor_prompt(conv_no_reason, turn_idx, max_turns):
    system_msg = f"""System:
You are an AI Doctor. You can ask at most {max_turns} questions. 
Each time you speak, include a hidden <reason>...</reason> block. 
Never reveal the text inside <reason> to the patient.

By the final turn (turn {max_turns}), if you haven't provided 
"Final diagnosis: <Disease>", do so on that turn. Then stop.

Conversation so far:
"""
    for turn in conv_no_reason:
        system_msg += f"{turn['role'].title()}: {turn['content']}\n"
    system_msg += "Doctor:"  # Expect the model to continue from here
    return system_msg

def run_episode(doctor_model):
    conv_no_reason = []
    conv_with_reason = []
    patient_prompt = start_patient_system_prompt()

    for turn_idx in range(1, MAX_TURNS + 1):
        # Build system prompt for Doctor
        doc_prompt = build_doctor_prompt(conv_no_reason, turn_idx, MAX_TURNS)
        full_doc_text, doc_visible = generate_doctor_turn(doctor_model, doc_prompt)
        
        # Store both versions
        conv_with_reason.append({"role": "doctor", "content": full_doc_text})
        conv_no_reason.append({"role": "doctor", "content": doc_visible})

        # Check if the Doctor gave a final diagnosis
        if "Final diagnosis:" in doc_visible:
            break

        # Patient turn
        pat_reply = get_patient_reply(conv_no_reason, patient_prompt)
        conv_no_reason.append({"role": "patient", "content": pat_reply})
        conv_with_reason.append({"role": "patient", "content": pat_reply})

    # At this point, either we reached the turn limit or got a final diagnosis
    hidden_dx = reveal_hidden_disease(conv_no_reason, patient_prompt)

    # Score conversation with reason
    score = score_conversation(conv_with_reason, hidden_dx)
    return conv_no_reason, conv_with_reason, score

def flatten_for_grpo(conv_no_reason):
    """
    Flatten the visible conversation for training data. 
    Typically you'd pass in role tokens or a chat-style format. 
    Here, we keep it simple.
    """
    lines = []
    for c in conv_no_reason:
        role = c['role'].upper()
        text = c['content']
        lines.append(f"{role}: {text}")
    return "\n\n".join(lines)

def main():
    # Load Doctor model & apply LoRA
    doc_model, doc_tokenizer = load_doctor_model()
    doc_model = prepare_lora(doc_model)

    # Configure GRPO
    grpo_cfg = GRPOConfig(
        # e.g., steps=1000, learning_rate, batch_size, etc.
    )
    trainer = GRPOTrainer(doc_model, grpo_cfg, doc_tokenizer)

    def compute_advantages(scores):
        avg_score = sum(scores) / len(scores)
        return [s - avg_score for s in scores]

    NUM_STEPS = 1000
    BATCH_SIZE = 2
    COMPLETIONS_PER_SCENARIO = 2

    for step in range(NUM_STEPS):
        data_batch = []
        
        # Generate multiple scenarios in each batch
        for _ in range(BATCH_SIZE):
            # For each scenario, sample multiple completions
            completions = []
            for _ in range(COMPLETIONS_PER_SCENARIO):
                conv_no_reason, conv_with_reason, sc = run_episode(doc_model)
                completions.append((conv_no_reason, sc))
            
            # Compute advantage-based rewards
            scores = [c[1] for c in completions]
            advs = compute_advantages(scores)

            # Build training records
            for (convNR, scr), adv in zip(completions, advs):
                text_data = flatten_for_grpo(convNR)
                data_batch.append((text_data, adv))

        # Update model with GRPO
        trainer.train_on_records(data_batch)

        if step % 50 == 0:
            print(f"Completed training step {step}")

    # Save the final LoRA checkpoint
    doc_model.save_pretrained("doctor_lora_checkpoint")

if __name__ == "__main__":
    main()