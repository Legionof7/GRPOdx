# -*- coding: utf-8 -*-
"""Doctor–Patient GRPO (Multi-Turn) with a base reward for any final diagnosis + 5 completions per scenario.
   Modified to simulate patient responses via OpenAI API.
"""

########################################
# 0. Imports & Setup
########################################

from unsloth import FastLanguageModel, PatchFastRL
PatchFastRL("GRPO", FastLanguageModel)

from unsloth import is_bfloat16_supported
from datasets import Dataset
import torch
import pandas as pd

import random
import re
import os
from contextlib import nullcontext
import datetime

from transformers import GenerationConfig
from trl import GRPOConfig
from accelerate.utils import broadcast_object_list, gather, gather_object, set_seed
from trl import maybe_apply_chat_template
from trl.trainer.grpo_trainer import pad

# Import the same patched Unsloth trainer from your Tic-Tac-Toe example
from unsloth_compiled_cache.UnslothGRPOTrainer import UnslothGRPOTrainer

# NEW: Import OpenAI
import openai

print("Imports complete.")

########################################
# 1. Load Base Model & LoRA
########################################

save_path = "/content/drive/MyDrive/UnslothGRPO/doctorExample"

max_seq_length = 2048
lora_rank = 32

model_name = "Qwen/Qwen2.5-1.5B-Instruct"

print("Loading base model ...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    load_in_4bit = True,   # 4-bit quant
    fast_inference = True, # vLLM backend
    max_lora_rank = lora_rank,
    gpu_memory_utilization = 0.5,
)

print("Attaching LoRA ...")
model = FastLanguageModel.get_peft_model(
    model,
    r = lora_rank,
    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
    lora_alpha = lora_rank,
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
)

print("Model + LoRA ready.")

########################################
# 2. Doctor–Patient Scenario
########################################

COMMON_DISEASES = [
    "Influenza",
    "Common cold",
    "Strep throat",
    "COVID-19",
    "Allergic rhinitis",
    "Migraine",
    "Mononucleosis",
]

SYSTEM_PROMPT = """
You are a Doctor diagnosing a patient. Always provide:
<reason> your chain-of-thought reasoning here </reason>
Then provide short visible text for the patient.

When you conclude, provide a final line like:
Final diagnosis: XYZ
"""

MAX_TURNS = 5

class DoctorGame:
    """
    Multi-turn scenario:
      - A hidden disease (condition) is generated by the OpenAI API if an API key is provided,
        otherwise chosen randomly from COMMON_DISEASES.
      - The Doctor must produce <reason> blocks + visible text.
      - The episode ends if "Final diagnosis: X" is produced or MAX_TURNS is exceeded.
      - Partial credit and a base reward are provided if any final diagnosis is given.
    """

    def __init__(self, openai_api_key: str = None):
        if openai_api_key:
            try:
                openai.api_key = openai_api_key
                prompt = ("Generate a plausible common medical condition (for example: Influenza, COVID-19, Migraine, etc.) "
                          "and provide only the name of the condition.")
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    max_tokens=10
                )
                self.hidden_disease = response['choices'][0]['message']['content'].strip()
            except Exception as e:
                print(f"OpenAI API error in generating condition: {e}")
                self.hidden_disease = random.choice(COMMON_DISEASES)
        else:
            self.hidden_disease = random.choice(COMMON_DISEASES)
        self.turn_count = 0
        self.done = False

    def is_done(self):
        return self.done or (self.turn_count >= MAX_TURNS)

    def parse_final_diagnosis(self, text: str) -> str:
        """If there's 'Final diagnosis: X', return X. Else ''."""
        match = re.search(r"Final\s*diagnosis:\s*(.*)", text, re.IGNORECASE)
        if match:
            return match.group(1).strip()
        return ""

    def compute_reward(self, final_guess: str) -> float:
        """
        If final_guess is non-empty => base reward of 0.2.
        Then partial credit if it matches the hidden disease.
        """
        if not final_guess:
            return 0.0

        base = 0.2
        guess_lower = final_guess.lower()
        disease_lower = self.hidden_disease.lower()

        if guess_lower == disease_lower:
            return 1.0
        if guess_lower in disease_lower or disease_lower in guess_lower:
            return 0.8
        return base

########################################
# 3. Multi-Turn Generation
########################################

class DoctorGRPOTrainer(UnslothGRPOTrainer):
    """
    Similar to Tic-Tac-Toe's CustomGRPOTrainer, but for the Doctor scenario.
    We override multi_turn_generation to include patient simulation via the OpenAI API.
    """

    def __init__(self, *args, game_object=None, **kwargs):
        self.game_object_factory = game_object
        super().__init__(*args, **kwargs)

    def simulate_patient_response(self, conversation_history: str, hidden_disease: str) -> str:
        """
        Uses the OpenAI API to generate a patient response. The prompt instructs the model
        to simulate a patient who has the hidden condition but does not reveal it.
        """
        try:
            openai.api_key = self.args.openai_api_key
            prompt = (
                f"You are a patient who has the following condition: {hidden_disease}. "
                "Answer the doctor's questions by describing your symptoms and feelings in a realistic manner "
                "without explicitly mentioning your condition. The conversation so far is:\n"
                f"{conversation_history}\n"
                "Now, provide your next response as a message starting with 'Patient:'"
            )
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You are a patient simulating your condition."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=150
            )
            patient_text = response['choices'][0]['message']['content']
        except Exception as e:
            print(f"OpenAI API error in simulating patient response: {e}")
            # Fallback to a default patient message if API call fails
            patient_text = "Patient: I still have the same symptoms, please refine your diagnosis."
        return patient_text + "\n"

    def multi_turn_generation(self, prompt, model, tokenizer, generation_config, max_new_tokens=50, game_object=None):
        print("============ Starting a new Doctor–Patient episode ============")
        game = self.game_object_factory() if self.game_object_factory else None
        if not game:
            raise ValueError("No game_object_factory provided")

        full_text = prompt
        total_reward = 0.0
        completion_ids = []

        while not game.is_done():
            outputs = self.llm.generate(
                [full_text],
                sampling_params=self.sampling_params,
                use_tqdm=False
            )
            new_text = outputs[0].outputs[0].text
            new_token_ids = outputs[0].outputs[0].token_ids

            full_text += new_text
            completion_ids.extend(new_token_ids)
            game.turn_count += 1

            # Check for final diagnosis in the doctor's output
            diag_match = re.search(r"Final\s*diagnosis:\s*(.*)", new_text, re.IGNORECASE)
            if diag_match:
                final_guess = diag_match.group(1).strip()
                game.done = True
                diag_reward = game.compute_reward(final_guess)
                total_reward += diag_reward
                print(f"Doctor final guess: {final_guess} => reward={diag_reward:.3f}")
                break

            if game.turn_count >= MAX_TURNS:
                print("No final diagnosis after max turns => 0 reward")
                break

            # Instead of a hardcoded patient message, simulate the patient's response via OpenAI API.
            patient_text = self.simulate_patient_response(full_text, game.hidden_disease)
            full_text += patient_text

        return completion_ids, total_reward

    def _prepare_inputs(self, inputs: dict):
        """
        Identical approach to your Tic-Tac-Toe code:
          - Gather all prompts.
          - Run multi_turn_generation on the main process.
          - Parse final rewards.
          - Compute advantage.
          - Return RL data.
        """
        if not hasattr(self, "generation_config"):
            if self.args.use_vllm:
                self.generation_config = self.sampling_params
            else:
                self.generation_config = GenerationConfig(
                    max_new_tokens=self.max_completion_length,
                    do_sample=True,
                    temperature=self.args.temperature,
                    pad_token_id=self.processing_class.pad_token_id,
                )
        device = self.accelerator.device

        prompts = [x["prompt"] for x in inputs]
        prompts_text = [
            maybe_apply_chat_template(example, self.processing_class)["prompt"] 
            for example in inputs
        ]

        prompt_inputs = self.processing_class(
            prompts_text,
            return_tensors="pt",
            padding=True,
            padding_side="left",
            add_special_tokens=False
        )
        prompt_ids = prompt_inputs["input_ids"].to(device)
        prompt_mask = prompt_inputs["attention_mask"].to(device)

        if self.max_prompt_length is not None:
            prompt_ids = prompt_ids[:, -self.max_prompt_length:]
            prompt_mask = prompt_mask[:, -self.max_prompt_length:]

        if self.args.use_vllm:
            if self.state.global_step != self._last_loaded_step:
                self._move_model_to_vllm()
                self._last_loaded_step = self.state.global_step

            all_prompts_text = gather_object(prompts_text)

            if self.accelerator.is_main_process:
                completion_ids_list = []
                game_rewards_list = []
                for ptxt in all_prompts_text:
                    cids, rew = self.multi_turn_generation(
                        ptxt, self.model, self.processing_class, self.generation_config,
                        max_new_tokens=self.max_completion_length
                    )
                    completion_ids_list.append(cids)
                    game_rewards_list.append(rew)
            else:
                completion_ids_list = [None]*len(all_prompts_text)
                game_rewards_list = [0.0]*len(all_prompts_text)

            completion_ids_list = broadcast_object_list(completion_ids_list, from_process=0)
            game_rewards_list = broadcast_object_list(game_rewards_list, from_process=0)

            game_rewards_tensor = torch.tensor(game_rewards_list, dtype=torch.float32, device=device)

            slice_ = slice(
                self.accelerator.process_index * len(prompts),
                (self.accelerator.process_index + 1)*len(prompts)
            )
            completion_ids_list = completion_ids_list[slice_]

            completion_ids_tensors = [torch.tensor(x, device=device) for x in completion_ids_list]
            completion_ids_padded = pad(completion_ids_tensors, padding_value=self.processing_class.pad_token_id)

            prompt_completion_ids = torch.cat([prompt_ids, completion_ids_padded], dim=1)
        else:
            raise NotImplementedError("This example requires use_vllm=True")

        is_eos = completion_ids_padded == self.processing_class.eos_token_id
        eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)
        eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]
        seq_indices = torch.arange(is_eos.size(1), device=device).expand(is_eos.size(0), -1)
        completion_mask = (seq_indices <= eos_idx.unsqueeze(1)).int()
        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)

        logits_to_keep = completion_ids_padded.size(1)

        with torch.inference_mode(), torch.amp.autocast(
            device_type='cuda',
            dtype=(torch.float16 if os.environ.get('ACCELERATE_MIXED_PRECISION','fp16')=='fp16'
                   else torch.bfloat16)
        ) if not torch.is_autocast_enabled('cuda') else nullcontext():
            if self.ref_model is not None:
                ref_per_token_logps = self._get_per_token_logps(
                    self.ref_model, prompt_completion_ids, attention_mask, logits_to_keep
                )
            else:
                with self.accelerator.unwrap_model(self.model, keep_fp32_wrapper=False).disable_adapter():
                    ref_per_token_logps = self._get_per_token_logps(
                        self.model, prompt_completion_ids, attention_mask, logits_to_keep
                    )

        completions_text = self.processing_class.batch_decode(completion_ids_padded, skip_special_tokens=True)

        rewards_per_func = torch.zeros(len(prompts), len(self.reward_funcs), device=device)
        for i,(reward_func,reward_proc_class) in enumerate(zip(self.reward_funcs,self.reward_processing_classes)):
            keys = [k for k in inputs[0] if k not in ["prompt","completion"]]
            reward_kwargs = {k: [ex[k] for ex in inputs] for k in keys}
            out_rews = reward_func(prompts=prompts, completions=completions_text, **reward_kwargs)
            rewards_per_func[:,i] = torch.tensor(out_rews, dtype=torch.float32, device=device)

        rewards_per_func = gather(rewards_per_func)
        game_rewards_tensor = gather(game_rewards_tensor)

        extended = torch.cat([rewards_per_func, game_rewards_tensor.unsqueeze(1)], dim=1)

        if not hasattr(self, 'reward_weights'):
            self.reward_weights = torch.ones(1, device=device)
        game_weight = torch.tensor([1.0], device=device)
        new_weights = torch.cat([self.reward_weights.to(device), game_weight])

        final_rewards = (extended * new_weights.unsqueeze(0)).sum(dim=1)
        mg = final_rewards.view(-1,self.num_generations).mean(dim=1)
        sg = final_rewards.view(-1,self.num_generations).std(dim=1)

        mg = mg.repeat_interleave(self.num_generations, dim=0)
        sg = sg.repeat_interleave(self.num_generations, dim=0)
        advantages = (final_rewards - mg)/(sg + 1e-4)

        local_slice = slice(
            self.accelerator.process_index * len(prompts),
            (self.accelerator.process_index + 1)*len(prompts)
        )
        advantages = advantages[local_slice]

        self._metrics["rewards/game_reward"].append(game_rewards_tensor.mean().item())
        self._metrics["reward"].append(final_rewards.mean().item())
        self._metrics["reward_std"].append(sg.mean().item())

        return {
            "prompt_ids": prompt_ids,
            "prompt_mask": prompt_mask,
            "completion_ids": completion_ids_padded,
            "completion_mask": completion_mask,
            "ref_per_token_logps": ref_per_token_logps,
            "advantages": advantages,
        }

########################################
# 4. Reward Stub & Build Dataset
########################################

def doctor_game_reward(prompts, completions, **kwargs) -> list[float]:
    """Stub that always returns 0, the real reward is from multi_turn_generation."""
    return [0.0]*len(prompts)

def create_doctor_database():
    """
    Create a single-row dataset with system and user messages in the 'prompt' field.
    """
    row = {
        "prompt": [
            {"role":"system","content":SYSTEM_PROMPT},
            {"role":"user","content":"I have a headache and fatigue, can you help me?"}
        ],
        "answer": ""
    }
    return [row]

########################################
# 5. Configure & Train
########################################

# Make sure the GRPOConfig now includes the openai_api_key flag.
config = GRPOConfig(
    use_vllm=True,
    learning_rate=5e-6,
    temperature=0.7,
    logging_steps=1,
    max_steps=20,       # just a small demo
    save_steps=10,
    max_prompt_length=max_seq_length-512,
    max_completion_length=512,
    num_generations=5,  # generate 5 completions per scenario => better advantage
    output_dir=f"{save_path}/outputs",
    openai_api_key="YOUR_OPENAI_API_KEY_HERE"  # <-- This flag should be passed via the command.
)

df = pd.DataFrame(create_doctor_database())
train_dataset = Dataset.from_pandas(df)

# Pass a lambda to provide the API key when instantiating the game.
trainer = DoctorGRPOTrainer(
    model=model,
    processing_class=tokenizer,
    reward_funcs=[doctor_game_reward],
    args=config,
    train_dataset=train_dataset,
    game_object=lambda: DoctorGame(openai_api_key=config.openai_api_key)
)

print("Starting training ...")
trainer.train()

# Save final LoRA & checkpoint
model.save_lora(f"{save_path}/doctor_grpo_saved_lora")
cp_path = f"{save_path}/doctor_checkpoint"
trainer.save_model(cp_path)
trainer.state.save_to_json(f"{cp_path}/trainer_state.json")
model.save_lora(f"{save_path}/doctor_final_lora")

print("Training complete!")
